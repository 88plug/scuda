// Generated code.

#include <cuda.h>
#include <cuda_runtime_api.h>
#include <nvml.h>
                
#include <string>
#include <unordered_map>

#include "gen_api.h"
#include "gen_server.h"

extern int rpc_read(const void *conn, void *data, const size_t size);
extern int rpc_write(const void *conn, const void *data, const size_t size);
extern int rpc_end_request(const void *conn);
extern int rpc_start_response(const void *conn, const int request_id);

/**
* Initialize NVML, but don't initialize any GPUs yet.
*
* \note nvmlInit_v3 introduces a "flags" argument, that allows passing boolean values
*       modifying the behaviour of nvmlInit().
* \note In NVML 5.319 new nvmlInit_v2 has replaced nvmlInit"_v1" (default in NVML 4.304 and older) that
*       did initialize all GPU devices in the system.
*
* This allows NVML to communicate with a GPU
* when other GPUs in the system are unstable or in a bad state.  When using this API, GPUs are
* discovered and initialized in nvmlDeviceGetHandleBy* functions instead.
*
* \note To contrast nvmlInit_v2 with nvmlInit"_v1", NVML 4.304 nvmlInit"_v1" will fail when any detected GPU is in
*       a bad or unstable state.
*
* For all products.
*
* This method, should be called once before invoking any other methods in the library.
* A reference count of the number of initializations is maintained.  Shutdown only occurs
* when the reference count reaches zero.
*
* @return
*         - \ref NVML_SUCCESS                   if NVML has been properly initialized
*         - \ref NVML_ERROR_DRIVER_NOT_LOADED   if NVIDIA driver is not running
*         - \ref NVML_ERROR_NO_PERMISSION       if NVML does not have permission to talk to the driver
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlInit_v2(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlInit_v2();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* nvmlInitWithFlags is a variant of nvmlInit(), that allows passing a set of boolean values
*       modifying the behaviour of nvmlInit().
*       Other than the "flags" parameter it is completely similar to \ref nvmlInit_v2.
*
* For all products.
*
* @param flags                                 behaviour modifier flags
*
* @return
*         - \ref NVML_SUCCESS                   if NVML has been properly initialized
*         - \ref NVML_ERROR_DRIVER_NOT_LOADED   if NVIDIA driver is not running
*         - \ref NVML_ERROR_NO_PERMISSION       if NVML does not have permission to talk to the driver
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlInitWithFlags(void *conn) {
    unsigned int flags;

    if (rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlInitWithFlags(flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Shut down NVML by releasing all GPU resources previously allocated with \ref nvmlInit_v2().
*
* For all products.
*
* This method should be called after NVML work is done, once for each call to \ref nvmlInit_v2()
* A reference count of the number of initializations is maintained.  Shutdown only occurs
* when the reference count reaches zero.  For backwards compatibility, no error is reported if
* nvmlShutdown() is called more times than nvmlInit().
*
* @return
*         - \ref NVML_SUCCESS                 if NVML has been properly shut down
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlShutdown(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlShutdown();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version of the system's graphics driver.
*
* For all products.
*
* The version identifier is an alphanumeric string.  It will not exceed 80 characters in length
* (including the NULL terminator).  See \ref nvmlConstants::NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE.
*
* @param version                              Reference in which to return the version identifier
* @param length                               The maximum allowed length of the string returned in \a version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a version is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*/
int handle_nvmlSystemGetDriverVersion(void *conn) {
    unsigned int length;

    if (rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlSystemGetDriverVersion(version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version of the NVML library.
*
* For all products.
*
* The version identifier is an alphanumeric string.  It will not exceed 80 characters in length
* (including the NULL terminator).  See \ref nvmlConstants::NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE.
*
* @param version                              Reference in which to return the version identifier
* @param length                               The maximum allowed length of the string returned in \a version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a version is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*/
int handle_nvmlSystemGetNVMLVersion(void *conn) {
    unsigned int length;

    if (rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlSystemGetNVMLVersion(version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version of the CUDA driver.
*
* For all products.
*
* The CUDA driver version returned will be retrieved from the currently installed version of CUDA.
* If the cuda library is not found, this function will return a known supported version number.
*
* @param cudaDriverVersion                    Reference in which to return the version identifier
*
* @return
*         - \ref NVML_SUCCESS                 if \a cudaDriverVersion has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a cudaDriverVersion is NULL
*/
int handle_nvmlSystemGetCudaDriverVersion(void *conn) {
    int cudaDriverVersion;

    if (rpc_read(conn, &cudaDriverVersion, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlSystemGetCudaDriverVersion(&cudaDriverVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &cudaDriverVersion, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version of the CUDA driver from the shared library.
*
* For all products.
*
* The returned CUDA driver version by calling cuDriverGetVersion()
*
* @param cudaDriverVersion                    Reference in which to return the version identifier
*
* @return
*         - \ref NVML_SUCCESS                  if \a cudaDriverVersion has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a cudaDriverVersion is NULL
*         - \ref NVML_ERROR_LIBRARY_NOT_FOUND  if \a libcuda.so.1 or libcuda.dll is not found
*         - \ref NVML_ERROR_FUNCTION_NOT_FOUND if \a cuDriverGetVersion() is not found in the shared library
*/
int handle_nvmlSystemGetCudaDriverVersion_v2(void *conn) {
    int cudaDriverVersion;

    if (rpc_read(conn, &cudaDriverVersion, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlSystemGetCudaDriverVersion_v2(&cudaDriverVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &cudaDriverVersion, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Gets name of the process with provided process id
*
* For all products.
*
* Returned process name is cropped to provided length.
* name string is encoded in ANSI.
*
* @param pid                                  The identifier of the process
* @param name                                 Reference in which to return the process name
* @param length                               The maximum allowed length of the string returned in \a name
*
* @return
*         - \ref NVML_SUCCESS                 if \a name has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a name is NULL or \a length is 0.
*         - \ref NVML_ERROR_NOT_FOUND         if process doesn't exists
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlSystemGetProcessName(void *conn) {
    unsigned int pid;
    unsigned int length;

    if (rpc_read(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* name = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlSystemGetProcessName(pid, name, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, name, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the number of units in the system.
*
* For S-class products.
*
* @param unitCount                            Reference in which to return the number of units
*
* @return
*         - \ref NVML_SUCCESS                 if \a unitCount has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unitCount is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetCount(void *conn) {
    unsigned int unitCount;

    if (rpc_read(conn, &unitCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetCount(&unitCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &unitCount, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Acquire the handle for a particular unit, based on its index.
*
* For S-class products.
*
* Valid indices are derived from the \a unitCount returned by \ref nvmlUnitGetCount().
*   For example, if \a unitCount is 2 the valid indices are 0 and 1, corresponding to UNIT 0 and UNIT 1.
*
* The order in which NVML enumerates units has no guarantees of consistency between reboots.
*
* @param index                                The index of the target unit, >= 0 and < \a unitCount
* @param unit                                 Reference in which to return the unit handle
*
* @return
*         - \ref NVML_SUCCESS                 if \a unit has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a index is invalid or \a unit is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetHandleByIndex(void *conn) {
    unsigned int index;
    nvmlUnit_t unit;

    if (rpc_read(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetHandleByIndex(index, &unit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the static information associated with a unit.
*
* For S-class products.
*
* See \ref nvmlUnitInfo_t for details on available unit info.
*
* @param unit                                 The identifier of the target unit
* @param info                                 Reference in which to return the unit information
*
* @return
*         - \ref NVML_SUCCESS                 if \a info has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit is invalid or \a info is NULL
*/
int handle_nvmlUnitGetUnitInfo(void *conn) {
    nvmlUnit_t unit;
    nvmlUnitInfo_t info;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlUnitInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetUnitInfo(unit, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlUnitInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the LED state associated with this unit.
*
* For S-class products.
*
* See \ref nvmlLedState_t for details on allowed states.
*
* @param unit                                 The identifier of the target unit
* @param state                                Reference in which to return the current LED state
*
* @return
*         - \ref NVML_SUCCESS                 if \a state has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit is invalid or \a state is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this is not an S-class product
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlUnitSetLedState()
*/
int handle_nvmlUnitGetLedState(void *conn) {
    nvmlUnit_t unit;
    nvmlLedState_t state;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &state, sizeof(nvmlLedState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetLedState(unit, &state);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &state, sizeof(nvmlLedState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the PSU stats for the unit.
*
* For S-class products.
*
* See \ref nvmlPSUInfo_t for details on available PSU info.
*
* @param unit                                 The identifier of the target unit
* @param psu                                  Reference in which to return the PSU information
*
* @return
*         - \ref NVML_SUCCESS                 if \a psu has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit is invalid or \a psu is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this is not an S-class product
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetPsuInfo(void *conn) {
    nvmlUnit_t unit;
    nvmlPSUInfo_t psu;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &psu, sizeof(nvmlPSUInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetPsuInfo(unit, &psu);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &psu, sizeof(nvmlPSUInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the temperature readings for the unit, in degrees C.
*
* For S-class products.
*
* Depending on the product, readings may be available for intake (type=0),
* exhaust (type=1) and board (type=2).
*
* @param unit                                 The identifier of the target unit
* @param type                                 The type of reading to take
* @param temp                                 Reference in which to return the intake temperature
*
* @return
*         - \ref NVML_SUCCESS                 if \a temp has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit or \a type is invalid or \a temp is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this is not an S-class product
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetTemperature(void *conn) {
    nvmlUnit_t unit;
    unsigned int type;
    unsigned int temp;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &type, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetTemperature(unit, type, &temp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the fan speed readings for the unit.
*
* For S-class products.
*
* See \ref nvmlUnitFanSpeeds_t for details on available fan speed info.
*
* @param unit                                 The identifier of the target unit
* @param fanSpeeds                            Reference in which to return the fan speed information
*
* @return
*         - \ref NVML_SUCCESS                 if \a fanSpeeds has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit is invalid or \a fanSpeeds is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this is not an S-class product
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetFanSpeedInfo(void *conn) {
    nvmlUnit_t unit;
    nvmlUnitFanSpeeds_t fanSpeeds;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &fanSpeeds, sizeof(nvmlUnitFanSpeeds_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitGetFanSpeedInfo(unit, &fanSpeeds);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fanSpeeds, sizeof(nvmlUnitFanSpeeds_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the set of GPU devices that are attached to the specified unit.
*
* For S-class products.
*
* The \a deviceCount argument is expected to be set to the size of the input \a devices array.
*
* @param unit                                 The identifier of the target unit
* @param deviceCount                          Reference in which to provide the \a devices array size, and
*                                             to return the number of attached GPU devices
* @param devices                              Reference in which to return the references to the attached GPU devices
*
* @return
*         - \ref NVML_SUCCESS                 if \a deviceCount and \a devices have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a deviceCount indicates that the \a devices array is too small
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit is invalid, either of \a deviceCount or \a devices is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlUnitGetDevices(void *conn) {
    nvmlUnit_t unit;
    unsigned int deviceCount;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &deviceCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlDevice_t* devices = (nvmlDevice_t*)malloc(deviceCount * sizeof(nvmlDevice_t));

    nvmlReturn_t result = nvmlUnitGetDevices(unit, &deviceCount, devices);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &deviceCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, devices, deviceCount * sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the IDs and firmware versions for any Host Interface Cards (HICs) in the system.
*
* For S-class products.
*
* The \a hwbcCount argument is expected to be set to the size of the input \a hwbcEntries array.
* The HIC must be connected to an S-class system for it to be reported by this function.
*
* @param hwbcCount                            Size of hwbcEntries array
* @param hwbcEntries                          Array holding information about hwbc
*
* @return
*         - \ref NVML_SUCCESS                 if \a hwbcCount and \a hwbcEntries have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if either \a hwbcCount or \a hwbcEntries is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a hwbcCount indicates that the \a hwbcEntries array is too small
*/
int handle_nvmlSystemGetHicVersion(void *conn) {
    unsigned int hwbcCount;

    if (rpc_read(conn, &hwbcCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlHwbcEntry_t* hwbcEntries = (nvmlHwbcEntry_t*)malloc(hwbcCount * sizeof(nvmlHwbcEntry_t));

    nvmlReturn_t result = nvmlSystemGetHicVersion(&hwbcCount, hwbcEntries);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &hwbcCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, hwbcEntries, hwbcCount * sizeof(nvmlHwbcEntry_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the number of compute devices in the system. A compute device is a single GPU.
*
* For all products.
*
* Note: New nvmlDeviceGetCount_v2 (default in NVML 5.319) returns count of all devices in the system
*       even if nvmlDeviceGetHandleByIndex_v2 returns NVML_ERROR_NO_PERMISSION for such device.
*       Update your code to handle this error, or use NVML 4.304 or older nvml header file.
*       For backward binary compatibility reasons _v1 version of the API is still present in the shared
*       library.
*       Old _v1 version of nvmlDeviceGetCount doesn't count devices that NVML has no permission to talk to.
*
* @param deviceCount                          Reference in which to return the number of accessible devices
*
* @return
*         - \ref NVML_SUCCESS                 if \a deviceCount has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a deviceCount is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetCount_v2(void *conn) {
    unsigned int deviceCount;

    if (rpc_read(conn, &deviceCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetCount_v2(&deviceCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &deviceCount, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get attributes (engine counts etc.) for the given NVML device handle.
*
* @note This API currently only supports MIG device handles.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               NVML device handle
* @param attributes                           Device attributes
*
* @return
*        - \ref NVML_SUCCESS                  if \a device attributes were successfully retrieved
*        - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device handle is invalid
*        - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*        - \ref NVML_ERROR_NOT_SUPPORTED      if this query is not supported by the device
*        - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetAttributes_v2(void *conn) {
    nvmlDevice_t device;
    nvmlDeviceAttributes_t attributes;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &attributes, sizeof(nvmlDeviceAttributes_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAttributes_v2(device, &attributes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &attributes, sizeof(nvmlDeviceAttributes_t)) < 0)
        return -1;

    return result;
}

/**
* Acquire the handle for a particular device, based on its index.
*
* For all products.
*
* Valid indices are derived from the \a accessibleDevices count returned by
*   \ref nvmlDeviceGetCount_v2(). For example, if \a accessibleDevices is 2 the valid indices
*   are 0 and 1, corresponding to GPU 0 and GPU 1.
*
* The order in which NVML enumerates devices has no guarantees of consistency between reboots. For that reason it
*   is recommended that devices be looked up by their PCI ids or UUID. See
*   \ref nvmlDeviceGetHandleByUUID() and \ref nvmlDeviceGetHandleByPciBusId_v2().
*
* Note: The NVML index may not correlate with other APIs, such as the CUDA device index.
*
* Starting from NVML 5, this API causes NVML to initialize the target GPU
* NVML may initialize additional GPUs if:
*  - The target GPU is an SLI slave
*
* Note: New nvmlDeviceGetCount_v2 (default in NVML 5.319) returns count of all devices in the system
*       even if nvmlDeviceGetHandleByIndex_v2 returns NVML_ERROR_NO_PERMISSION for such device.
*       Update your code to handle this error, or use NVML 4.304 or older nvml header file.
*       For backward binary compatibility reasons _v1 version of the API is still present in the shared
*       library.
*       Old _v1 version of nvmlDeviceGetCount doesn't count devices that NVML has no permission to talk to.
*
*       This means that nvmlDeviceGetHandleByIndex_v2 and _v1 can return different devices for the same index.
*       If you don't touch macros that map old (_v1) versions to _v2 versions at the top of the file you don't
*       need to worry about that.
*
* @param index                                The index of the target GPU, >= 0 and < \a accessibleDevices
* @param device                               Reference in which to return the device handle
*
* @return
*         - \ref NVML_SUCCESS                  if \a device has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a index is invalid or \a device is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_POWER if any attached devices have improperly attached external power cables
*         - \ref NVML_ERROR_NO_PERMISSION      if the user doesn't have permission to talk to this device
*         - \ref NVML_ERROR_IRQ_ISSUE          if NVIDIA kernel detected an interrupt issue with the attached GPUs
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*
* @see nvmlDeviceGetIndex
* @see nvmlDeviceGetCount
*/
int handle_nvmlDeviceGetHandleByIndex_v2(void *conn) {
    unsigned int index;
    nvmlDevice_t device;

    if (rpc_read(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetHandleByIndex_v2(index, &device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Acquire the handle for a particular device, based on its board serial number.
*
* For Fermi &tm; or newer fully supported devices.
*
* This number corresponds to the value printed directly on the board, and to the value returned by
*   \ref nvmlDeviceGetSerial().
*
* @deprecated Since more than one GPU can exist on a single board this function is deprecated in favor
*             of \ref nvmlDeviceGetHandleByUUID.
*             For dual GPU boards this function will return NVML_ERROR_INVALID_ARGUMENT.
*
* Starting from NVML 5, this API causes NVML to initialize the target GPU
* NVML may initialize additional GPUs as it searches for the target GPU
*
* @param serial                               The board serial number of the target GPU
* @param device                               Reference in which to return the device handle
*
* @return
*         - \ref NVML_SUCCESS                  if \a device has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a serial is invalid, \a device is NULL or more than one
*                                              device has the same serial (dual GPU boards)
*         - \ref NVML_ERROR_NOT_FOUND          if \a serial does not match a valid device on the system
*         - \ref NVML_ERROR_INSUFFICIENT_POWER if any attached devices have improperly attached external power cables
*         - \ref NVML_ERROR_IRQ_ISSUE          if NVIDIA kernel detected an interrupt issue with the attached GPUs
*         - \ref NVML_ERROR_GPU_IS_LOST        if any GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*
* @see nvmlDeviceGetSerial
* @see nvmlDeviceGetHandleByUUID
*/
int handle_nvmlDeviceGetHandleBySerial(void *conn) {
    char serial;
    nvmlDevice_t device;

    if (rpc_read(conn, &serial, sizeof(char)) < 0 ||
        rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetHandleBySerial(&serial, &device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Acquire the handle for a particular device, based on its globally unique immutable UUID associated with each device.
*
* For all products.
*
* @param uuid                                 The UUID of the target GPU or MIG instance
* @param device                               Reference in which to return the device handle or MIG device handle
*
* Starting from NVML 5, this API causes NVML to initialize the target GPU
* NVML may initialize additional GPUs as it searches for the target GPU
*
* @return
*         - \ref NVML_SUCCESS                  if \a device has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a uuid is invalid or \a device is null
*         - \ref NVML_ERROR_NOT_FOUND          if \a uuid does not match a valid device on the system
*         - \ref NVML_ERROR_INSUFFICIENT_POWER if any attached devices have improperly attached external power cables
*         - \ref NVML_ERROR_IRQ_ISSUE          if NVIDIA kernel detected an interrupt issue with the attached GPUs
*         - \ref NVML_ERROR_GPU_IS_LOST        if any GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*
* @see nvmlDeviceGetUUID
*/
int handle_nvmlDeviceGetHandleByUUID(void *conn) {
    char uuid;
    nvmlDevice_t device;

    if (rpc_read(conn, &uuid, sizeof(char)) < 0 ||
        rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetHandleByUUID(&uuid, &device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Acquire the handle for a particular device, based on its PCI bus id.
*
* For all products.
*
* This value corresponds to the nvmlPciInfo_t::busId returned by \ref nvmlDeviceGetPciInfo_v3().
*
* Starting from NVML 5, this API causes NVML to initialize the target GPU
* NVML may initialize additional GPUs if:
*  - The target GPU is an SLI slave
*
* \note NVML 4.304 and older version of nvmlDeviceGetHandleByPciBusId"_v1" returns NVML_ERROR_NOT_FOUND
*       instead of NVML_ERROR_NO_PERMISSION.
*
* @param pciBusId                             The PCI bus id of the target GPU
* @param device                               Reference in which to return the device handle
*
* @return
*         - \ref NVML_SUCCESS                  if \a device has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a pciBusId is invalid or \a device is NULL
*         - \ref NVML_ERROR_NOT_FOUND          if \a pciBusId does not match a valid device on the system
*         - \ref NVML_ERROR_INSUFFICIENT_POWER if the attached device has improperly attached external power cables
*         - \ref NVML_ERROR_NO_PERMISSION      if the user doesn't have permission to talk to this device
*         - \ref NVML_ERROR_IRQ_ISSUE          if NVIDIA kernel detected an interrupt issue with the attached GPUs
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetHandleByPciBusId_v2(void *conn) {
    char pciBusId;
    nvmlDevice_t device;

    if (rpc_read(conn, &pciBusId, sizeof(char)) < 0 ||
        rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetHandleByPciBusId_v2(&pciBusId, &device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the name of this device.
*
* For all products.
*
* The name is an alphanumeric string that denotes a particular product, e.g. Tesla &tm; C2070. It will not
* exceed 96 characters in length (including the NULL terminator).  See \ref
* nvmlConstants::NVML_DEVICE_NAME_V2_BUFFER_SIZE.
*
* When used with MIG device handles the API returns MIG device names which can be used to identify devices
* based on their attributes.
*
* @param device                               The identifier of the target device
* @param name                                 Reference in which to return the product name
* @param length                               The maximum allowed length of the string returned in \a name
*
* @return
*         - \ref NVML_SUCCESS                 if \a name has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a name is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetName(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* name = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetName(device, name, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, name, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the brand of this device.
*
* For all products.
*
* The type is a member of \ref nvmlBrandType_t defined above.
*
* @param device                               The identifier of the target device
* @param type                                 Reference in which to return the product brand type
*
* @return
*         - \ref NVML_SUCCESS                 if \a name has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a type is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetBrand(void *conn) {
    nvmlDevice_t device;
    nvmlBrandType_t type;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlBrandType_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetBrand(device, &type);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &type, sizeof(nvmlBrandType_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the NVML index of this device.
*
* For all products.
*
* Valid indices are derived from the \a accessibleDevices count returned by
*   \ref nvmlDeviceGetCount_v2(). For example, if \a accessibleDevices is 2 the valid indices
*   are 0 and 1, corresponding to GPU 0 and GPU 1.
*
* The order in which NVML enumerates devices has no guarantees of consistency between reboots. For that reason it
*   is recommended that devices be looked up by their PCI ids or GPU UUID. See
*   \ref nvmlDeviceGetHandleByPciBusId_v2() and \ref nvmlDeviceGetHandleByUUID().
*
* When used with MIG device handles this API returns indices that can be
* passed to \ref nvmlDeviceGetMigDeviceHandleByIndex to retrieve an identical handle.
* MIG device indices are unique within a device.
*
* Note: The NVML index may not correlate with other APIs, such as the CUDA device index.
*
* @param device                               The identifier of the target device
* @param index                                Reference in which to return the NVML index of the device
*
* @return
*         - \ref NVML_SUCCESS                 if \a index has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a index is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetHandleByIndex()
* @see nvmlDeviceGetCount()
*/
int handle_nvmlDeviceGetIndex(void *conn) {
    nvmlDevice_t device;
    unsigned int index;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &index, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetIndex(device, &index);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &index, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the globally unique board serial number associated with this device's board.
*
* For all products with an inforom.
*
* The serial number is an alphanumeric string that will not exceed 30 characters (including the NULL terminator).
* This number matches the serial number tag that is physically attached to the board.  See \ref
* nvmlConstants::NVML_DEVICE_SERIAL_BUFFER_SIZE.
*
* @param device                               The identifier of the target device
* @param serial                               Reference in which to return the board/module serial number
* @param length                               The maximum allowed length of the string returned in \a serial
*
* @return
*         - \ref NVML_SUCCESS                 if \a serial has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a serial is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetSerial(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* serial = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetSerial(device, serial, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, serial, length * sizeof(char)) < 0)
        return -1;

    return result;
}

int handle_nvmlDeviceGetMemoryAffinity(void *conn) {
    nvmlDevice_t device;
    unsigned int nodeSetSize;
    nvmlAffinityScope_t scope;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &nodeSetSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &scope, sizeof(nvmlAffinityScope_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned long* nodeSet = (unsigned long*)malloc(nodeSetSize * sizeof(unsigned long));

    nvmlReturn_t result = nvmlDeviceGetMemoryAffinity(device, nodeSetSize, nodeSet, scope);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, nodeSet, nodeSetSize * sizeof(unsigned long)) < 0)
        return -1;

    return result;
}

int handle_nvmlDeviceGetCpuAffinityWithinScope(void *conn) {
    nvmlDevice_t device;
    unsigned int cpuSetSize;
    nvmlAffinityScope_t scope;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &cpuSetSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &scope, sizeof(nvmlAffinityScope_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned long* cpuSet = (unsigned long*)malloc(cpuSetSize * sizeof(unsigned long));

    nvmlReturn_t result = nvmlDeviceGetCpuAffinityWithinScope(device, cpuSetSize, cpuSet, scope);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, cpuSet, cpuSetSize * sizeof(unsigned long)) < 0)
        return -1;

    return result;
}

/**
* Retrieves an array of unsigned ints (sized to cpuSetSize) of bitmasks with the ideal CPU affinity for the device
* For example, if processors 0, 1, 32, and 33 are ideal for the device and cpuSetSize == 2,
*     result[0] = 0x3, result[1] = 0x3
* This is equivalent to calling \ref nvmlDeviceGetCpuAffinityWithinScope with \ref NVML_AFFINITY_SCOPE_NODE.
*
* For Kepler &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               The identifier of the target device
* @param cpuSetSize                           The size of the cpuSet array that is safe to access
* @param cpuSet                               Array reference in which to return a bitmask of CPUs, 64 CPUs per
*                                                 unsigned long on 64-bit machines, 32 on 32-bit machines
*
* @return
*         - \ref NVML_SUCCESS                 if \a cpuAffinity has been filled
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, cpuSetSize == 0, or cpuSet is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetCpuAffinity(void *conn) {
    nvmlDevice_t device;
    unsigned int cpuSetSize;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &cpuSetSize, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned long* cpuSet = (unsigned long*)malloc(cpuSetSize * sizeof(unsigned long));

    nvmlReturn_t result = nvmlDeviceGetCpuAffinity(device, cpuSetSize, cpuSet);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, cpuSet, cpuSetSize * sizeof(unsigned long)) < 0)
        return -1;

    return result;
}

/**
* Sets the ideal affinity for the calling thread and device using the guidelines
* given in nvmlDeviceGetCpuAffinity().  Note, this is a change as of version 8.0.
* Older versions set the affinity for a calling process and all children.
* Currently supports up to 1024 processors.
*
* For Kepler &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if the calling process has been successfully bound
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetCpuAffinity(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetCpuAffinity(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Clear all affinity bindings for the calling thread.  Note, this is a change as of version
* 8.0 as older versions cleared the affinity for a calling process and all children.
*
* For Kepler &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if the calling process has been successfully unbound
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceClearCpuAffinity(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceClearCpuAffinity(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/** @} */
int handle_nvmlDeviceGetTopologyCommonAncestor(void *conn) {
    nvmlDevice_t device1;
    nvmlDevice_t device2;
    nvmlGpuTopologyLevel_t pathInfo;

    if (rpc_read(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pathInfo, sizeof(nvmlGpuTopologyLevel_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTopologyCommonAncestor(device1, device2, &pathInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pathInfo, sizeof(nvmlGpuTopologyLevel_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the set of GPUs that are nearest to a given device at a specific interconnectivity level
* For all products.
* Supported on Linux only.
*
* @param device                               The identifier of the first device
* @param level                                The \ref nvmlGpuTopologyLevel_t level to search for other GPUs
* @param count                                When zero, is set to the number of matching GPUs such that \a deviceArray
*                                             can be malloc'd.  When non-zero, \a deviceArray will be filled with \a count
*                                             number of device handles.
* @param deviceArray                          An array of device handles for GPUs found at \a level
*
* @return
*         - \ref NVML_SUCCESS                 if \a deviceArray or \a count (if initially zero) has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a level, or \a count is invalid, or \a deviceArray is NULL with a non-zero \a count
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device or OS does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           an error has occurred in underlying topology discovery
*/
int handle_nvmlDeviceGetTopologyNearestGpus(void *conn) {
    nvmlDevice_t device;
    nvmlGpuTopologyLevel_t level;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &level, sizeof(nvmlGpuTopologyLevel_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlDevice_t* deviceArray = (nvmlDevice_t*)malloc(count * sizeof(nvmlDevice_t));

    nvmlReturn_t result = nvmlDeviceGetTopologyNearestGpus(device, level, &count, deviceArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, deviceArray, count * sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the set of GPUs that have a CPU affinity with the given CPU number
* For all products.
* Supported on Linux only.
*
* @param cpuNumber                            The CPU number
* @param count                                When zero, is set to the number of matching GPUs such that \a deviceArray
*                                             can be malloc'd.  When non-zero, \a deviceArray will be filled with \a count
*                                             number of device handles.
* @param deviceArray                          An array of device handles for GPUs found with affinity to \a cpuNumber
*
* @return
*         - \ref NVML_SUCCESS                 if \a deviceArray or \a count (if initially zero) has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a cpuNumber, or \a count is invalid, or \a deviceArray is NULL with a non-zero \a count
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device or OS does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           an error has occurred in underlying topology discovery
*/
int handle_nvmlSystemGetTopologyGpuSet(void *conn) {
    unsigned int cpuNumber;
    unsigned int count;
    nvmlDevice_t deviceArray;

    if (rpc_read(conn, &cpuNumber, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &deviceArray, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlSystemGetTopologyGpuSet(cpuNumber, &count, &deviceArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &deviceArray, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the status for a given p2p capability index between a given pair of GPU
*
* @param device1                              The first device
* @param device2                              The second device
* @param p2pIndex                             p2p Capability Index being looked for between \a device1 and \a device2
* @param p2pStatus                            Reference in which to return the status of the \a p2pIndex
*                                             between \a device1 and \a device2
* @return
*         - \ref NVML_SUCCESS         if \a p2pStatus has been populated
*         - \ref NVML_ERROR_INVALID_ARGUMENT     if \a device1 or \a device2 or \a p2pIndex is invalid or \a p2pStatus is NULL
*         - \ref NVML_ERROR_UNKNOWN              on any unexpected error
*/
int handle_nvmlDeviceGetP2PStatus(void *conn) {
    nvmlDevice_t device1;
    nvmlDevice_t device2;
    nvmlGpuP2PCapsIndex_t p2pIndex;
    nvmlGpuP2PStatus_t p2pStatus;

    if (rpc_read(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &p2pIndex, sizeof(nvmlGpuP2PCapsIndex_t)) < 0 ||
        rpc_read(conn, &p2pStatus, sizeof(nvmlGpuP2PStatus_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetP2PStatus(device1, device2, p2pIndex, &p2pStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &p2pStatus, sizeof(nvmlGpuP2PStatus_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the globally unique immutable UUID associated with this device, as a 5 part hexadecimal string,
* that augments the immutable, board serial identifier.
*
* For all products.
*
* The UUID is a globally unique identifier. It is the only available identifier for pre-Fermi-architecture products.
* It does NOT correspond to any identifier printed on the board.  It will not exceed 96 characters in length
* (including the NULL terminator).  See \ref nvmlConstants::NVML_DEVICE_UUID_V2_BUFFER_SIZE.
*
* When used with MIG device handles the API returns globally unique UUIDs which can be used to identify MIG
* devices across both GPU and MIG devices. UUIDs are immutable for the lifetime of a MIG device.
*
* @param device                               The identifier of the target device
* @param uuid                                 Reference in which to return the GPU UUID
* @param length                               The maximum allowed length of the string returned in \a uuid
*
* @return
*         - \ref NVML_SUCCESS                 if \a uuid has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a uuid is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetUUID(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* uuid = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetUUID(device, uuid, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, uuid, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the MDEV UUID of a vGPU instance.
*
* The MDEV UUID is a globally unique identifier of the mdev device assigned to the VM, and is returned as a 5-part hexadecimal string,
* not exceeding 80 characters in length (including the NULL terminator).
* MDEV UUID is displayed only on KVM platform.
* See \ref nvmlConstants::NVML_DEVICE_UUID_BUFFER_SIZE.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param mdevUuid                 Pointer to caller-supplied buffer to hold MDEV UUID
* @param size                     Size of buffer in bytes
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_NOT_SUPPORTED     on any hypervisor other than KVM
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a mdevUuid is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetMdevUUID(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int size;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* mdevUuid = (char*)malloc(size * sizeof(char));

    nvmlReturn_t result = nvmlVgpuInstanceGetMdevUUID(vgpuInstance, mdevUuid, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, mdevUuid, size * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves minor number for the device. The minor number for the device is such that the Nvidia device node file for
* each GPU will have the form /dev/nvidia[minor number].
*
* For all products.
* Supported only for Linux
*
* @param device                                The identifier of the target device
* @param minorNumber                           Reference in which to return the minor number for the device
* @return
*         - \ref NVML_SUCCESS                 if the minor number is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a minorNumber is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMinorNumber(void *conn) {
    nvmlDevice_t device;
    unsigned int minorNumber;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minorNumber, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMinorNumber(device, &minorNumber);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minorNumber, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the the device board part number which is programmed into the board's InfoROM
*
* For all products.
*
* @param device                                Identifier of the target device
* @param partNumber                            Reference to the buffer to return
* @param length                                Length of the buffer reference
*
* @return
*         - \ref NVML_SUCCESS                  if \a partNumber has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_NOT_SUPPORTED      if the needed VBIOS fields have not been filled
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device is invalid or \a serial is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetBoardPartNumber(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* partNumber = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetBoardPartNumber(device, partNumber, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, partNumber, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version information for the device's infoROM object.
*
* For all products with an inforom.
*
* Fermi and higher parts have non-volatile on-board memory for persisting device info, such as aggregate
* ECC counts. The version of the data structures in this memory may change from time to time. It will not
* exceed 16 characters in length (including the NULL terminator).
* See \ref nvmlConstants::NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE.
*
* See \ref nvmlInforomObject_t for details on the available infoROM objects.
*
* @param device                               The identifier of the target device
* @param object                               The target infoROM object
* @param version                              Reference in which to return the infoROM version
* @param length                               The maximum allowed length of the string returned in \a version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a version is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have an infoROM
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetInforomImageVersion
*/
int handle_nvmlDeviceGetInforomVersion(void *conn) {
    nvmlDevice_t device;
    nvmlInforomObject_t object;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &object, sizeof(nvmlInforomObject_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetInforomVersion(device, object, version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the global infoROM image version
*
* For all products with an inforom.
*
* Image version just like VBIOS version uniquely describes the exact version of the infoROM flashed on the board
* in contrast to infoROM object version which is only an indicator of supported features.
* Version string will not exceed 16 characters in length (including the NULL terminator).
* See \ref nvmlConstants::NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE.
*
* @param device                               The identifier of the target device
* @param version                              Reference in which to return the infoROM image version
* @param length                               The maximum allowed length of the string returned in \a version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a version is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have an infoROM
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetInforomVersion
*/
int handle_nvmlDeviceGetInforomImageVersion(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetInforomImageVersion(device, version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the checksum of the configuration stored in the device's infoROM.
*
* For all products with an inforom.
*
* Can be used to make sure that two GPUs have the exact same configuration.
* Current checksum takes into account configuration stored in PWR and ECC infoROM objects.
* Checksum can change between driver releases or when user changes configuration (e.g. disable/enable ECC)
*
* @param device                               The identifier of the target device
* @param checksum                             Reference in which to return the infoROM configuration checksum
*
* @return
*         - \ref NVML_SUCCESS                 if \a checksum has been set
*         - \ref NVML_ERROR_CORRUPTED_INFOROM if the device's checksum couldn't be retrieved due to infoROM corruption
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a checksum is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetInforomConfigurationChecksum(void *conn) {
    nvmlDevice_t device;
    unsigned int checksum;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &checksum, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetInforomConfigurationChecksum(device, &checksum);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &checksum, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Reads the infoROM from the flash and verifies the checksums.
*
* For all products with an inforom.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if infoROM is not corrupted
*         - \ref NVML_ERROR_CORRUPTED_INFOROM if the device's infoROM is corrupted
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceValidateInforom(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceValidateInforom(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the display mode for the device.
*
* For all products.
*
* This method indicates whether a physical display (e.g. monitor) is currently connected to
* any of the device's connectors.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param display                              Reference in which to return the display mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a display has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a display is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetDisplayMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t display;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &display, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDisplayMode(device, &display);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &display, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the display active state for the device.
*
* For all products.
*
* This method indicates whether a display is initialized on the device.
* For example whether X Server is attached to this device and has allocated memory for the screen.
*
* Display can be active even when no monitor is physically attached.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param isActive                             Reference in which to return the display active state
*
* @return
*         - \ref NVML_SUCCESS                 if \a isActive has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a isActive is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetDisplayActive(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t isActive;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &isActive, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDisplayActive(device, &isActive);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isActive, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the persistence mode associated with this device.
*
* For all products.
* For Linux only.
*
* When driver persistence mode is enabled the driver software state is not torn down when the last
* client disconnects. By default this feature is disabled.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param mode                                 Reference in which to return the current driver persistence mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a mode has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetPersistenceMode()
*/
int handle_nvmlDeviceGetPersistenceMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPersistenceMode(device, &mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the PCI attributes of this device.
*
* For all products.
*
* See \ref nvmlPciInfo_t for details on the available PCI info.
*
* @param device                               The identifier of the target device
* @param pci                                  Reference in which to return the PCI info
*
* @return
*         - \ref NVML_SUCCESS                 if \a pci has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pci is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPciInfo_v3(void *conn) {
    nvmlDevice_t device;
    nvmlPciInfo_t pci;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pci, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPciInfo_v3(device, &pci);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pci, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the maximum PCIe link generation possible with this device and system
*
* I.E. for a generation 2 PCIe device attached to a generation 1 PCIe bus the max link generation this function will
* report is generation 1.
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param maxLinkGen                           Reference in which to return the max PCIe link generation
*
* @return
*         - \ref NVML_SUCCESS                 if \a maxLinkGen has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a maxLinkGen is null
*         - \ref NVML_ERROR_NOT_SUPPORTED     if PCIe link information is not available
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMaxPcieLinkGeneration(void *conn) {
    nvmlDevice_t device;
    unsigned int maxLinkGen;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &maxLinkGen, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMaxPcieLinkGeneration(device, &maxLinkGen);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxLinkGen, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the maximum PCIe link generation supported by this device
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param maxLinkGenDevice                     Reference in which to return the max PCIe link generation
*
* @return
*         - \ref NVML_SUCCESS                 if \a maxLinkGenDevice has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a maxLinkGenDevice is null
*         - \ref NVML_ERROR_NOT_SUPPORTED     if PCIe link information is not available
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGpuMaxPcieLinkGeneration(void *conn) {
    nvmlDevice_t device;
    unsigned int maxLinkGenDevice;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &maxLinkGenDevice, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuMaxPcieLinkGeneration(device, &maxLinkGenDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxLinkGenDevice, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the maximum PCIe link width possible with this device and system
*
* I.E. for a device with a 16x PCIe bus width attached to a 8x PCIe system bus this function will report
* a max link width of 8.
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param maxLinkWidth                         Reference in which to return the max PCIe link generation
*
* @return
*         - \ref NVML_SUCCESS                 if \a maxLinkWidth has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a maxLinkWidth is null
*         - \ref NVML_ERROR_NOT_SUPPORTED     if PCIe link information is not available
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMaxPcieLinkWidth(void *conn) {
    nvmlDevice_t device;
    unsigned int maxLinkWidth;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &maxLinkWidth, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMaxPcieLinkWidth(device, &maxLinkWidth);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxLinkWidth, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current PCIe link generation
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param currLinkGen                          Reference in which to return the current PCIe link generation
*
* @return
*         - \ref NVML_SUCCESS                 if \a currLinkGen has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a currLinkGen is null
*         - \ref NVML_ERROR_NOT_SUPPORTED     if PCIe link information is not available
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetCurrPcieLinkGeneration(void *conn) {
    nvmlDevice_t device;
    unsigned int currLinkGen;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &currLinkGen, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetCurrPcieLinkGeneration(device, &currLinkGen);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &currLinkGen, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current PCIe link width
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param currLinkWidth                        Reference in which to return the current PCIe link generation
*
* @return
*         - \ref NVML_SUCCESS                 if \a currLinkWidth has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a currLinkWidth is null
*         - \ref NVML_ERROR_NOT_SUPPORTED     if PCIe link information is not available
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetCurrPcieLinkWidth(void *conn) {
    nvmlDevice_t device;
    unsigned int currLinkWidth;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &currLinkWidth, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetCurrPcieLinkWidth(device, &currLinkWidth);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &currLinkWidth, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve PCIe utilization information.
* This function is querying a byte counter over a 20ms interval and thus is the
*   PCIe throughput over that interval.
*
* For Maxwell &tm; or newer fully supported devices.
*
* This method is not supported in virtual machines running virtual GPU (vGPU).
*
* @param device                               The identifier of the target device
* @param counter                              The specific counter that should be queried \ref nvmlPcieUtilCounter_t
* @param value                                Reference in which to return throughput in KB/s
*
* @return
*         - \ref NVML_SUCCESS                 if \a value has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a counter is invalid, or \a value is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPcieThroughput(void *conn) {
    nvmlDevice_t device;
    nvmlPcieUtilCounter_t counter;
    unsigned int value;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &counter, sizeof(nvmlPcieUtilCounter_t)) < 0 ||
        rpc_read(conn, &value, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPcieThroughput(device, counter, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the PCIe replay counter.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param value                                Reference in which to return the counter's value
*
* @return
*         - \ref NVML_SUCCESS                 if \a value has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a value is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPcieReplayCounter(void *conn) {
    nvmlDevice_t device;
    unsigned int value;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &value, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPcieReplayCounter(device, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current clock speeds for the device.
*
* For Fermi &tm; or newer fully supported devices.
*
* See \ref nvmlClockType_t for details on available clock information.
*
* @param device                               The identifier of the target device
* @param type                                 Identify which clock domain to query
* @param clock                                Reference in which to return the clock speed in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clock has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clock is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device cannot report the specified clock
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetClockInfo(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t type;
    unsigned int clock;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clock, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetClockInfo(device, type, &clock);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clock, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the maximum clock speeds for the device.
*
* For Fermi &tm; or newer fully supported devices.
*
* See \ref nvmlClockType_t for details on available clock information.
*
* \note On GPUs from Fermi family current P0 clocks (reported by \ref nvmlDeviceGetClockInfo) can differ from max clocks
*       by few MHz.
*
* @param device                               The identifier of the target device
* @param type                                 Identify which clock domain to query
* @param clock                                Reference in which to return the clock speed in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clock has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clock is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device cannot report the specified clock
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMaxClockInfo(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t type;
    unsigned int clock;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clock, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMaxClockInfo(device, type, &clock);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clock, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current setting of a clock that applications will use unless an overspec situation occurs.
* Can be changed using \ref nvmlDeviceSetApplicationsClocks.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param clockType                            Identify which clock domain to query
* @param clockMHz                             Reference in which to return the clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clockMHz has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clockMHz is NULL or \a clockType is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetApplicationsClock(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t clockType;
    unsigned int clockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetApplicationsClock(device, clockType, &clockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the default applications clock that GPU boots with or
* defaults to after \ref nvmlDeviceResetApplicationsClocks call.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param clockType                            Identify which clock domain to query
* @param clockMHz                             Reference in which to return the default clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clockMHz has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clockMHz is NULL or \a clockType is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* \see nvmlDeviceGetApplicationsClock
*/
int handle_nvmlDeviceGetDefaultApplicationsClock(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t clockType;
    unsigned int clockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDefaultApplicationsClock(device, clockType, &clockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Resets the application clock to the default value
*
* This is the applications clock that will be used after system reboot or driver reload.
* Default value is constant, but the current value an be changed using \ref nvmlDeviceSetApplicationsClocks.
*
* On Pascal and newer hardware, if clocks were previously locked with \ref nvmlDeviceSetApplicationsClocks,
* this call will unlock clocks. This returns clocks their default behavior ofautomatically boosting above
* base clocks as thermal limits allow.
*
* @see nvmlDeviceGetApplicationsClock
* @see nvmlDeviceSetApplicationsClocks
*
* For Fermi &tm; or newer non-GeForce fully supported devices and Maxwell or newer GeForce devices.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceResetApplicationsClocks(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceResetApplicationsClocks(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the clock speed for the clock specified by the clock type and clock ID.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param clockType                            Identify which clock domain to query
* @param clockId                              Identify which clock in the domain to query
* @param clockMHz                             Reference in which to return the clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clockMHz has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clockMHz is NULL or \a clockType is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetClock(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t clockType;
    nvmlClockId_t clockId;
    unsigned int clockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clockId, sizeof(nvmlClockId_t)) < 0 ||
        rpc_read(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetClock(device, clockType, clockId, &clockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the customer defined maximum boost clock speed specified by the given clock type.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param clockType                            Identify which clock domain to query
* @param clockMHz                             Reference in which to return the clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a clockMHz has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clockMHz is NULL or \a clockType is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device or the \a clockType on this device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMaxCustomerBoostClock(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t clockType;
    unsigned int clockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMaxCustomerBoostClock(device, clockType, &clockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clockMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the list of possible memory clocks that can be used as an argument for \ref nvmlDeviceSetApplicationsClocks.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param count                                Reference in which to provide the \a clocksMHz array size, and
*                                             to return the number of elements
* @param clocksMHz                            Reference in which to return the clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a count and \a clocksMHz have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a count is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a count is too small (\a count is set to the number of
*                                                required elements)
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetApplicationsClocks
* @see nvmlDeviceGetSupportedGraphicsClocks
*/
int handle_nvmlDeviceGetSupportedMemoryClocks(void *conn) {
    nvmlDevice_t device;
    unsigned int count;
    unsigned int clocksMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &clocksMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetSupportedMemoryClocks(device, &count, &clocksMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &clocksMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the list of possible graphics clocks that can be used as an argument for \ref nvmlDeviceSetApplicationsClocks.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param memoryClockMHz                       Memory clock for which to return possible graphics clocks
* @param count                                Reference in which to provide the \a clocksMHz array size, and
*                                             to return the number of elements
* @param clocksMHz                            Reference in which to return the clocks in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if \a count and \a clocksMHz have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_NOT_FOUND         if the specified \a memoryClockMHz is not a supported frequency
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clock is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a count is too small
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetApplicationsClocks
* @see nvmlDeviceGetSupportedMemoryClocks
*/
int handle_nvmlDeviceGetSupportedGraphicsClocks(void *conn) {
    nvmlDevice_t device;
    unsigned int memoryClockMHz;
    unsigned int count;
    unsigned int clocksMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &memoryClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &clocksMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetSupportedGraphicsClocks(device, memoryClockMHz, &count, &clocksMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &clocksMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the current state of Auto Boosted clocks on a device and store it in \a isEnabled
*
* For Kepler &tm; or newer fully supported devices.
*
* Auto Boosted clocks are enabled by default on some hardware, allowing the GPU to run at higher clock rates
* to maximize performance as thermal limits allow.
*
* On Pascal and newer hardware, Auto Aoosted clocks are controlled through application clocks.
* Use \ref nvmlDeviceSetApplicationsClocks and \ref nvmlDeviceResetApplicationsClocks to control Auto Boost
* behavior.
*
* @param device                               The identifier of the target device
* @param isEnabled                            Where to store the current state of Auto Boosted clocks of the target device
* @param defaultIsEnabled                     Where to store the default Auto Boosted clocks behavior of the target device that the device will
*                                                 revert to when no applications are using the GPU
*
* @return
*         - \ref NVML_SUCCESS                 If \a isEnabled has been been set with the Auto Boosted clocks state of \a device
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a isEnabled is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support Auto Boosted clocks
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
*/
int handle_nvmlDeviceGetAutoBoostedClocksEnabled(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t isEnabled;
    nvmlEnableState_t defaultIsEnabled;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &defaultIsEnabled, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAutoBoostedClocksEnabled(device, &isEnabled, &defaultIsEnabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isEnabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_write(conn, &defaultIsEnabled, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Try to set the current state of Auto Boosted clocks on a device.
*
* For Kepler &tm; or newer fully supported devices.
*
* Auto Boosted clocks are enabled by default on some hardware, allowing the GPU to run at higher clock rates
* to maximize performance as thermal limits allow. Auto Boosted clocks should be disabled if fixed clock
* rates are desired.
*
* Non-root users may use this API by default but can be restricted by root from using this API by calling
* \ref nvmlDeviceSetAPIRestriction with apiType=NVML_RESTRICTED_API_SET_AUTO_BOOSTED_CLOCKS.
* Note: Persistence Mode is required to modify current Auto Boost settings, therefore, it must be enabled.
*
* On Pascal and newer hardware, Auto Boosted clocks are controlled through application clocks.
* Use \ref nvmlDeviceSetApplicationsClocks and \ref nvmlDeviceResetApplicationsClocks to control Auto Boost
* behavior.
*
* @param device                               The identifier of the target device
* @param enabled                              What state to try to set Auto Boosted clocks of the target device to
*
* @return
*         - \ref NVML_SUCCESS                 If the Auto Boosted clocks were successfully set to the state specified by \a enabled
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support Auto Boosted clocks
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
*/
int handle_nvmlDeviceSetAutoBoostedClocksEnabled(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t enabled;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &enabled, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetAutoBoostedClocksEnabled(device, enabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Try to set the default state of Auto Boosted clocks on a device. This is the default state that Auto Boosted clocks will
* return to when no compute running processes (e.g. CUDA application which have an active context) are running
*
* For Kepler &tm; or newer non-GeForce fully supported devices and Maxwell or newer GeForce devices.
* Requires root/admin permissions.
*
* Auto Boosted clocks are enabled by default on some hardware, allowing the GPU to run at higher clock rates
* to maximize performance as thermal limits allow. Auto Boosted clocks should be disabled if fixed clock
* rates are desired.
*
* On Pascal and newer hardware, Auto Boosted clocks are controlled through application clocks.
* Use \ref nvmlDeviceSetApplicationsClocks and \ref nvmlDeviceResetApplicationsClocks to control Auto Boost
* behavior.
*
* @param device                               The identifier of the target device
* @param enabled                              What state to try to set default Auto Boosted clocks of the target device to
* @param flags                                Flags that change the default behavior. Currently Unused.
*
* @return
*         - \ref NVML_SUCCESS                 If the Auto Boosted clock's default state was successfully set to the state specified by \a enabled
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_NO_PERMISSION     If the calling user does not have permission to change Auto Boosted clock's default state.
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support Auto Boosted clocks
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
*/
int handle_nvmlDeviceSetDefaultAutoBoostedClocksEnabled(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t enabled;
    unsigned int flags;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &enabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetDefaultAutoBoostedClocksEnabled(device, enabled, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the intended operating speed of the device's fan.
*
* Note: The reported speed is the intended fan speed.  If the fan is physically blocked and unable to spin, the
* output will not match the actual fan speed.
*
* For all discrete products with dedicated fans.
*
* The fan speed is expressed as a percentage of the product's maximum noise tolerance fan speed.
* This value may exceed 100% in certain cases.
*
* @param device                               The identifier of the target device
* @param speed                                Reference in which to return the fan speed percentage
*
* @return
*         - \ref NVML_SUCCESS                 if \a speed has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a speed is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have a fan
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetFanSpeed(void *conn) {
    nvmlDevice_t device;
    unsigned int speed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &speed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetFanSpeed(device, &speed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &speed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the intended operating speed of the device's specified fan.
*
* Note: The reported speed is the intended fan speed. If the fan is physically blocked and unable to spin, the
* output will not match the actual fan speed.
*
* For all discrete products with dedicated fans.
*
* The fan speed is expressed as a percentage of the product's maximum noise tolerance fan speed.
* This value may exceed 100% in certain cases.
*
* @param device                                The identifier of the target device
* @param fan                                   The index of the target fan, zero indexed.
* @param speed                                 Reference in which to return the fan speed percentage
*
* @return
*        - \ref NVML_SUCCESS                   if \a speed has been set
*        - \ref NVML_ERROR_UNINITIALIZED       if the library has not been successfully initialized
*        - \ref NVML_ERROR_INVALID_ARGUMENT    if \a device is invalid, \a fan is not an acceptable index, or \a speed is NULL
*        - \ref NVML_ERROR_NOT_SUPPORTED       if the device does not have a fan or is newer than Maxwell
*        - \ref NVML_ERROR_GPU_IS_LOST         if the target GPU has fallen off the bus or is otherwise inaccessible
*        - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetFanSpeed_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;
    unsigned int speed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &speed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetFanSpeed_v2(device, fan, &speed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &speed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the intended target speed of the device's specified fan.
*
* Normally, the driver dynamically adjusts the fan based on
* the needs of the GPU.  But when user set fan speed using nvmlDeviceSetFanSpeed_v2,
* the driver will attempt to make the fan achieve the setting in
* nvmlDeviceSetFanSpeed_v2.  The actual current speed of the fan
* is reported in nvmlDeviceGetFanSpeed_v2.
*
* For all discrete products with dedicated fans.
*
* The fan speed is expressed as a percentage of the product's maximum noise tolerance fan speed.
* This value may exceed 100% in certain cases.
*
* @param device                                The identifier of the target device
* @param fan                                   The index of the target fan, zero indexed.
* @param targetSpeed                           Reference in which to return the fan speed percentage
*
* @return
*        - \ref NVML_SUCCESS                   if \a speed has been set
*        - \ref NVML_ERROR_UNINITIALIZED       if the library has not been successfully initialized
*        - \ref NVML_ERROR_INVALID_ARGUMENT    if \a device is invalid, \a fan is not an acceptable index, or \a speed is NULL
*        - \ref NVML_ERROR_NOT_SUPPORTED       if the device does not have a fan or is newer than Maxwell
*        - \ref NVML_ERROR_GPU_IS_LOST         if the target GPU has fallen off the bus or is otherwise inaccessible
*        - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetTargetFanSpeed(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;
    unsigned int targetSpeed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &targetSpeed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTargetFanSpeed(device, fan, &targetSpeed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &targetSpeed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Sets the speed of the fan control policy to default.
*
* For all cuda-capable discrete products with fans
*
* @param device                        The identifier of the target device
* @param fan                           The index of the fan, starting at zero
*
* return
*         NVML_SUCCESS                 if speed has been adjusted
*         NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         NVML_ERROR_INVALID_ARGUMENT  if device is invalid
*         NVML_ERROR_NOT_SUPPORTED     if the device does not support this
*                                      (doesn't have fans)
*         NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetDefaultFanSpeed_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetDefaultFanSpeed_v2(device, fan);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the min and max fan speed that user can set for the GPU fan.
*
* For all cuda-capable discrete products with fans
*
* @param device                        The identifier of the target device
* @param minSpeed                      The minimum speed allowed to set
* @param maxSpeed                      The maximum speed allowed to set
*
* return
*         NVML_SUCCESS                 if speed has been adjusted
*         NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         NVML_ERROR_INVALID_ARGUMENT  if device is invalid
*         NVML_ERROR_NOT_SUPPORTED     if the device does not support this
*                                      (doesn't have fans)
*         NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMinMaxFanSpeed(void *conn) {
    nvmlDevice_t device;
    unsigned int minSpeed;
    unsigned int maxSpeed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &maxSpeed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMinMaxFanSpeed(device, &minSpeed, &maxSpeed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minSpeed, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &maxSpeed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets current fan control policy.
*
* For Maxwell &tm; or newer fully supported devices.
*
* For all cuda-capable discrete products with fans
*
* device                               The identifier of the target \a device
* policy                               Reference in which to return the fan control \a policy
*
* return
*         NVML_SUCCESS                 if \a policy has been populated
*         NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a policy is null or the \a fan given doesn't reference
*                                            a fan that exists.
*         NVML_ERROR_NOT_SUPPORTED     if the \a device is older than Maxwell
*         NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetFanControlPolicy_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;
    nvmlFanControlPolicy_t policy;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &policy, sizeof(nvmlFanControlPolicy_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetFanControlPolicy_v2(device, fan, &policy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &policy, sizeof(nvmlFanControlPolicy_t)) < 0)
        return -1;

    return result;
}

/**
* Sets current fan control policy.
*
* For Maxwell &tm; or newer fully supported devices.
*
* Requires privileged user.
*
* For all cuda-capable discrete products with fans
*
* device                               The identifier of the target \a device
* policy                               The fan control \a policy to set
*
* return
*         NVML_SUCCESS                 if \a policy has been set
*         NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a policy is null or the \a fan given doesn't reference
*                                            a fan that exists.
*         NVML_ERROR_NOT_SUPPORTED     if the \a device is older than Maxwell
*         NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetFanControlPolicy(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;
    nvmlFanControlPolicy_t policy;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &policy, sizeof(nvmlFanControlPolicy_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetFanControlPolicy(device, fan, policy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the number of fans on the device.
*
* For all discrete products with dedicated fans.
*
* @param device                               The identifier of the target device
* @param numFans                              The number of fans
*
* @return
*         - \ref NVML_SUCCESS                 if \a fan number query was successful
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a numFans is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have a fan
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNumFans(void *conn) {
    nvmlDevice_t device;
    unsigned int numFans;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &numFans, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNumFans(device, &numFans);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numFans, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current temperature readings for the device, in degrees C.
*
* For all products.
*
* See \ref nvmlTemperatureSensors_t for details on available temperature sensors.
*
* @param device                               The identifier of the target device
* @param sensorType                           Flag that indicates which sensor reading to retrieve
* @param temp                                 Reference in which to return the temperature reading
*
* @return
*         - \ref NVML_SUCCESS                 if \a temp has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a sensorType is invalid or \a temp is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have the specified sensor
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetTemperature(void *conn) {
    nvmlDevice_t device;
    nvmlTemperatureSensors_t sensorType;
    unsigned int temp;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &sensorType, sizeof(nvmlTemperatureSensors_t)) < 0 ||
        rpc_read(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTemperature(device, sensorType, &temp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the temperature threshold for the GPU with the specified threshold type in degrees C.
*
* For Kepler &tm; or newer fully supported devices.
*
* See \ref nvmlTemperatureThresholds_t for details on available temperature thresholds.
*
* @param device                               The identifier of the target device
* @param thresholdType                        The type of threshold value queried
* @param temp                                 Reference in which to return the temperature reading
* @return
*         - \ref NVML_SUCCESS                 if \a temp has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a thresholdType is invalid or \a temp is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have a temperature sensor or is unsupported
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetTemperatureThreshold(void *conn) {
    nvmlDevice_t device;
    nvmlTemperatureThresholds_t thresholdType;
    unsigned int temp;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &thresholdType, sizeof(nvmlTemperatureThresholds_t)) < 0 ||
        rpc_read(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTemperatureThreshold(device, thresholdType, &temp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &temp, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Sets the temperature threshold for the GPU with the specified threshold type in degrees C.
*
* For Maxwell &tm; or newer fully supported devices.
*
* See \ref nvmlTemperatureThresholds_t for details on available temperature thresholds.
*
* @param device                               The identifier of the target device
* @param thresholdType                        The type of threshold value to be set
* @param temp                                 Reference which hold the value to be set
* @return
*         - \ref NVML_SUCCESS                 if \a temp has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a thresholdType is invalid or \a temp is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not have a temperature sensor or is unsupported
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetTemperatureThreshold(void *conn) {
    nvmlDevice_t device;
    nvmlTemperatureThresholds_t thresholdType;
    int temp;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &thresholdType, sizeof(nvmlTemperatureThresholds_t)) < 0 ||
        rpc_read(conn, &temp, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetTemperatureThreshold(device, thresholdType, &temp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &temp, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Used to execute a list of thermal system instructions.
*
* @param device                               The identifier of the target device
* @param sensorIndex                          The index of the thermal sensor
* @param pThermalSettings                     Reference in which to return the thermal sensor information
*
* @return
*         - \ref NVML_SUCCESS                 if \a pThermalSettings has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pThermalSettings is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetThermalSettings(void *conn) {
    nvmlDevice_t device;
    unsigned int sensorIndex;
    nvmlGpuThermalSettings_t pThermalSettings;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &sensorIndex, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &pThermalSettings, sizeof(nvmlGpuThermalSettings_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetThermalSettings(device, sensorIndex, &pThermalSettings);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pThermalSettings, sizeof(nvmlGpuThermalSettings_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current performance state for the device.
*
* For Fermi &tm; or newer fully supported devices.
*
* See \ref nvmlPstates_t for details on allowed performance states.
*
* @param device                               The identifier of the target device
* @param pState                               Reference in which to return the performance state reading
*
* @return
*         - \ref NVML_SUCCESS                 if \a pState has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pState is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPerformanceState(void *conn) {
    nvmlDevice_t device;
    nvmlPstates_t pState;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pState, sizeof(nvmlPstates_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPerformanceState(device, &pState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pState, sizeof(nvmlPstates_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves current clocks throttling reasons.
*
* For all fully supported products.
*
* \note More than one bit can be enabled at the same time. Multiple reasons can be affecting clocks at once.
*
* @param device                                The identifier of the target device
* @param clocksThrottleReasons                 Reference in which to return bitmask of active clocks throttle
*                                                  reasons
*
* @return
*         - \ref NVML_SUCCESS                 if \a clocksThrottleReasons has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a clocksThrottleReasons is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlClocksThrottleReasons
* @see nvmlDeviceGetSupportedClocksThrottleReasons
*/
int handle_nvmlDeviceGetCurrentClocksThrottleReasons(void *conn) {
    nvmlDevice_t device;
    unsigned long long clocksThrottleReasons;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &clocksThrottleReasons, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetCurrentClocksThrottleReasons(device, &clocksThrottleReasons);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clocksThrottleReasons, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Retrieves bitmask of supported clocks throttle reasons that can be returned by
* \ref nvmlDeviceGetCurrentClocksThrottleReasons
*
* For all fully supported products.
*
* This method is not supported in virtual machines running virtual GPU (vGPU).
*
* @param device                               The identifier of the target device
* @param supportedClocksThrottleReasons       Reference in which to return bitmask of supported
*                                              clocks throttle reasons
*
* @return
*         - \ref NVML_SUCCESS                 if \a supportedClocksThrottleReasons has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a supportedClocksThrottleReasons is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlClocksThrottleReasons
* @see nvmlDeviceGetCurrentClocksThrottleReasons
*/
int handle_nvmlDeviceGetSupportedClocksThrottleReasons(void *conn) {
    nvmlDevice_t device;
    unsigned long long supportedClocksThrottleReasons;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &supportedClocksThrottleReasons, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetSupportedClocksThrottleReasons(device, &supportedClocksThrottleReasons);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &supportedClocksThrottleReasons, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Use \ref nvmlDeviceGetPerformanceState. This function exposes an incorrect generalization.
*
* Retrieve the current performance state for the device.
*
* For Fermi &tm; or newer fully supported devices.
*
* See \ref nvmlPstates_t for details on allowed performance states.
*
* @param device                               The identifier of the target device
* @param pState                               Reference in which to return the performance state reading
*
* @return
*         - \ref NVML_SUCCESS                 if \a pState has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pState is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPowerState(void *conn) {
    nvmlDevice_t device;
    nvmlPstates_t pState;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pState, sizeof(nvmlPstates_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerState(device, &pState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pState, sizeof(nvmlPstates_t)) < 0)
        return -1;

    return result;
}

/**
* This API has been deprecated.
*
* Retrieves the power management mode associated with this device.
*
* For products from the Fermi family.
*     - Requires \a NVML_INFOROM_POWER version 3.0 or higher.
*
* For from the Kepler or newer families.
*     - Does not require \a NVML_INFOROM_POWER object.
*
* This flag indicates whether any power management algorithm is currently active on the device. An
* enabled state does not necessarily mean the device is being actively throttled -- only that
* that the driver will do so if the appropriate conditions are met.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param mode                                 Reference in which to return the current power management mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a mode has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPowerManagementMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerManagementMode(device, &mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the power management limit associated with this device.
*
* For Fermi &tm; or newer fully supported devices.
*
* The power limit defines the upper boundary for the card's power draw. If
* the card's total power draw reaches this limit the power management algorithm kicks in.
*
* This reading is only available if power management mode is supported.
* See \ref nvmlDeviceGetPowerManagementMode.
*
* @param device                               The identifier of the target device
* @param limit                                Reference in which to return the power management limit in milliwatts
*
* @return
*         - \ref NVML_SUCCESS                 if \a limit has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a limit is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPowerManagementLimit(void *conn) {
    nvmlDevice_t device;
    unsigned int limit;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerManagementLimit(device, &limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &limit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves information about possible values of power management limits on this device.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param minLimit                             Reference in which to return the minimum power management limit in milliwatts
* @param maxLimit                             Reference in which to return the maximum power management limit in milliwatts
*
* @return
*         - \ref NVML_SUCCESS                 if \a minLimit and \a maxLimit have been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a minLimit or \a maxLimit is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetPowerManagementLimit
*/
int handle_nvmlDeviceGetPowerManagementLimitConstraints(void *conn) {
    nvmlDevice_t device;
    unsigned int minLimit;
    unsigned int maxLimit;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &maxLimit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerManagementLimitConstraints(device, &minLimit, &maxLimit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minLimit, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &maxLimit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves default power management limit on this device, in milliwatts.
* Default power management limit is a power management limit that the device boots with.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param defaultLimit                         Reference in which to return the default power management limit in milliwatts
*
* @return
*         - \ref NVML_SUCCESS                 if \a defaultLimit has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a defaultLimit is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPowerManagementDefaultLimit(void *conn) {
    nvmlDevice_t device;
    unsigned int defaultLimit;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &defaultLimit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerManagementDefaultLimit(device, &defaultLimit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &defaultLimit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves power usage for this GPU in milliwatts and its associated circuitry (e.g. memory)
*
* For Fermi &tm; or newer fully supported devices.
*
* On Fermi and Kepler GPUs the reading is accurate to within +/- 5% of current power draw.
*
* It is only available if power management mode is supported. See \ref nvmlDeviceGetPowerManagementMode.
*
* @param device                               The identifier of the target device
* @param power                                Reference in which to return the power usage information
*
* @return
*         - \ref NVML_SUCCESS                 if \a power has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a power is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support power readings
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPowerUsage(void *conn) {
    nvmlDevice_t device;
    unsigned int power;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &power, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerUsage(device, &power);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &power, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves total energy consumption for this GPU in millijoules (mJ) since the driver was last reloaded
*
* For Volta &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param energy                               Reference in which to return the energy consumption information
*
* @return
*         - \ref NVML_SUCCESS                 if \a energy has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a energy is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support energy readings
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetTotalEnergyConsumption(void *conn) {
    nvmlDevice_t device;
    unsigned long long energy;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &energy, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTotalEnergyConsumption(device, &energy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &energy, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Get the effective power limit that the driver enforces after taking into account all limiters
*
* Note: This can be different from the \ref nvmlDeviceGetPowerManagementLimit if other limits are set elsewhere
* This includes the out of band power limit interface
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                           The device to communicate with
* @param limit                            Reference in which to return the power management limit in milliwatts
*
* @return
*         - \ref NVML_SUCCESS                 if \a limit has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a limit is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetEnforcedPowerLimit(void *conn) {
    nvmlDevice_t device;
    unsigned int limit;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetEnforcedPowerLimit(device, &limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &limit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current GOM and pending GOM (the one that GPU will switch to after reboot).
*
* For GK110 M-class and X-class Tesla &tm; products from the Kepler family.
* Modes \ref NVML_GOM_LOW_DP and \ref NVML_GOM_ALL_ON are supported on fully supported GeForce products.
* Not supported on Quadro &reg; and Tesla &tm; C-class products.
*
* @param device                               The identifier of the target device
* @param current                              Reference in which to return the current GOM
* @param pending                              Reference in which to return the pending GOM
*
* @return
*         - \ref NVML_SUCCESS                 if \a mode has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a current or \a pending is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlGpuOperationMode_t
* @see nvmlDeviceSetGpuOperationMode
*/
int handle_nvmlDeviceGetGpuOperationMode(void *conn) {
    nvmlDevice_t device;
    nvmlGpuOperationMode_t current;
    nvmlGpuOperationMode_t pending;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &current, sizeof(nvmlGpuOperationMode_t)) < 0 ||
        rpc_read(conn, &pending, sizeof(nvmlGpuOperationMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuOperationMode(device, &current, &pending);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &current, sizeof(nvmlGpuOperationMode_t)) < 0 ||
        rpc_write(conn, &pending, sizeof(nvmlGpuOperationMode_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the amount of used, free, reserved and total memory available on the device, in bytes.
* The reserved amount is supported on version 2 only.
*
* For all products.
*
* Enabling ECC reduces the amount of total available memory, due to the extra required parity bits.
* Under WDDM most device memory is allocated and managed on startup by Windows.
*
* Under Linux and Windows TCC, the reported amount of used memory is equal to the sum of memory allocated
* by all active channels on the device.
*
* See \ref nvmlMemory_v2_t for details on available memory info.
*
* @note In MIG mode, if device handle is provided, the API returns aggregate
*       information, only if the caller has appropriate privileges. Per-instance
*       information can be queried by using specific MIG device handles.
*
* @note nvmlDeviceGetMemoryInfo_v2 adds additional memory information.
*
* @param device                               The identifier of the target device
* @param memory                               Reference in which to return the memory information
*
* @return
*         - \ref NVML_SUCCESS                 if \a memory has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a memory is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMemoryInfo(void *conn) {
    nvmlDevice_t device;
    nvmlMemory_t memory;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &memory, sizeof(nvmlMemory_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemoryInfo(device, &memory);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memory, sizeof(nvmlMemory_t)) < 0)
        return -1;

    return result;
}

int handle_nvmlDeviceGetMemoryInfo_v2(void *conn) {
    nvmlDevice_t device;
    nvmlMemory_v2_t memory;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &memory, sizeof(nvmlMemory_v2_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemoryInfo_v2(device, &memory);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memory, sizeof(nvmlMemory_v2_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current compute mode for the device.
*
* For all products.
*
* See \ref nvmlComputeMode_t for details on allowed compute modes.
*
* @param device                               The identifier of the target device
* @param mode                                 Reference in which to return the current compute mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a mode has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetComputeMode()
*/
int handle_nvmlDeviceGetComputeMode(void *conn) {
    nvmlDevice_t device;
    nvmlComputeMode_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlComputeMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetComputeMode(device, &mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(nvmlComputeMode_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the CUDA compute capability of the device.
*
* For all products.
*
* Returns the major and minor compute capability version numbers of the
* device.  The major and minor versions are equivalent to the
* CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR and
* CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR attributes that would be
* returned by CUDA's cuDeviceGetAttribute().
*
* @param device                               The identifier of the target device
* @param major                                Reference in which to return the major CUDA compute capability
* @param minor                                Reference in which to return the minor CUDA compute capability
*
* @return
*         - \ref NVML_SUCCESS                 if \a major and \a minor have been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a major or \a minor are NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetCudaComputeCapability(void *conn) {
    nvmlDevice_t device;
    int major;
    int minor;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &major, sizeof(int)) < 0 ||
        rpc_read(conn, &minor, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetCudaComputeCapability(device, &major, &minor);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &major, sizeof(int)) < 0 ||
        rpc_write(conn, &minor, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current and pending ECC modes for the device.
*
* For Fermi &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher.
*
* Changing ECC modes requires a reboot. The "pending" ECC mode refers to the target mode following
* the next reboot.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param current                              Reference in which to return the current ECC mode
* @param pending                              Reference in which to return the pending ECC mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a current and \a pending have been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or either \a current or \a pending is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetEccMode()
*/
int handle_nvmlDeviceGetEccMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t current;
    nvmlEnableState_t pending;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &current, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &pending, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetEccMode(device, &current, &pending);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &current, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_write(conn, &pending, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the default ECC modes for the device.
*
* For Fermi &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher.
*
* See \ref nvmlEnableState_t for details on allowed modes.
*
* @param device                               The identifier of the target device
* @param defaultMode                          Reference in which to return the default ECC mode
*
* @return
*         - \ref NVML_SUCCESS                 if \a current and \a pending have been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a default is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetEccMode()
*/
int handle_nvmlDeviceGetDefaultEccMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t defaultMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &defaultMode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDefaultEccMode(device, &defaultMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &defaultMode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the device boardId from 0-N.
* Devices with the same boardId indicate GPUs connected to the same PLX.  Use in conjunction with
*  \ref nvmlDeviceGetMultiGpuBoard() to decide if they are on the same board as well.
*  The boardId returned is a unique ID for the current configuration.  Uniqueness and ordering across
*  reboots and system configurations is not guaranteed (i.e. if a Tesla K40c returns 0x100 and
*  the two GPUs on a Tesla K10 in the same system returns 0x200 it is not guaranteed they will
*  always return those values but they will always be different from each other).
*
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param boardId                              Reference in which to return the device's board ID
*
* @return
*         - \ref NVML_SUCCESS                 if \a boardId has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a boardId is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetBoardId(void *conn) {
    nvmlDevice_t device;
    unsigned int boardId;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &boardId, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetBoardId(device, &boardId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &boardId, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves whether the device is on a Multi-GPU Board
* Devices that are on multi-GPU boards will set \a multiGpuBool to a non-zero value.
*
* For Fermi &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param multiGpuBool                         Reference in which to return a zero or non-zero value
*                                                 to indicate whether the device is on a multi GPU board
*
* @return
*         - \ref NVML_SUCCESS                 if \a multiGpuBool has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a multiGpuBool is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMultiGpuBoard(void *conn) {
    nvmlDevice_t device;
    unsigned int multiGpuBool;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &multiGpuBool, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMultiGpuBoard(device, &multiGpuBool);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &multiGpuBool, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the total ECC error counts for the device.
*
* For Fermi &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher.
* Requires ECC Mode to be enabled.
*
* The total error count is the sum of errors across each of the separate memory systems, i.e. the total set of
* errors across the entire device.
*
* See \ref nvmlMemoryErrorType_t for a description of available error types.\n
* See \ref nvmlEccCounterType_t for a description of available counter types.
*
* @param device                               The identifier of the target device
* @param errorType                            Flag that specifies the type of the errors.
* @param counterType                          Flag that specifies the counter-type of the errors.
* @param eccCounts                            Reference in which to return the specified ECC errors
*
* @return
*         - \ref NVML_SUCCESS                 if \a eccCounts has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a errorType or \a counterType is invalid, or \a eccCounts is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceClearEccErrorCounts()
*/
int handle_nvmlDeviceGetTotalEccErrors(void *conn) {
    nvmlDevice_t device;
    nvmlMemoryErrorType_t errorType;
    nvmlEccCounterType_t counterType;
    unsigned long long eccCounts;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_read(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_read(conn, &eccCounts, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetTotalEccErrors(device, errorType, counterType, &eccCounts);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &eccCounts, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the detailed ECC error counts for the device.
*
* @deprecated   This API supports only a fixed set of ECC error locations
*               On different GPU architectures different locations are supported
*               See \ref nvmlDeviceGetMemoryErrorCounter
*
* For Fermi &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 2.0 or higher to report aggregate location-based ECC counts.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher to report all other ECC counts.
* Requires ECC Mode to be enabled.
*
* Detailed errors provide separate ECC counts for specific parts of the memory system.
*
* Reports zero for unsupported ECC error counters when a subset of ECC error counters are supported.
*
* See \ref nvmlMemoryErrorType_t for a description of available bit types.\n
* See \ref nvmlEccCounterType_t for a description of available counter types.\n
* See \ref nvmlEccErrorCounts_t for a description of provided detailed ECC counts.
*
* @param device                               The identifier of the target device
* @param errorType                            Flag that specifies the type of the errors.
* @param counterType                          Flag that specifies the counter-type of the errors.
* @param eccCounts                            Reference in which to return the specified ECC errors
*
* @return
*         - \ref NVML_SUCCESS                 if \a eccCounts has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a errorType or \a counterType is invalid, or \a eccCounts is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceClearEccErrorCounts()
*/
int handle_nvmlDeviceGetDetailedEccErrors(void *conn) {
    nvmlDevice_t device;
    nvmlMemoryErrorType_t errorType;
    nvmlEccCounterType_t counterType;
    nvmlEccErrorCounts_t eccCounts;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_read(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_read(conn, &eccCounts, sizeof(nvmlEccErrorCounts_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDetailedEccErrors(device, errorType, counterType, &eccCounts);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &eccCounts, sizeof(nvmlEccErrorCounts_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the requested memory error counter for the device.
*
* For Fermi &tm; or newer fully supported devices.
* Requires \a NVML_INFOROM_ECC version 2.0 or higher to report aggregate location-based memory error counts.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher to report all other memory error counts.
*
* Only applicable to devices with ECC.
*
* Requires ECC Mode to be enabled.
*
* @note On MIG-enabled GPUs, per instance information can be queried using specific
*       MIG device handles. Per instance information is currently only supported for
*       non-DRAM uncorrectable volatile errors. Querying volatile errors using device
*       handles is currently not supported.
*
* See \ref nvmlMemoryErrorType_t for a description of available memory error types.\n
* See \ref nvmlEccCounterType_t for a description of available counter types.\n
* See \ref nvmlMemoryLocation_t for a description of available counter locations.\n
*
* @param device                               The identifier of the target device
* @param errorType                            Flag that specifies the type of error.
* @param counterType                          Flag that specifies the counter-type of the errors.
* @param locationType                         Specifies the location of the counter.
* @param count                                Reference in which to return the ECC counter
*
* @return
*         - \ref NVML_SUCCESS                 if \a count has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a bitTyp,e \a counterType or \a locationType is
*                                             invalid, or \a count is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support ECC error reporting in the specified memory
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMemoryErrorCounter(void *conn) {
    nvmlDevice_t device;
    nvmlMemoryErrorType_t errorType;
    nvmlEccCounterType_t counterType;
    nvmlMemoryLocation_t locationType;
    unsigned long long count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_read(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_read(conn, &locationType, sizeof(nvmlMemoryLocation_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemoryErrorCounter(device, errorType, counterType, locationType, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current utilization rates for the device's major subsystems.
*
* For Fermi &tm; or newer fully supported devices.
*
* See \ref nvmlUtilization_t for details on available utilization rates.
*
* \note During driver initialization when ECC is enabled one can see high GPU and Memory Utilization readings.
*       This is caused by ECC Memory Scrubbing mechanism that is performed during driver initialization.
*
* @note On MIG-enabled GPUs, querying device utilization rates is not currently supported.
*
* @param device                               The identifier of the target device
* @param utilization                          Reference in which to return the utilization information
*
* @return
*         - \ref NVML_SUCCESS                 if \a utilization has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a utilization is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetUtilizationRates(void *conn) {
    nvmlDevice_t device;
    nvmlUtilization_t utilization;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &utilization, sizeof(nvmlUtilization_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetUtilizationRates(device, &utilization);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &utilization, sizeof(nvmlUtilization_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current utilization and sampling size in microseconds for the Encoder
*
* For Kepler &tm; or newer fully supported devices.
*
* @note On MIG-enabled GPUs, querying encoder utilization is not currently supported.
*
* @param device                               The identifier of the target device
* @param utilization                          Reference to an unsigned int for encoder utilization info
* @param samplingPeriodUs                     Reference to an unsigned int for the sampling period in US
*
* @return
*         - \ref NVML_SUCCESS                 if \a utilization has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a utilization is NULL, or \a samplingPeriodUs is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetEncoderUtilization(void *conn) {
    nvmlDevice_t device;
    unsigned int utilization;
    unsigned int samplingPeriodUs;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &samplingPeriodUs, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetEncoderUtilization(device, &utilization, &samplingPeriodUs);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &utilization, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &samplingPeriodUs, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current capacity of the device's encoder, as a percentage of maximum encoder capacity with valid values in the range 0-100.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param encoderQueryType                  Type of encoder to query
* @param encoderCapacity                   Reference to an unsigned int for the encoder capacity
*
* @return
*         - \ref NVML_SUCCESS                  if \a encoderCapacity is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a encoderCapacity is NULL, or \a device or \a encoderQueryType
*                                              are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED      if device does not support the encoder specified in \a encodeQueryType
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetEncoderCapacity(void *conn) {
    nvmlDevice_t device;
    nvmlEncoderType_t encoderQueryType;
    unsigned int encoderCapacity;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &encoderQueryType, sizeof(nvmlEncoderType_t)) < 0 ||
        rpc_read(conn, &encoderCapacity, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetEncoderCapacity(device, encoderQueryType, &encoderCapacity);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &encoderCapacity, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current encoder statistics for a given device.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param sessionCount                      Reference to an unsigned int for count of active encoder sessions
* @param averageFps                        Reference to an unsigned int for trailing average FPS of all active sessions
* @param averageLatency                    Reference to an unsigned int for encode latency in microseconds
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionCount, \a averageFps and \a averageLatency is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a sessionCount, or \a device or \a averageFps,
*                                              or \a averageLatency is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetEncoderStats(void *conn) {
    nvmlDevice_t device;
    unsigned int sessionCount;
    unsigned int averageFps;
    unsigned int averageLatency;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &averageFps, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &averageLatency, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetEncoderStats(device, &sessionCount, &averageFps, &averageLatency);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &averageFps, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &averageLatency, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves information about active encoder sessions on a target device.
*
* An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \a sessionInfos. The
* array elememt count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
* written to the buffer.
*
* If the supplied buffer is not large enough to accommodate the active session array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \a sessionCount.
* To query the number of active encoder sessions, call this function with *sessionCount = 0.  The code will return
* NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param sessionCount                      Reference to caller supplied array size, and returns the number of sessions.
* @param sessionInfos                      Reference in which to return the session information
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionInfos is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE  if \a sessionCount is too small, array element count is returned in \a sessionCount
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a sessionCount is NULL.
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_SUPPORTED      if this query is not supported by \a device
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetEncoderSessions(void *conn) {
    nvmlDevice_t device;
    unsigned int sessionCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlEncoderSessionInfo_t* sessionInfos = (nvmlEncoderSessionInfo_t*)malloc(sessionCount * sizeof(nvmlEncoderSessionInfo_t));

    nvmlReturn_t result = nvmlDeviceGetEncoderSessions(device, &sessionCount, sessionInfos);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, sessionInfos, sessionCount * sizeof(nvmlEncoderSessionInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current utilization and sampling size in microseconds for the Decoder
*
* For Kepler &tm; or newer fully supported devices.
*
* @note On MIG-enabled GPUs, querying decoder utilization is not currently supported.
*
* @param device                               The identifier of the target device
* @param utilization                          Reference to an unsigned int for decoder utilization info
* @param samplingPeriodUs                     Reference to an unsigned int for the sampling period in US
*
* @return
*         - \ref NVML_SUCCESS                 if \a utilization has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a utilization is NULL, or \a samplingPeriodUs is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetDecoderUtilization(void *conn) {
    nvmlDevice_t device;
    unsigned int utilization;
    unsigned int samplingPeriodUs;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &samplingPeriodUs, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDecoderUtilization(device, &utilization, &samplingPeriodUs);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &utilization, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &samplingPeriodUs, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the active frame buffer capture sessions statistics for a given device.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats
*
* @return
*         - \ref NVML_SUCCESS                  if \a fbcStats is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a fbcStats is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetFBCStats(void *conn) {
    nvmlDevice_t device;
    nvmlFBCStats_t fbcStats;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fbcStats, sizeof(nvmlFBCStats_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetFBCStats(device, &fbcStats);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fbcStats, sizeof(nvmlFBCStats_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves information about active frame buffer capture sessions on a target device.
*
* An array of active FBC sessions is returned in the caller-supplied buffer pointed at by \a sessionInfo. The
* array element count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
* written to the buffer.
*
* If the supplied buffer is not large enough to accomodate the active session array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlFBCSessionInfo_t array required in \a sessionCount.
* To query the number of active FBC sessions, call this function with *sessionCount = 0.  The code will return
* NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @note hResolution, vResolution, averageFPS and averageLatency data for a FBC session returned in \a sessionInfo may
*       be zero if there are no new frames captured since the session started.
*
* @param device                            The identifier of the target device
* @param sessionCount                      Reference to caller supplied array size, and returns the number of sessions.
* @param sessionInfo                       Reference in which to return the session information
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionInfo is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE  if \a sessionCount is too small, array element count is returned in \a sessionCount
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a sessionCount is NULL.
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetFBCSessions(void *conn) {
    nvmlDevice_t device;
    unsigned int sessionCount;
    nvmlFBCSessionInfo_t sessionInfo;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &sessionInfo, sizeof(nvmlFBCSessionInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetFBCSessions(device, &sessionCount, &sessionInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &sessionInfo, sizeof(nvmlFBCSessionInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current and pending driver model for the device.
*
* For Fermi &tm; or newer fully supported devices.
* For windows only.
*
* On Windows platforms the device driver can run in either WDDM or WDM (TCC) mode. If a display is attached
* to the device it must run in WDDM mode. TCC mode is preferred if a display is not attached.
*
* See \ref nvmlDriverModel_t for details on available driver models.
*
* @param device                               The identifier of the target device
* @param current                              Reference in which to return the current driver model
* @param pending                              Reference in which to return the pending driver model
*
* @return
*         - \ref NVML_SUCCESS                 if either \a current and/or \a pending have been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or both \a current and \a pending are NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the platform is not windows
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceSetDriverModel()
*/
int handle_nvmlDeviceGetDriverModel(void *conn) {
    nvmlDevice_t device;
    nvmlDriverModel_t current;
    nvmlDriverModel_t pending;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &current, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_read(conn, &pending, sizeof(nvmlDriverModel_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDriverModel(device, &current, &pending);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &current, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_write(conn, &pending, sizeof(nvmlDriverModel_t)) < 0)
        return -1;

    return result;
}

/**
* Get VBIOS version of the device.
*
* For all products.
*
* The VBIOS version may change from time to time. It will not exceed 32 characters in length
* (including the NULL terminator).  See \ref nvmlConstants::NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE.
*
* @param device                               The identifier of the target device
* @param version                              Reference to which to return the VBIOS version
* @param length                               The maximum allowed length of the string returned in \a version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a version is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetVbiosVersion(void *conn) {
    nvmlDevice_t device;
    unsigned int length;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetVbiosVersion(device, version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Get Bridge Chip Information for all the bridge chips on the board.
*
* For all fully supported products.
* Only applicable to multi-GPU products.
*
* @param device                                The identifier of the target device
* @param bridgeHierarchy                       Reference to the returned bridge chip Hierarchy
*
* @return
*         - \ref NVML_SUCCESS                 if bridge chip exists
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a bridgeInfo is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if bridge chip not supported on the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
*/
int handle_nvmlDeviceGetBridgeChipInfo(void *conn) {
    nvmlDevice_t device;
    nvmlBridgeChipHierarchy_t bridgeHierarchy;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &bridgeHierarchy, sizeof(nvmlBridgeChipHierarchy_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetBridgeChipInfo(device, &bridgeHierarchy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &bridgeHierarchy, sizeof(nvmlBridgeChipHierarchy_t)) < 0)
        return -1;

    return result;
}

/**
* Get information about processes with a compute context on a device
*
* For Fermi &tm; or newer fully supported devices.
*
* This function returns information only about compute running processes (e.g. CUDA application which have
* active context). Any graphics applications (e.g. using OpenGL, DirectX) won't be listed by this function.
*
* To query the current number of running compute processes, call this function with *infoCount = 0. The
* return code will be NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if none are running. For this call
* \a infos is allowed to be NULL.
*
* The usedGpuMemory field returned is all of the memory used by the application.
*
* Keep in mind that information returned by this call is dynamic and the number of elements might change in
* time. Allocate more space for \a infos table in case new compute processes are spawned.
*
* @note In MIG mode, if device handle is provided, the API returns aggregate information, only if
*       the caller has appropriate privileges. Per-instance information can be queried by using
*       specific MIG device handles.
*       Querying per-instance information using MIG device handles is not supported if the device is in vGPU Host virtualization mode.
*
* @param device                               The device handle or MIG device handle
* @param infoCount                            Reference in which to provide the \a infos array size, and
*                                             to return the number of returned elements
* @param infos                                Reference in which to return the process information
*
* @return
*         - \ref NVML_SUCCESS                 if \a infoCount and \a infos have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a infoCount indicates that the \a infos array is too small
*                                             \a infoCount will contain minimal amount of space necessary for
*                                             the call to complete
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, either of \a infoCount or \a infos is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by \a device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see \ref nvmlSystemGetProcessName
*/
int handle_nvmlDeviceGetComputeRunningProcesses_v3(void *conn) {
    nvmlDevice_t device;
    unsigned int infoCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &infoCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlProcessInfo_t* infos = (nvmlProcessInfo_t*)malloc(infoCount * sizeof(nvmlProcessInfo_t));

    nvmlReturn_t result = nvmlDeviceGetComputeRunningProcesses_v3(device, &infoCount, infos);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &infoCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, infos, infoCount * sizeof(nvmlProcessInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Get information about processes with a graphics context on a device
*
* For Kepler &tm; or newer fully supported devices.
*
* This function returns information only about graphics based processes
* (eg. applications using OpenGL, DirectX)
*
* To query the current number of running graphics processes, call this function with *infoCount = 0. The
* return code will be NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if none are running. For this call
* \a infos is allowed to be NULL.
*
* The usedGpuMemory field returned is all of the memory used by the application.
*
* Keep in mind that information returned by this call is dynamic and the number of elements might change in
* time. Allocate more space for \a infos table in case new graphics processes are spawned.
*
* @note In MIG mode, if device handle is provided, the API returns aggregate information, only if
*       the caller has appropriate privileges. Per-instance information can be queried by using
*       specific MIG device handles.
*       Querying per-instance information using MIG device handles is not supported if the device is in vGPU Host virtualization mode.
*
* @param device                               The device handle or MIG device handle
* @param infoCount                            Reference in which to provide the \a infos array size, and
*                                             to return the number of returned elements
* @param infos                                Reference in which to return the process information
*
* @return
*         - \ref NVML_SUCCESS                 if \a infoCount and \a infos have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a infoCount indicates that the \a infos array is too small
*                                             \a infoCount will contain minimal amount of space necessary for
*                                             the call to complete
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, either of \a infoCount or \a infos is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by \a device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see \ref nvmlSystemGetProcessName
*/
int handle_nvmlDeviceGetGraphicsRunningProcesses_v3(void *conn) {
    nvmlDevice_t device;
    unsigned int infoCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &infoCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlProcessInfo_t* infos = (nvmlProcessInfo_t*)malloc(infoCount * sizeof(nvmlProcessInfo_t));

    nvmlReturn_t result = nvmlDeviceGetGraphicsRunningProcesses_v3(device, &infoCount, infos);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &infoCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, infos, infoCount * sizeof(nvmlProcessInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Get information about processes with a MPS compute context on a device
*
* For Volta &tm; or newer fully supported devices.
*
* This function returns information only about compute running processes (e.g. CUDA application which have
* active context) utilizing MPS. Any graphics applications (e.g. using OpenGL, DirectX) won't be listed by
* this function.
*
* To query the current number of running compute processes, call this function with *infoCount = 0. The
* return code will be NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if none are running. For this call
* \a infos is allowed to be NULL.
*
* The usedGpuMemory field returned is all of the memory used by the application.
*
* Keep in mind that information returned by this call is dynamic and the number of elements might change in
* time. Allocate more space for \a infos table in case new compute processes are spawned.
*
* @note In MIG mode, if device handle is provided, the API returns aggregate information, only if
*       the caller has appropriate privileges. Per-instance information can be queried by using
*       specific MIG device handles.
*       Querying per-instance information using MIG device handles is not supported if the device is in vGPU Host virtualization mode.
*
* @param device                               The device handle or MIG device handle
* @param infoCount                            Reference in which to provide the \a infos array size, and
*                                             to return the number of returned elements
* @param infos                                Reference in which to return the process information
*
* @return
*         - \ref NVML_SUCCESS                 if \a infoCount and \a infos have been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a infoCount indicates that the \a infos array is too small
*                                             \a infoCount will contain minimal amount of space necessary for
*                                             the call to complete
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, either of \a infoCount or \a infos is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by \a device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see \ref nvmlSystemGetProcessName
*/
int handle_nvmlDeviceGetMPSComputeRunningProcesses_v3(void *conn) {
    nvmlDevice_t device;
    unsigned int infoCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &infoCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlProcessInfo_t* infos = (nvmlProcessInfo_t*)malloc(infoCount * sizeof(nvmlProcessInfo_t));

    nvmlReturn_t result = nvmlDeviceGetMPSComputeRunningProcesses_v3(device, &infoCount, infos);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &infoCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, infos, infoCount * sizeof(nvmlProcessInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Check if the GPU devices are on the same physical board.
*
* For all fully supported products.
*
* @param device1                               The first GPU device
* @param device2                               The second GPU device
* @param onSameBoard                           Reference in which to return the status.
*                                              Non-zero indicates that the GPUs are on the same board.
*
* @return
*         - \ref NVML_SUCCESS                 if \a onSameBoard has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a dev1 or \a dev2 are invalid or \a onSameBoard is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this check is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the either GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceOnSameBoard(void *conn) {
    nvmlDevice_t device1;
    nvmlDevice_t device2;
    int onSameBoard;

    if (rpc_read(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &onSameBoard, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceOnSameBoard(device1, device2, &onSameBoard);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &onSameBoard, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the root/admin permissions on the target API. See \a nvmlRestrictedAPI_t for the list of supported APIs.
* If an API is restricted only root users can call that API. See \a nvmlDeviceSetAPIRestriction to change current permissions.
*
* For all fully supported products.
*
* @param device                               The identifier of the target device
* @param apiType                              Target API type for this operation
* @param isRestricted                         Reference in which to return the current restriction
*                                             NVML_FEATURE_ENABLED indicates that the API is root-only
*                                             NVML_FEATURE_DISABLED indicates that the API is accessible to all users
*
* @return
*         - \ref NVML_SUCCESS                 if \a isRestricted has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a apiType incorrect or \a isRestricted is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device or the device does not support
*                                                 the feature that is being queried (E.G. Enabling/disabling Auto Boosted clocks is
*                                                 not supported by the device)
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlRestrictedAPI_t
*/
int handle_nvmlDeviceGetAPIRestriction(void *conn) {
    nvmlDevice_t device;
    nvmlRestrictedAPI_t apiType;
    nvmlEnableState_t isRestricted;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &apiType, sizeof(nvmlRestrictedAPI_t)) < 0 ||
        rpc_read(conn, &isRestricted, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAPIRestriction(device, apiType, &isRestricted);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isRestricted, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Gets recent samples for the GPU.
*
* For Kepler &tm; or newer fully supported devices.
*
* Based on type, this method can be used to fetch the power, utilization or clock samples maintained in the buffer by
* the driver.
*
* Power, Utilization and Clock samples are returned as type "unsigned int" for the union nvmlValue_t.
*
* To get the size of samples that user needs to allocate, the method is invoked with samples set to NULL.
* The returned samplesCount will provide the number of samples that can be queried. The user needs to
* allocate the buffer with size as samplesCount * sizeof(nvmlSample_t).
*
* lastSeenTimeStamp represents CPU timestamp in microseconds. Set it to 0 to fetch all the samples maintained by the
* underlying buffer. Set lastSeenTimeStamp to one of the timeStamps retrieved from the date of the previous query
* to get more recent samples.
*
* This method fetches the number of entries which can be accommodated in the provided samples array, and the
* reference samplesCount is updated to indicate how many samples were actually retrieved. The advantage of using this
* method for samples in contrast to polling via existing methods is to get get higher frequency data at lower polling cost.
*
* @note On MIG-enabled GPUs, querying the following sample types, NVML_GPU_UTILIZATION_SAMPLES, NVML_MEMORY_UTILIZATION_SAMPLES
*       NVML_ENC_UTILIZATION_SAMPLES and NVML_DEC_UTILIZATION_SAMPLES, is not currently supported.
*
* @param device                        The identifier for the target device
* @param type                          Type of sampling event
* @param lastSeenTimeStamp             Return only samples with timestamp greater than lastSeenTimeStamp.
* @param sampleValType                 Output parameter to represent the type of sample value as described in nvmlSampleVal_t
* @param sampleCount                   Reference to provide the number of elements which can be queried in samples array
* @param samples                       Reference in which samples are returned
* @return
*         - \ref NVML_SUCCESS                 if samples are successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a samplesCount is NULL or
*                                             reference to \a sampleCount is 0 for non null \a samples
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_FOUND         if sample entries are not found
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetSamples(void *conn) {
    nvmlDevice_t device;
    nvmlSamplingType_t type;
    unsigned long long lastSeenTimeStamp;
    nvmlValueType_t sampleValType;
    unsigned int sampleCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlSamplingType_t)) < 0 ||
        rpc_read(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_read(conn, &sampleCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlSample_t* samples = (nvmlSample_t*)malloc(sampleCount * sizeof(nvmlSample_t));

    nvmlReturn_t result = nvmlDeviceGetSamples(device, type, lastSeenTimeStamp, &sampleValType, &sampleCount, samples);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_write(conn, &sampleCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, samples, sampleCount * sizeof(nvmlSample_t)) < 0)
        return -1;

    return result;
}

/**
* Gets Total, Available and Used size of BAR1 memory.
*
* BAR1 is used to map the FB (device memory) so that it can be directly accessed by the CPU or by 3rd party
* devices (peer-to-peer on the PCIE bus).
*
* @note In MIG mode, if device handle is provided, the API returns aggregate
*       information, only if the caller has appropriate privileges. Per-instance
*       information can be queried by using specific MIG device handles.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param bar1Memory                           Reference in which BAR1 memory
*                                             information is returned.
*
* @return
*         - \ref NVML_SUCCESS                 if BAR1 memory is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a bar1Memory is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
*/
int handle_nvmlDeviceGetBAR1MemoryInfo(void *conn) {
    nvmlDevice_t device;
    nvmlBAR1Memory_t bar1Memory;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &bar1Memory, sizeof(nvmlBAR1Memory_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetBAR1MemoryInfo(device, &bar1Memory);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &bar1Memory, sizeof(nvmlBAR1Memory_t)) < 0)
        return -1;

    return result;
}

/**
* Gets the duration of time during which the device was throttled (lower than requested clocks) due to power
* or thermal constraints.
*
* The method is important to users who are tying to understand if their GPUs throttle at any point during their applications. The
* difference in violation times at two different reference times gives the indication of GPU throttling event.
*
* Violation for thermal capping is not supported at this time.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param perfPolicyType                       Represents Performance policy which can trigger GPU throttling
* @param violTime                             Reference to which violation time related information is returned
*
*
* @return
*         - \ref NVML_SUCCESS                 if violation time is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a perfPolicyType is invalid, or \a violTime is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetViolationStatus(void *conn) {
    nvmlDevice_t device;
    nvmlPerfPolicyType_t perfPolicyType;
    nvmlViolationTime_t violTime;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &perfPolicyType, sizeof(nvmlPerfPolicyType_t)) < 0 ||
        rpc_read(conn, &violTime, sizeof(nvmlViolationTime_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetViolationStatus(device, perfPolicyType, &violTime);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &violTime, sizeof(nvmlViolationTime_t)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's interrupt number
*
* @param device                               The identifier of the target device
* @param irqNum                               The interrupt number associated with the specified device
*
* @return
*         - \ref NVML_SUCCESS                 if irq number is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a irqNum is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetIrqNum(void *conn) {
    nvmlDevice_t device;
    unsigned int irqNum;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &irqNum, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetIrqNum(device, &irqNum);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &irqNum, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's core count
*
* @param device                               The identifier of the target device
* @param numCores                             The number of cores for the specified device
*
* @return
*         - \ref NVML_SUCCESS                 if Gpu core count is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a numCores is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetNumGpuCores(void *conn) {
    nvmlDevice_t device;
    unsigned int numCores;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &numCores, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNumGpuCores(device, &numCores);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numCores, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets the devices power source
*
* @param device                               The identifier of the target device
* @param powerSource                          The power source of the device
*
* @return
*         - \ref NVML_SUCCESS                 if the current power source was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a powerSource is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetPowerSource(void *conn) {
    nvmlDevice_t device;
    nvmlPowerSource_t powerSource;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &powerSource, sizeof(nvmlPowerSource_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPowerSource(device, &powerSource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &powerSource, sizeof(nvmlPowerSource_t)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's memory bus width
*
* @param device                               The identifier of the target device
* @param busWidth                             The devices's memory bus width
*
* @return
*         - \ref NVML_SUCCESS                 if the memory bus width is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a busWidth is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetMemoryBusWidth(void *conn) {
    nvmlDevice_t device;
    unsigned int busWidth;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &busWidth, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemoryBusWidth(device, &busWidth);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &busWidth, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's PCIE Max Link speed in MBPS
*
* @param device                               The identifier of the target device
* @param maxSpeed                             The devices's PCIE Max Link speed in MBPS
*
* @return
*         - \ref NVML_SUCCESS                 if Pcie Max Link Speed is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a maxSpeed is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetPcieLinkMaxSpeed(void *conn) {
    nvmlDevice_t device;
    unsigned int maxSpeed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &maxSpeed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPcieLinkMaxSpeed(device, &maxSpeed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxSpeed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's PCIe Link speed in Mbps
*
* @param device                               The identifier of the target device
* @param pcieSpeed                            The devices's PCIe Max Link speed in Mbps
*
* @return
*         - \ref NVML_SUCCESS                 if \a pcieSpeed has been retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pcieSpeed is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support PCIe speed getting
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetPcieSpeed(void *conn) {
    nvmlDevice_t device;
    unsigned int pcieSpeed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pcieSpeed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetPcieSpeed(device, &pcieSpeed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pcieSpeed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Gets the device's Adaptive Clock status
*
* @param device                               The identifier of the target device
* @param adaptiveClockStatus                  The current adaptive clocking status
*
* @return
*         - \ref NVML_SUCCESS                 if the current adaptive clocking status is successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, or \a adaptiveClockStatus is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*
*/
int handle_nvmlDeviceGetAdaptiveClockInfoStatus(void *conn) {
    nvmlDevice_t device;
    unsigned int adaptiveClockStatus;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &adaptiveClockStatus, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAdaptiveClockInfoStatus(device, &adaptiveClockStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &adaptiveClockStatus, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Queries the state of per process accounting mode.
*
* For Kepler &tm; or newer fully supported devices.
*
* See \ref nvmlDeviceGetAccountingStats for more details.
* See \ref nvmlDeviceSetAccountingMode
*
* @param device                               The identifier of the target device
* @param mode                                 Reference in which to return the current accounting mode
*
* @return
*         - \ref NVML_SUCCESS                 if the mode has been successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode are NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetAccountingMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAccountingMode(device, &mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Queries process's accounting stats.
*
* For Kepler &tm; or newer fully supported devices.
*
* Accounting stats capture GPU utilization and other statistics across the lifetime of a process.
* Accounting stats can be queried during life time of the process and after its termination.
* The time field in \ref nvmlAccountingStats_t is reported as 0 during the lifetime of the process and
* updated to actual running time after its termination.
* Accounting stats are kept in a circular buffer, newly created processes overwrite information about old
* processes.
*
* See \ref nvmlAccountingStats_t for description of each returned metric.
* List of processes that can be queried can be retrieved from \ref nvmlDeviceGetAccountingPids.
*
* @note Accounting Mode needs to be on. See \ref nvmlDeviceGetAccountingMode.
* @note Only compute and graphics applications stats can be queried. Monitoring applications stats can't be
*         queried since they don't contribute to GPU utilization.
* @note In case of pid collision stats of only the latest process (that terminated last) will be reported
*
* @warning On Kepler devices per process statistics are accurate only if there's one process running on a GPU.
*
* @param device                               The identifier of the target device
* @param pid                                  Process Id of the target process to query stats for
* @param stats                                Reference in which to return the process's accounting stats
*
* @return
*         - \ref NVML_SUCCESS                 if stats have been successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a stats are NULL
*         - \ref NVML_ERROR_NOT_FOUND         if process stats were not found
*         - \ref NVML_ERROR_NOT_SUPPORTED     if \a device doesn't support this feature or accounting mode is disabled
*                                              or on vGPU host.
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetAccountingBufferSize
*/
int handle_nvmlDeviceGetAccountingStats(void *conn) {
    nvmlDevice_t device;
    unsigned int pid;
    nvmlAccountingStats_t stats;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stats, sizeof(nvmlAccountingStats_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAccountingStats(device, pid, &stats);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &stats, sizeof(nvmlAccountingStats_t)) < 0)
        return -1;

    return result;
}

/**
* Queries list of processes that can be queried for accounting stats. The list of processes returned
* can be in running or terminated state.
*
* For Kepler &tm; or newer fully supported devices.
*
* To just query the number of processes ready to be queried, call this function with *count = 0 and
* pids=NULL. The return code will be NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if list is empty.
*
* For more details see \ref nvmlDeviceGetAccountingStats.
*
* @note In case of PID collision some processes might not be accessible before the circular buffer is full.
*
* @param device                               The identifier of the target device
* @param count                                Reference in which to provide the \a pids array size, and
*                                               to return the number of elements ready to be queried
* @param pids                                 Reference in which to return list of process ids
*
* @return
*         - \ref NVML_SUCCESS                 if pids were successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a count is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if \a device doesn't support this feature or accounting mode is disabled
*                                              or on vGPU host.
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a count is too small (\a count is set to
*                                                 expected value)
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetAccountingBufferSize
*/
int handle_nvmlDeviceGetAccountingPids(void *conn) {
    nvmlDevice_t device;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned int* pids = (unsigned int*)malloc(count * sizeof(unsigned int));

    nvmlReturn_t result = nvmlDeviceGetAccountingPids(device, &count, pids);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, pids, count * sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Returns the number of processes that the circular buffer with accounting pids can hold.
*
* For Kepler &tm; or newer fully supported devices.
*
* This is the maximum number of processes that accounting information will be stored for before information
* about oldest processes will get overwritten by information about new processes.
*
* @param device                               The identifier of the target device
* @param bufferSize                           Reference in which to provide the size (in number of elements)
*                                               of the circular buffer for accounting stats.
*
* @return
*         - \ref NVML_SUCCESS                 if buffer size was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a bufferSize is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature or accounting mode is disabled
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetAccountingStats
* @see nvmlDeviceGetAccountingPids
*/
int handle_nvmlDeviceGetAccountingBufferSize(void *conn) {
    nvmlDevice_t device;
    unsigned int bufferSize;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetAccountingBufferSize(device, &bufferSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Returns the list of retired pages by source, including pages that are pending retirement
* The address information provided from this API is the hardware address of the page that was retired.  Note
* that this does not match the virtual address used in CUDA, but will match the address information in XID 63
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param cause                             Filter page addresses by cause of retirement
* @param pageCount                         Reference in which to provide the \a addresses buffer size, and
*                                          to return the number of retired pages that match \a cause
*                                          Set to 0 to query the size without allocating an \a addresses buffer
* @param addresses                         Buffer to write the page addresses into
*
* @return
*         - \ref NVML_SUCCESS                 if \a pageCount was populated and \a addresses was filled
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a pageCount indicates the buffer is not large enough to store all the
*                                             matching page addresses.  \a pageCount is set to the needed size.
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a pageCount is NULL, \a cause is invalid, or
*                                             \a addresses is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetRetiredPages(void *conn) {
    nvmlDevice_t device;
    nvmlPageRetirementCause_t cause;
    unsigned int pageCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &cause, sizeof(nvmlPageRetirementCause_t)) < 0 ||
        rpc_read(conn, &pageCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned long long* addresses = (unsigned long long*)malloc(pageCount * sizeof(unsigned long long));

    nvmlReturn_t result = nvmlDeviceGetRetiredPages(device, cause, &pageCount, addresses);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pageCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, addresses, pageCount * sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Returns the list of retired pages by source, including pages that are pending retirement
* The address information provided from this API is the hardware address of the page that was retired.  Note
* that this does not match the virtual address used in CUDA, but will match the address information in XID 63
*
* \note nvmlDeviceGetRetiredPages_v2 adds an additional timestamps paramter to return the time of each page's
*       retirement.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param cause                             Filter page addresses by cause of retirement
* @param pageCount                         Reference in which to provide the \a addresses buffer size, and
*                                          to return the number of retired pages that match \a cause
*                                          Set to 0 to query the size without allocating an \a addresses buffer
* @param addresses                         Buffer to write the page addresses into
* @param timestamps                        Buffer to write the timestamps of page retirement, additional for _v2
*
* @return
*         - \ref NVML_SUCCESS                 if \a pageCount was populated and \a addresses was filled
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a pageCount indicates the buffer is not large enough to store all the
*                                             matching page addresses.  \a pageCount is set to the needed size.
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a pageCount is NULL, \a cause is invalid, or
*                                             \a addresses is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetRetiredPages_v2(void *conn) {
    nvmlDevice_t device;
    nvmlPageRetirementCause_t cause;
    unsigned int pageCount;
    unsigned long long timestamps;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &cause, sizeof(nvmlPageRetirementCause_t)) < 0 ||
        rpc_read(conn, &pageCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &timestamps, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned long long* addresses = (unsigned long long*)malloc(pageCount * sizeof(unsigned long long));

    nvmlReturn_t result = nvmlDeviceGetRetiredPages_v2(device, cause, &pageCount, addresses, &timestamps);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pageCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, addresses, pageCount * sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &timestamps, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Check if any pages are pending retirement and need a reboot to fully retire.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                            The identifier of the target device
* @param isPending                         Reference in which to return the pending status
*
* @return
*         - \ref NVML_SUCCESS                 if \a isPending was populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a isPending is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetRetiredPagesPendingStatus(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t isPending;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &isPending, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetRetiredPagesPendingStatus(device, &isPending);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isPending, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Get number of remapped rows. The number of rows reported will be based on
* the cause of the remapping. isPending indicates whether or not there are
* pending remappings. A reset will be required to actually remap the row.
* failureOccurred will be set if a row remapping ever failed in the past. A
* pending remapping won't affect future work on the GPU since
* error-containment and dynamic page blacklisting will take care of that.
*
* @note On MIG-enabled GPUs with active instances, querying the number of
* remapped rows is not supported
*
* For Ampere &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param corrRows                             Reference for number of rows remapped due to correctable errors
* @param uncRows                              Reference for number of rows remapped due to uncorrectable errors
* @param isPending                            Reference for whether or not remappings are pending
* @param failureOccurred                      Reference that is set when a remapping has failed in the past
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a corrRows, \a uncRows, \a isPending or \a failureOccurred is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If MIG is enabled or if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           Unexpected error
*/
int handle_nvmlDeviceGetRemappedRows(void *conn) {
    nvmlDevice_t device;
    unsigned int corrRows;
    unsigned int uncRows;
    unsigned int isPending;
    unsigned int failureOccurred;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &corrRows, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &uncRows, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &isPending, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &failureOccurred, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetRemappedRows(device, &corrRows, &uncRows, &isPending, &failureOccurred);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &corrRows, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &uncRows, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &isPending, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &failureOccurred, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get the row remapper histogram. Returns the remap availability for each bank
* on the GPU.
*
* @param device                               Device handle
* @param values                               Histogram values
*
* @return
*        - \ref NVML_SUCCESS                  On success
*        - \ref NVML_ERROR_UNKNOWN            On any unexpected error
*/
int handle_nvmlDeviceGetRowRemapperHistogram(void *conn) {
    nvmlDevice_t device;
    nvmlRowRemapperHistogramValues_t values;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &values, sizeof(nvmlRowRemapperHistogramValues_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetRowRemapperHistogram(device, &values);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &values, sizeof(nvmlRowRemapperHistogramValues_t)) < 0)
        return -1;

    return result;
}

/**
* Get architecture for device
*
* @param device                               The identifier of the target device
* @param arch                                 Reference where architecture is returned, if call successful.
*                                             Set to NVML_DEVICE_ARCH_* upon success
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device or \a arch (output refererence) are invalid
*/
int handle_nvmlDeviceGetArchitecture(void *conn) {
    nvmlDevice_t device;
    nvmlDeviceArchitecture_t arch;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &arch, sizeof(nvmlDeviceArchitecture_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetArchitecture(device, &arch);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &arch, sizeof(nvmlDeviceArchitecture_t)) < 0)
        return -1;

    return result;
}

/**
* Set the LED state for the unit. The LED can be either green (0) or amber (1).
*
* For S-class products.
* Requires root/admin permissions.
*
* This operation takes effect immediately.
*
*
* <b>Current S-Class products don't provide unique LEDs for each unit. As such, both front
* and back LEDs will be toggled in unison regardless of which unit is specified with this command.</b>
*
* See \ref nvmlLedColor_t for available colors.
*
* @param unit                                 The identifier of the target unit
* @param color                                The target LED color
*
* @return
*         - \ref NVML_SUCCESS                 if the LED color has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a unit or \a color is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this is not an S-class product
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlUnitGetLedState()
*/
int handle_nvmlUnitSetLedState(void *conn) {
    nvmlUnit_t unit;
    nvmlLedColor_t color;

    if (rpc_read(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &color, sizeof(nvmlLedColor_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlUnitSetLedState(unit, color);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set the persistence mode for the device.
*
* For all products.
* For Linux only.
* Requires root/admin permissions.
*
* The persistence mode determines whether the GPU driver software is torn down after the last client
* exits.
*
* This operation takes effect immediately. It is not persistent across reboots. After each reboot the
* persistence mode is reset to "Disabled".
*
* See \ref nvmlEnableState_t for available modes.
*
* After calling this API with mode set to NVML_FEATURE_DISABLED on a device that has its own NUMA
* memory, the given device handle will no longer be valid, and to continue to interact with this
* device, a new handle should be obtained from one of the nvmlDeviceGetHandleBy*() APIs. This
* limitation is currently only applicable to devices that have a coherent NVLink connection to
* system memory.
*
* @param device                               The identifier of the target device
* @param mode                                 The target persistence mode
*
* @return
*         - \ref NVML_SUCCESS                 if the persistence mode was set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetPersistenceMode()
*/
int handle_nvmlDeviceSetPersistenceMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetPersistenceMode(device, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set the compute mode for the device.
*
* For all products.
* Requires root/admin permissions.
*
* The compute mode determines whether a GPU can be used for compute operations and whether it can
* be shared across contexts.
*
* This operation takes effect immediately. Under Linux it is not persistent across reboots and
* always resets to "Default". Under windows it is persistent.
*
* Under windows compute mode may only be set to DEFAULT when running in WDDM
*
* @note On MIG-enabled GPUs, compute mode would be set to DEFAULT and changing it is not supported.
*
* See \ref nvmlComputeMode_t for details on available compute modes.
*
* @param device                               The identifier of the target device
* @param mode                                 The target compute mode
*
* @return
*         - \ref NVML_SUCCESS                 if the compute mode was set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetComputeMode()
*/
int handle_nvmlDeviceSetComputeMode(void *conn) {
    nvmlDevice_t device;
    nvmlComputeMode_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlComputeMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetComputeMode(device, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set the ECC mode for the device.
*
* For Kepler &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher.
* Requires root/admin permissions.
*
* The ECC mode determines whether the GPU enables its ECC support.
*
* This operation takes effect after the next reboot.
*
* See \ref nvmlEnableState_t for details on available modes.
*
* @param device                               The identifier of the target device
* @param ecc                                  The target ECC mode
*
* @return
*         - \ref NVML_SUCCESS                 if the ECC mode was set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a ecc is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetEccMode()
*/
int handle_nvmlDeviceSetEccMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t ecc;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &ecc, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetEccMode(device, ecc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Clear the ECC error and other memory error counts for the device.
*
* For Kepler &tm; or newer fully supported devices.
* Only applicable to devices with ECC.
* Requires \a NVML_INFOROM_ECC version 2.0 or higher to clear aggregate location-based ECC counts.
* Requires \a NVML_INFOROM_ECC version 1.0 or higher to clear all other ECC counts.
* Requires root/admin permissions.
* Requires ECC Mode to be enabled.
*
* Sets all of the specified ECC counters to 0, including both detailed and total counts.
*
* This operation takes effect immediately.
*
* See \ref nvmlMemoryErrorType_t for details on available counter types.
*
* @param device                               The identifier of the target device
* @param counterType                          Flag that indicates which type of errors should be cleared.
*
* @return
*         - \ref NVML_SUCCESS                 if the error counts were cleared
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a counterType is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see
*      - nvmlDeviceGetDetailedEccErrors()
*      - nvmlDeviceGetTotalEccErrors()
*/
int handle_nvmlDeviceClearEccErrorCounts(void *conn) {
    nvmlDevice_t device;
    nvmlEccCounterType_t counterType;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceClearEccErrorCounts(device, counterType);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set the driver model for the device.
*
* For Fermi &tm; or newer fully supported devices.
* For windows only.
* Requires root/admin permissions.
*
* On Windows platforms the device driver can run in either WDDM or WDM (TCC) mode. If a display is attached
* to the device it must run in WDDM mode.
*
* It is possible to force the change to WDM (TCC) while the display is still attached with a force flag (nvmlFlagForce).
* This should only be done if the host is subsequently powered down and the display is detached from the device
* before the next reboot.
*
* This operation takes effect after the next reboot.
*
* Windows driver model may only be set to WDDM when running in DEFAULT compute mode.
*
* Change driver model to WDDM is not supported when GPU doesn't support graphics acceleration or
* will not support it after reboot. See \ref nvmlDeviceSetGpuOperationMode.
*
* See \ref nvmlDriverModel_t for details on available driver models.
* See \ref nvmlFlagDefault and \ref nvmlFlagForce
*
* @param device                               The identifier of the target device
* @param driverModel                          The target driver model
* @param flags                                Flags that change the default behavior
*
* @return
*         - \ref NVML_SUCCESS                 if the driver model has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a driverModel is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the platform is not windows or the device does not support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetDriverModel()
*/
int handle_nvmlDeviceSetDriverModel(void *conn) {
    nvmlDevice_t device;
    nvmlDriverModel_t driverModel;
    unsigned int flags;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &driverModel, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetDriverModel(device, driverModel, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set clocks that device will lock to.
*
* Sets the clocks that the device will be running at to the value in the range of minGpuClockMHz to maxGpuClockMHz.
* Setting this will supercede application clock values and take effect regardless if a cuda app is running.
* See /ref nvmlDeviceSetApplicationsClocks
*
* Can be used as a setting to request constant performance.
*
* This can be called with a pair of integer clock frequencies in MHz, or a pair of /ref nvmlClockLimitId_t values.
* See the table below for valid combinations of these values.
*
* minGpuClock | maxGpuClock | Effect
* ------------+-------------+--------------------------------------------------
*     tdp     |     tdp     | Lock clock to TDP
*  unlimited  |     tdp     | Upper bound is TDP but clock may drift below this
*     tdp     |  unlimited  | Lower bound is TDP but clock may boost above this
*  unlimited  |  unlimited  | Unlocked (== nvmlDeviceResetGpuLockedClocks)
*
* If one arg takes one of these values, the other must be one of these values as
* well. Mixed numeric and symbolic calls return NVML_ERROR_INVALID_ARGUMENT.
*
* Requires root/admin permissions.
*
* After system reboot or driver reload applications clocks go back to their default value.
* See \ref nvmlDeviceResetGpuLockedClocks.
*
* For Volta &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param minGpuClockMHz                       Requested minimum gpu clock in MHz
* @param maxGpuClockMHz                       Requested maximum gpu clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a minGpuClockMHz and \a maxGpuClockMHz
*                                                 is not a valid clock combination
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetGpuLockedClocks(void *conn) {
    nvmlDevice_t device;
    unsigned int minGpuClockMHz;
    unsigned int maxGpuClockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minGpuClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &maxGpuClockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetGpuLockedClocks(device, minGpuClockMHz, maxGpuClockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Resets the gpu clock to the default value
*
* This is the gpu clock that will be used after system reboot or driver reload.
* Default values are idle clocks, but the current values can be changed using \ref nvmlDeviceSetApplicationsClocks.
*
* @see nvmlDeviceSetGpuLockedClocks
*
* For Volta &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceResetGpuLockedClocks(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceResetGpuLockedClocks(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set memory clocks that device will lock to.
*
* Sets the device's memory clocks to the value in the range of minMemClockMHz to maxMemClockMHz.
* Setting this will supersede application clock values and take effect regardless of whether a cuda app is running.
* See /ref nvmlDeviceSetApplicationsClocks
*
* Can be used as a setting to request constant performance.
*
* Requires root/admin permissions.
*
* After system reboot or driver reload applications clocks go back to their default value.
* See \ref nvmlDeviceResetMemoryLockedClocks.
*
* For Ampere &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param minMemClockMHz                       Requested minimum memory clock in MHz
* @param maxMemClockMHz                       Requested maximum memory clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a minGpuClockMHz and \a maxGpuClockMHz
*                                                 is not a valid clock combination
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetMemoryLockedClocks(void *conn) {
    nvmlDevice_t device;
    unsigned int minMemClockMHz;
    unsigned int maxMemClockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minMemClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &maxMemClockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetMemoryLockedClocks(device, minMemClockMHz, maxMemClockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Resets the memory clock to the default value
*
* This is the memory clock that will be used after system reboot or driver reload.
* Default values are idle clocks, but the current values can be changed using \ref nvmlDeviceSetApplicationsClocks.
*
* @see nvmlDeviceSetMemoryLockedClocks
*
* For Ampere &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceResetMemoryLockedClocks(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceResetMemoryLockedClocks(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Set clocks that applications will lock to.
*
* Sets the clocks that compute and graphics applications will be running at.
* e.g. CUDA driver requests these clocks during context creation which means this property
* defines clocks at which CUDA applications will be running unless some overspec event
* occurs (e.g. over power, over thermal or external HW brake).
*
* Can be used as a setting to request constant performance.
*
* On Pascal and newer hardware, this will automatically disable automatic boosting of clocks.
*
* On K80 and newer Kepler and Maxwell GPUs, users desiring fixed performance should also call
* \ref nvmlDeviceSetAutoBoostedClocksEnabled to prevent clocks from automatically boosting
* above the clock value being set.
*
* For Kepler &tm; or newer non-GeForce fully supported devices and Maxwell or newer GeForce devices.
* Requires root/admin permissions.
*
* See \ref nvmlDeviceGetSupportedMemoryClocks and \ref nvmlDeviceGetSupportedGraphicsClocks
* for details on how to list available clocks combinations.
*
* After system reboot or driver reload applications clocks go back to their default value.
* See \ref nvmlDeviceResetApplicationsClocks.
*
* @param device                               The identifier of the target device
* @param memClockMHz                          Requested memory clock in MHz
* @param graphicsClockMHz                     Requested graphics clock in MHz
*
* @return
*         - \ref NVML_SUCCESS                 if new settings were successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a memClockMHz and \a graphicsClockMHz
*                                                 is not a valid clock combination
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetApplicationsClocks(void *conn) {
    nvmlDevice_t device;
    unsigned int memClockMHz;
    unsigned int graphicsClockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &memClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &graphicsClockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetApplicationsClocks(device, memClockMHz, graphicsClockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the frequency monitor fault status for the device.
*
* For Ampere &tm; or newer fully supported devices.
* Requires root user.
*
* See \ref nvmlClkMonStatus_t for details on decoding the status output.
*
* @param device                               The identifier of the target device
* @param status                               Reference in which to return the clkmon fault status
*
* @return
*         - \ref NVML_SUCCESS                 if \a status has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a status is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetClkMonStatus()
*/
int handle_nvmlDeviceGetClkMonStatus(void *conn) {
    nvmlDevice_t device;
    nvmlClkMonStatus_t status;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &status, sizeof(nvmlClkMonStatus_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetClkMonStatus(device, &status);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &status, sizeof(nvmlClkMonStatus_t)) < 0)
        return -1;

    return result;
}

/**
* Set new power limit of this device.
*
* For Kepler &tm; or newer fully supported devices.
* Requires root/admin permissions.
*
* See \ref nvmlDeviceGetPowerManagementLimitConstraints to check the allowed ranges of values.
*
* \note Limit is not persistent across reboots or driver unloads.
* Enable persistent mode to prevent driver from unloading when no application is using the device.
*
* @param device                               The identifier of the target device
* @param limit                                Power management limit in milliwatts to set
*
* @return
*         - \ref NVML_SUCCESS                 if \a limit has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a defaultLimit is out of range
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceGetPowerManagementLimitConstraints
* @see nvmlDeviceGetPowerManagementDefaultLimit
*/
int handle_nvmlDeviceSetPowerManagementLimit(void *conn) {
    nvmlDevice_t device;
    unsigned int limit;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetPowerManagementLimit(device, limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Sets new GOM. See \a nvmlGpuOperationMode_t for details.
*
* For GK110 M-class and X-class Tesla &tm; products from the Kepler family.
* Modes \ref NVML_GOM_LOW_DP and \ref NVML_GOM_ALL_ON are supported on fully supported GeForce products.
* Not supported on Quadro &reg; and Tesla &tm; C-class products.
* Requires root/admin permissions.
*
* Changing GOMs requires a reboot.
* The reboot requirement might be removed in the future.
*
* Compute only GOMs don't support graphics acceleration. Under windows switching to these GOMs when
* pending driver model is WDDM is not supported. See \ref nvmlDeviceSetDriverModel.
*
* @param device                               The identifier of the target device
* @param mode                                 Target GOM
*
* @return
*         - \ref NVML_SUCCESS                 if \a mode has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a mode incorrect
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support GOM or specific mode
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlGpuOperationMode_t
* @see nvmlDeviceGetGpuOperationMode
*/
int handle_nvmlDeviceSetGpuOperationMode(void *conn) {
    nvmlDevice_t device;
    nvmlGpuOperationMode_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlGpuOperationMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetGpuOperationMode(device, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Changes the root/admin restructions on certain APIs. See \a nvmlRestrictedAPI_t for the list of supported APIs.
* This method can be used by a root/admin user to give non-root/admin access to certain otherwise-restricted APIs.
* The new setting lasts for the lifetime of the NVIDIA driver; it is not persistent. See \a nvmlDeviceGetAPIRestriction
* to query the current restriction settings.
*
* For Kepler &tm; or newer fully supported devices.
* Requires root/admin permissions.
*
* @param device                               The identifier of the target device
* @param apiType                              Target API type for this operation
* @param isRestricted                         The target restriction
*
* @return
*         - \ref NVML_SUCCESS                 if \a isRestricted has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a apiType incorrect
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support changing API restrictions or the device does not support
*                                                 the feature that api restrictions are being set for (E.G. Enabling/disabling auto
*                                                 boosted clocks is not supported by the device)
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlRestrictedAPI_t
*/
int handle_nvmlDeviceSetAPIRestriction(void *conn) {
    nvmlDevice_t device;
    nvmlRestrictedAPI_t apiType;
    nvmlEnableState_t isRestricted;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &apiType, sizeof(nvmlRestrictedAPI_t)) < 0 ||
        rpc_read(conn, &isRestricted, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetAPIRestriction(device, apiType, isRestricted);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Enables or disables per process accounting.
*
* For Kepler &tm; or newer fully supported devices.
* Requires root/admin permissions.
*
* @note This setting is not persistent and will default to disabled after driver unloads.
*       Enable persistence mode to be sure the setting doesn't switch off to disabled.
*
* @note Enabling accounting mode has no negative impact on the GPU performance.
*
* @note Disabling accounting clears all accounting pids information.
*
* @note On MIG-enabled GPUs, accounting mode would be set to DISABLED and changing it is not supported.
*
* See \ref nvmlDeviceGetAccountingMode
* See \ref nvmlDeviceGetAccountingStats
* See \ref nvmlDeviceClearAccountingPids
*
* @param device                               The identifier of the target device
* @param mode                                 The target accounting mode
*
* @return
*         - \ref NVML_SUCCESS                 if the new mode has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a mode are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetAccountingMode(void *conn) {
    nvmlDevice_t device;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetAccountingMode(device, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Clears accounting information about all processes that have already terminated.
*
* For Kepler &tm; or newer fully supported devices.
* Requires root/admin permissions.
*
* See \ref nvmlDeviceGetAccountingMode
* See \ref nvmlDeviceGetAccountingStats
* See \ref nvmlDeviceSetAccountingMode
*
* @param device                               The identifier of the target device
*
* @return
*         - \ref NVML_SUCCESS                 if accounting information has been cleared
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceClearAccountingPids(void *conn) {
    nvmlDevice_t device;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceClearAccountingPids(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the state of the device's NvLink for the link specified
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param isActive                             \a nvmlEnableState_t where NVML_FEATURE_ENABLED indicates that
*                                             the link is active and NVML_FEATURE_DISABLED indicates it
*                                             is inactive
*
* @return
*         - \ref NVML_SUCCESS                 if \a isActive has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a link is invalid or \a isActive is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkState(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    nvmlEnableState_t isActive;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &isActive, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkState(device, link, &isActive);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isActive, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the version of the device's NvLink for the link specified
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param version                              Requested NvLink version
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a link is invalid or \a version is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkVersion(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int version;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &version, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkVersion(device, link, &version);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &version, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the requested capability from the device's NvLink for the link specified
* Please refer to the \a nvmlNvLinkCapability_t structure for the specific caps that can be queried
* The return value should be treated as a boolean.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param capability                           Specifies the \a nvmlNvLinkCapability_t to be queried
* @param capResult                            A boolean for the queried capability indicating that feature is available
*
* @return
*         - \ref NVML_SUCCESS                 if \a capResult has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a link, or \a capability is invalid or \a capResult is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkCapability(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    nvmlNvLinkCapability_t capability;
    unsigned int capResult;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &capability, sizeof(nvmlNvLinkCapability_t)) < 0 ||
        rpc_read(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkCapability(device, link, capability, &capResult);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the PCI information for the remote node on a NvLink link
* Note: pciSubSystemId is not filled in this function and is indeterminate
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param pci                                  \a nvmlPciInfo_t of the remote node for the specified link
*
* @return
*         - \ref NVML_SUCCESS                 if \a pci has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a link is invalid or \a pci is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkRemotePciInfo_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    nvmlPciInfo_t pci;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &pci, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkRemotePciInfo_v2(device, link, &pci);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pci, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the specified error counter value
* Please refer to \a nvmlNvLinkErrorCounter_t for error counters that are available
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param counter                              Specifies the NvLink counter to be queried
* @param counterValue                         Returned counter value
*
* @return
*         - \ref NVML_SUCCESS                 if \a counter has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a link, or \a counter is invalid or \a counterValue is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkErrorCounter(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    nvmlNvLinkErrorCounter_t counter;
    unsigned long long counterValue;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(nvmlNvLinkErrorCounter_t)) < 0 ||
        rpc_read(conn, &counterValue, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkErrorCounter(device, link, counter, &counterValue);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &counterValue, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Resets all error counters to zero
* Please refer to \a nvmlNvLinkErrorCounter_t for the list of error counters that are reset
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
*
* @return
*         - \ref NVML_SUCCESS                 if the reset is successful
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a link is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceResetNvLinkErrorCounters(void *conn) {
    nvmlDevice_t device;
    unsigned int link;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceResetNvLinkErrorCounters(device, link);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Setting utilization counter control is no longer supported.
*
* Set the NVLINK utilization counter control information for the specified counter, 0 or 1.
* Please refer to \a nvmlNvLinkUtilizationControl_t for the structure definition.  Performs a reset
* of the counters if the reset parameter is non-zero.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param counter                              Specifies the counter that should be set (0 or 1).
* @param link                                 Specifies the NvLink link to be queried
* @param control                              A reference to the \a nvmlNvLinkUtilizationControl_t to set
* @param reset                                Resets the counters on set if non-zero
*
* @return
*         - \ref NVML_SUCCESS                 if the control has been set successfully
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a counter, \a link, or \a control is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetNvLinkUtilizationControl(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int counter;
    nvmlNvLinkUtilizationControl_t control;
    unsigned int reset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &control, sizeof(nvmlNvLinkUtilizationControl_t)) < 0 ||
        rpc_read(conn, &reset, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetNvLinkUtilizationControl(device, link, counter, &control, reset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &control, sizeof(nvmlNvLinkUtilizationControl_t)) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Getting utilization counter control is no longer supported.
*
* Get the NVLINK utilization counter control information for the specified counter, 0 or 1.
* Please refer to \a nvmlNvLinkUtilizationControl_t for the structure definition
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param counter                              Specifies the counter that should be set (0 or 1).
* @param link                                 Specifies the NvLink link to be queried
* @param control                              A reference to the \a nvmlNvLinkUtilizationControl_t to place information
*
* @return
*         - \ref NVML_SUCCESS                 if the control has been set successfully
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a counter, \a link, or \a control is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkUtilizationControl(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int counter;
    nvmlNvLinkUtilizationControl_t control;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &control, sizeof(nvmlNvLinkUtilizationControl_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkUtilizationControl(device, link, counter, &control);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &control, sizeof(nvmlNvLinkUtilizationControl_t)) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Use \ref nvmlDeviceGetFieldValues with NVML_FI_DEV_NVLINK_THROUGHPUT_* as field values instead.
*
* Retrieve the NVLINK utilization counter based on the current control for a specified counter.
* In general it is good practice to use \a nvmlDeviceSetNvLinkUtilizationControl
*  before reading the utilization counters as they have no default state
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param counter                              Specifies the counter that should be read (0 or 1).
* @param rxcounter                            Receive counter return value
* @param txcounter                            Transmit counter return value
*
* @return
*         - \ref NVML_SUCCESS                 if \a rxcounter and \a txcounter have been successfully set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a counter, or \a link is invalid or \a rxcounter or \a txcounter are NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkUtilizationCounter(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int counter;
    unsigned long long rxcounter;
    unsigned long long txcounter;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &rxcounter, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &txcounter, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkUtilizationCounter(device, link, counter, &rxcounter, &txcounter);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &rxcounter, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &txcounter, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Freezing NVLINK utilization counters is no longer supported.
*
* Freeze the NVLINK utilization counters
* Both the receive and transmit counters are operated on by this function
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be queried
* @param counter                              Specifies the counter that should be frozen (0 or 1).
* @param freeze                               NVML_FEATURE_ENABLED = freeze the receive and transmit counters
*                                             NVML_FEATURE_DISABLED = unfreeze the receive and transmit counters
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully frozen or unfrozen
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a link, \a counter, or \a freeze is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceFreezeNvLinkUtilizationCounter(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int counter;
    nvmlEnableState_t freeze;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &freeze, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceFreezeNvLinkUtilizationCounter(device, link, counter, freeze);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Deprecated: Resetting NVLINK utilization counters is no longer supported.
*
* Reset the NVLINK utilization counters
* Both the receive and transmit counters are operated on by this function
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                               The identifier of the target device
* @param link                                 Specifies the NvLink link to be reset
* @param counter                              Specifies the counter that should be reset (0 or 1)
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully reset
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a link, or \a counter is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceResetNvLinkUtilizationCounter(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    unsigned int counter;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &counter, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceResetNvLinkUtilizationCounter(device, link, counter);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Get the NVLink device type of the remote device connected over the given link.
*
* @param device                                The device handle of the target GPU
* @param link                                  The NVLink link index on the target GPU
* @param pNvLinkDeviceType                     Pointer in which the output remote device type is returned
*
* @return
*         - \ref NVML_SUCCESS                  if \a pNvLinkDeviceType has been set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_NOT_SUPPORTED      if NVLink is not supported
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device or \a link is invalid, or
*                                              \a pNvLinkDeviceType is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is
*                                              otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetNvLinkRemoteDeviceType(void *conn) {
    nvmlDevice_t device;
    unsigned int link;
    nvmlIntNvLinkDeviceType_t pNvLinkDeviceType;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &pNvLinkDeviceType, sizeof(nvmlIntNvLinkDeviceType_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetNvLinkRemoteDeviceType(device, link, &pNvLinkDeviceType);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNvLinkDeviceType, sizeof(nvmlIntNvLinkDeviceType_t)) < 0)
        return -1;

    return result;
}

/**
* Create an empty set of events.
* Event set should be freed by \ref nvmlEventSetFree
*
* For Fermi &tm; or newer fully supported devices.
* @param set                                  Reference in which to return the event handle
*
* @return
*         - \ref NVML_SUCCESS                 if the event has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a set is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlEventSetFree
*/
int handle_nvmlEventSetCreate(void *conn) {
    nvmlEventSet_t set;

    if (rpc_read(conn, &set, sizeof(nvmlEventSet_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlEventSetCreate(&set);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &set, sizeof(nvmlEventSet_t)) < 0)
        return -1;

    return result;
}

/**
* Starts recording of events on a specified devices and add the events to specified \ref nvmlEventSet_t
*
* For Fermi &tm; or newer fully supported devices.
* Ecc events are available only on ECC enabled devices (see \ref nvmlDeviceGetTotalEccErrors)
* Power capping events are available only on Power Management enabled devices (see \ref nvmlDeviceGetPowerManagementMode)
*
* For Linux only.
*
* \b IMPORTANT: Operations on \a set are not thread safe
*
* This call starts recording of events on specific device.
* All events that occurred before this call are not recorded.
* Checking if some event occurred can be done with \ref nvmlEventSetWait_v2
*
* If function reports NVML_ERROR_UNKNOWN, event set is in undefined state and should be freed.
* If function reports NVML_ERROR_NOT_SUPPORTED, event set can still be used. None of the requested eventTypes
*     are registered in that case.
*
* @param device                               The identifier of the target device
* @param eventTypes                           Bitmask of \ref nvmlEventType to record
* @param set                                  Set to which add new event types
*
* @return
*         - \ref NVML_SUCCESS                 if the event has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a eventTypes is invalid or \a set is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the platform does not support this feature or some of requested event types
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlEventType
* @see nvmlDeviceGetSupportedEventTypes
* @see nvmlEventSetWait
* @see nvmlEventSetFree
*/
int handle_nvmlDeviceRegisterEvents(void *conn) {
    nvmlDevice_t device;
    unsigned long long eventTypes;
    nvmlEventSet_t set;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &eventTypes, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &set, sizeof(nvmlEventSet_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceRegisterEvents(device, eventTypes, set);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Returns information about events supported on device
*
* For Fermi &tm; or newer fully supported devices.
*
* Events are not supported on Windows. So this function returns an empty mask in \a eventTypes on Windows.
*
* @param device                               The identifier of the target device
* @param eventTypes                           Reference in which to return bitmask of supported events
*
* @return
*         - \ref NVML_SUCCESS                 if the eventTypes has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a eventType is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlEventType
* @see nvmlDeviceRegisterEvents
*/
int handle_nvmlDeviceGetSupportedEventTypes(void *conn) {
    nvmlDevice_t device;
    unsigned long long eventTypes;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &eventTypes, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetSupportedEventTypes(device, &eventTypes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &eventTypes, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Waits on events and delivers events
*
* For Fermi &tm; or newer fully supported devices.
*
* If some events are ready to be delivered at the time of the call, function returns immediately.
* If there are no events ready to be delivered, function sleeps till event arrives
* but not longer than specified timeout. This function in certain conditions can return before
* specified timeout passes (e.g. when interrupt arrives)
*
* On Windows, in case of xid error, the function returns the most recent xid error type seen by the system.
* If there are multiple xid errors generated before nvmlEventSetWait is invoked then the last seen xid error
* type is returned for all xid error events.
*
* On Linux, every xid error event would return the associated event data and other information if applicable.
*
* In MIG mode, if device handle is provided, the API reports all the events for the available instances,
* only if the caller has appropriate privileges. In absence of required privileges, only the events which
* affect all the instances (i.e. whole device) are reported.
*
* This API does not currently support per-instance event reporting using MIG device handles.
*
* @param set                                  Reference to set of events to wait on
* @param data                                 Reference in which to return event data
* @param timeoutms                            Maximum amount of wait time in milliseconds for registered event
*
* @return
*         - \ref NVML_SUCCESS                 if the data has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a data is NULL
*         - \ref NVML_ERROR_TIMEOUT           if no event arrived in specified timeout or interrupt arrived
*         - \ref NVML_ERROR_GPU_IS_LOST       if a GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlEventType
* @see nvmlDeviceRegisterEvents
*/
int handle_nvmlEventSetWait_v2(void *conn) {
    nvmlEventSet_t set;
    nvmlEventData_t data;
    unsigned int timeoutms;

    if (rpc_read(conn, &set, sizeof(nvmlEventSet_t)) < 0 ||
        rpc_read(conn, &data, sizeof(nvmlEventData_t)) < 0 ||
        rpc_read(conn, &timeoutms, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlEventSetWait_v2(set, &data, timeoutms);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(nvmlEventData_t)) < 0)
        return -1;

    return result;
}

/**
* Releases events in the set
*
* For Fermi &tm; or newer fully supported devices.
*
* @param set                                  Reference to events to be released
*
* @return
*         - \ref NVML_SUCCESS                 if the event has been successfully released
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlDeviceRegisterEvents
*/
int handle_nvmlEventSetFree(void *conn) {
    nvmlEventSet_t set;

    if (rpc_read(conn, &set, sizeof(nvmlEventSet_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlEventSetFree(set);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Modify the drain state of a GPU.  This method forces a GPU to no longer accept new incoming requests.
* Any new NVML process will no longer see this GPU.  Persistence mode for this GPU must be turned off before
* this call is made.
* Must be called as administrator.
* For Linux only.
*
* For Pascal &tm; or newer fully supported devices.
* Some Kepler devices supported.
*
* @param pciInfo                              The PCI address of the GPU drain state to be modified
* @param newState                             The drain state that should be entered, see \ref nvmlEnableState_t
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully reset
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a nvmlIndex or \a newState is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the calling process has insufficient permissions to perform operation
*         - \ref NVML_ERROR_IN_USE            if the device has persistence mode turned on
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceModifyDrainState(void *conn) {
    nvmlPciInfo_t pciInfo;
    nvmlEnableState_t newState;

    if (rpc_read(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &newState, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceModifyDrainState(&pciInfo, newState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Query the drain state of a GPU.  This method is used to check if a GPU is in a currently draining
* state.
* For Linux only.
*
* For Pascal &tm; or newer fully supported devices.
* Some Kepler devices supported.
*
* @param pciInfo                              The PCI address of the GPU drain state to be queried
* @param currentState                         The current drain state for this GPU, see \ref nvmlEnableState_t
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully reset
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a nvmlIndex or \a currentState is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceQueryDrainState(void *conn) {
    nvmlPciInfo_t pciInfo;
    nvmlEnableState_t currentState;

    if (rpc_read(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &currentState, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceQueryDrainState(&pciInfo, &currentState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_write(conn, &currentState, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* This method will remove the specified GPU from the view of both NVML and the NVIDIA kernel driver
* as long as no other processes are attached. If other processes are attached, this call will return
* NVML_ERROR_IN_USE and the GPU will be returned to its original "draining" state. Note: the
* only situation where a process can still be attached after nvmlDeviceModifyDrainState() is called
* to initiate the draining state is if that process was using, and is still using, a GPU before the
* call was made. Also note, persistence mode counts as an attachment to the GPU thus it must be disabled
* prior to this call.
*
* For long-running NVML processes please note that this will change the enumeration of current GPUs.
* For example, if there are four GPUs present and GPU1 is removed, the new enumeration will be 0-2.
* Also, device handles after the removed GPU will not be valid and must be re-established.
* Must be run as administrator.
* For Linux only.
*
* For Pascal &tm; or newer fully supported devices.
* Some Kepler devices supported.
*
* @param pciInfo                              The PCI address of the GPU to be removed
* @param gpuState                             Whether the GPU is to be removed, from the OS
*                                             see \ref nvmlDetachGpuState_t
* @param linkState                            Requested upstream PCIe link state, see \ref nvmlPcieLinkState_t
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully reset
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a nvmlIndex is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device doesn't support this feature
*         - \ref NVML_ERROR_IN_USE            if the device is still in use and cannot be removed
*/
int handle_nvmlDeviceRemoveGpu_v2(void *conn) {
    nvmlPciInfo_t pciInfo;
    nvmlDetachGpuState_t gpuState;
    nvmlPcieLinkState_t linkState;

    if (rpc_read(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &gpuState, sizeof(nvmlDetachGpuState_t)) < 0 ||
        rpc_read(conn, &linkState, sizeof(nvmlPcieLinkState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceRemoveGpu_v2(&pciInfo, gpuState, linkState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Request the OS and the NVIDIA kernel driver to rediscover a portion of the PCI subsystem looking for GPUs that
* were previously removed. The portion of the PCI tree can be narrowed by specifying a domain, bus, and device.
* If all are zeroes then the entire PCI tree will be searched.  Please note that for long-running NVML processes
* the enumeration will change based on how many GPUs are discovered and where they are inserted in bus order.
*
* In addition, all newly discovered GPUs will be initialized and their ECC scrubbed which may take several seconds
* per GPU. Also, all device handles are no longer guaranteed to be valid post discovery.
*
* Must be run as administrator.
* For Linux only.
*
* For Pascal &tm; or newer fully supported devices.
* Some Kepler devices supported.
*
* @param pciInfo                              The PCI tree to be searched.  Only the domain, bus, and device
*                                             fields are used in this call.
*
* @return
*         - \ref NVML_SUCCESS                 if counters were successfully reset
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a pciInfo is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the operating system does not support this feature
*         - \ref NVML_ERROR_OPERATING_SYSTEM  if the operating system is denying this feature
*         - \ref NVML_ERROR_NO_PERMISSION     if the calling process has insufficient permissions to perform operation
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceDiscoverGpus(void *conn) {
    nvmlPciInfo_t pciInfo;

    if (rpc_read(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceDiscoverGpus(&pciInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciInfo, sizeof(nvmlPciInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Request values for a list of fields for a device. This API allows multiple fields to be queried at once.
* If any of the underlying fieldIds are populated by the same driver call, the results for those field IDs
* will be populated from a single call rather than making a driver call for each fieldId.
*
* @param device                               The device handle of the GPU to request field values for
* @param valuesCount                          Number of entries in values that should be retrieved
* @param values                               Array of \a valuesCount structures to hold field values.
*                                             Each value's fieldId must be populated prior to this call
*
* @return
*         - \ref NVML_SUCCESS                 if any values in \a values were populated. Note that you must
*                                             check the nvmlReturn field of each value for each individual
*                                             status
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a values is NULL
*/
int handle_nvmlDeviceGetFieldValues(void *conn) {
    nvmlDevice_t device;
    int valuesCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &valuesCount, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlFieldValue_t* values = (nvmlFieldValue_t*)malloc(valuesCount * sizeof(nvmlFieldValue_t));

    nvmlReturn_t result = nvmlDeviceGetFieldValues(device, valuesCount, values);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, values, valuesCount * sizeof(nvmlFieldValue_t)) < 0)
        return -1;

    return result;
}

/**
* Clear values for a list of fields for a device. This API allows multiple fields to be cleared at once.
*
* @param device                               The device handle of the GPU to request field values for
* @param valuesCount                          Number of entries in values that should be cleared
* @param values                               Array of \a valuesCount structures to hold field values.
*                                             Each value's fieldId must be populated prior to this call
*
* @return
*         - \ref NVML_SUCCESS                 if any values in \a values were cleared. Note that you must
*                                             check the nvmlReturn field of each value for each individual
*                                             status
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a values is NULL
*/
int handle_nvmlDeviceClearFieldValues(void *conn) {
    nvmlDevice_t device;
    int valuesCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &valuesCount, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlFieldValue_t* values = (nvmlFieldValue_t*)malloc(valuesCount * sizeof(nvmlFieldValue_t));

    nvmlReturn_t result = nvmlDeviceClearFieldValues(device, valuesCount, values);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, values, valuesCount * sizeof(nvmlFieldValue_t)) < 0)
        return -1;

    return result;
}

/**
* This method is used to get the virtualization mode corresponding to the GPU.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                    Identifier of the target device
* @param pVirtualMode              Reference to virtualization mode. One of NVML_GPU_VIRTUALIZATION_?
*
* @return
*         - \ref NVML_SUCCESS                  if \a pVirtualMode is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device is invalid or \a pVirtualMode is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetVirtualizationMode(void *conn) {
    nvmlDevice_t device;
    nvmlGpuVirtualizationMode_t pVirtualMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pVirtualMode, sizeof(nvmlGpuVirtualizationMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVirtualizationMode(device, &pVirtualMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pVirtualMode, sizeof(nvmlGpuVirtualizationMode_t)) < 0)
        return -1;

    return result;
}

/**
* Queries if SR-IOV host operation is supported on a vGPU supported device.
*
* Checks whether SR-IOV host capability is supported by the device and the
* driver, and indicates device is in SR-IOV mode if both of these conditions
* are true.
*
* @param device                                The identifier of the target device
* @param pHostVgpuMode                         Reference in which to return the current vGPU mode
*
* @return
*         - \ref NVML_SUCCESS                  if device's vGPU mode has been successfully retrieved
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device handle is 0 or \a pVgpuMode is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED      if \a device doesn't support this feature.
*         - \ref NVML_ERROR_UNKNOWN            if any unexpected error occurred
*/
int handle_nvmlDeviceGetHostVgpuMode(void *conn) {
    nvmlDevice_t device;
    nvmlHostVgpuMode_t pHostVgpuMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pHostVgpuMode, sizeof(nvmlHostVgpuMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetHostVgpuMode(device, &pHostVgpuMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHostVgpuMode, sizeof(nvmlHostVgpuMode_t)) < 0)
        return -1;

    return result;
}

/**
* This method is used to set the virtualization mode corresponding to the GPU.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                    Identifier of the target device
* @param virtualMode               virtualization mode. One of NVML_GPU_VIRTUALIZATION_?
*
* @return
*         - \ref NVML_SUCCESS                  if \a pVirtualMode is set
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device is invalid or \a pVirtualMode is NULL
*         - \ref NVML_ERROR_GPU_IS_LOST        if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_SUPPORTED      if setting of virtualization mode is not supported.
*         - \ref NVML_ERROR_NO_PERMISSION      if setting of virtualization mode is not allowed for this client.
*/
int handle_nvmlDeviceSetVirtualizationMode(void *conn) {
    nvmlDevice_t device;
    nvmlGpuVirtualizationMode_t virtualMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &virtualMode, sizeof(nvmlGpuVirtualizationMode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetVirtualizationMode(device, virtualMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieve the vGPU Software licensable features.
*
* Identifies whether the system supports vGPU Software Licensing. If it does, return the list of licensable feature(s)
* and their current license status.
*
* @param device                    Identifier of the target device
* @param pGridLicensableFeatures   Pointer to structure in which vGPU software licensable features are returned
*
* @return
*         - \ref NVML_SUCCESS                 if licensable features are successfully retrieved
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a pGridLicensableFeatures is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGridLicensableFeatures_v4(void *conn) {
    nvmlDevice_t device;
    nvmlGridLicensableFeatures_t pGridLicensableFeatures;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pGridLicensableFeatures, sizeof(nvmlGridLicensableFeatures_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGridLicensableFeatures_v4(device, &pGridLicensableFeatures);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGridLicensableFeatures, sizeof(nvmlGridLicensableFeatures_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current utilization and process ID
*
* For Maxwell &tm; or newer fully supported devices.
*
* Reads recent utilization of GPU SM (3D/Compute), framebuffer, video encoder, and video decoder for processes running.
* Utilization values are returned as an array of utilization sample structures in the caller-supplied buffer pointed at
* by \a utilization. One utilization sample structure is returned per process running, that had some non-zero utilization
* during the last sample period. It includes the CPU timestamp at which  the samples were recorded. Individual utilization values
* are returned as "unsigned int" values.
*
* To read utilization values, first determine the size of buffer required to hold the samples by invoking the function with
* \a utilization set to NULL. The caller should allocate a buffer of size
* processSamplesCount * sizeof(nvmlProcessUtilizationSample_t). Invoke the function again with the allocated buffer passed
* in \a utilization, and \a processSamplesCount set to the number of entries the buffer is sized for.
*
* On successful return, the function updates \a processSamplesCount with the number of process utilization sample
* structures that were actually written. This may differ from a previously read value as instances are created or
* destroyed.
*
* lastSeenTimeStamp represents the CPU timestamp in microseconds at which utilization samples were last read. Set it to 0
* to read utilization based on all the samples maintained by the driver's internal sample buffer. Set lastSeenTimeStamp
* to a timeStamp retrieved from a previous query to read utilization since the previous query.
*
* @note On MIG-enabled GPUs, querying process utilization is not currently supported.
*
* @param device                    The identifier of the target device
* @param utilization               Pointer to caller-supplied buffer in which guest process utilization samples are returned
* @param processSamplesCount       Pointer to caller-supplied array size, and returns number of processes running
* @param lastSeenTimeStamp         Return only samples with timestamp greater than lastSeenTimeStamp.
* @return
*         - \ref NVML_SUCCESS                 if \a utilization has been populated
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a utilization is NULL, or \a samplingPeriodUs is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_NOT_FOUND         if sample entries are not found
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetProcessUtilization(void *conn) {
    nvmlDevice_t device;
    nvmlProcessUtilizationSample_t utilization;
    unsigned int processSamplesCount;
    unsigned long long lastSeenTimeStamp;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &utilization, sizeof(nvmlProcessUtilizationSample_t)) < 0 ||
        rpc_read(conn, &processSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetProcessUtilization(device, &utilization, &processSamplesCount, lastSeenTimeStamp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &utilization, sizeof(nvmlProcessUtilizationSample_t)) < 0 ||
        rpc_write(conn, &processSamplesCount, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve GSP firmware version.
*
* The caller passes in buffer via \a version and corresponding GSP firmware numbered version
* is returned with the same parameter in string format.
*
* @param device                               Device handle
* @param version                              The retrieved GSP firmware version
*
* @return
*         - \ref NVML_SUCCESS                 if GSP firmware version is sucessfully retrieved
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or GSP \a version pointer is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if GSP firmware is not enabled for GPU
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGspFirmwareVersion(void *conn) {
    nvmlDevice_t device;
    char version;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &version, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGspFirmwareVersion(device, &version);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &version, sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieve GSP firmware mode.
*
* The caller passes in integer pointers. GSP firmware enablement and default mode information is returned with
* corresponding parameters. The return value in \a isEnabled and \a defaultMode should be treated as boolean.
*
* @param device                               Device handle
* @param isEnabled                            Pointer to specify if GSP firmware is enabled
* @param defaultMode                          Pointer to specify if GSP firmware is supported by default on \a device
*
* @return
*         - \ref NVML_SUCCESS                 if GSP firmware mode is sucessfully retrieved
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or any of \a isEnabled or \a defaultMode is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGspFirmwareMode(void *conn) {
    nvmlDevice_t device;
    unsigned int isEnabled;
    unsigned int defaultMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &defaultMode, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGspFirmwareMode(device, &isEnabled, &defaultMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &defaultMode, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the requested vGPU driver capability.
*
* Refer to the \a nvmlVgpuDriverCapability_t structure for the specific capabilities that can be queried.
* The return value in \a capResult should be treated as a boolean, with a non-zero value indicating that the capability
* is supported.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param capability      Specifies the \a nvmlVgpuDriverCapability_t to be queried
* @param capResult       A boolean for the queried capability indicating that feature is supported
*
* @return
*      - \ref NVML_SUCCESS                      successful completion
*      - \ref NVML_ERROR_UNINITIALIZED          if the library has not been successfully initialized
*      - \ref NVML_ERROR_INVALID_ARGUMENT       if \a capability is invalid, or \a capResult is NULL
*      - \ref NVML_ERROR_NOT_SUPPORTED          the API is not supported in current state or \a devices not in vGPU mode
*      - \ref NVML_ERROR_UNKNOWN                on any unexpected error
*/
int handle_nvmlGetVgpuDriverCapabilities(void *conn) {
    nvmlVgpuDriverCapability_t capability;
    unsigned int capResult;

    if (rpc_read(conn, &capability, sizeof(nvmlVgpuDriverCapability_t)) < 0 ||
        rpc_read(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGetVgpuDriverCapabilities(capability, &capResult);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the requested vGPU capability for GPU.
*
* Refer to the \a nvmlDeviceVgpuCapability_t structure for the specific capabilities that can be queried.
* The return value in \a capResult reports a non-zero value indicating that the capability
* is supported, and also reports the capability's data based on the queried capability.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param device     The identifier of the target device
* @param capability Specifies the \a nvmlDeviceVgpuCapability_t to be queried
* @param capResult  Specifies that the queried capability is supported, and also returns capability's data
*
* @return
*      - \ref NVML_SUCCESS                      successful completion
*      - \ref NVML_ERROR_UNINITIALIZED          if the library has not been successfully initialized
*      - \ref NVML_ERROR_INVALID_ARGUMENT       if \a device is invalid, or \a capability is invalid, or \a capResult is NULL
*      - \ref NVML_ERROR_NOT_SUPPORTED          the API is not supported in current state or \a device not in vGPU mode
*      - \ref NVML_ERROR_UNKNOWN                on any unexpected error
*/
int handle_nvmlDeviceGetVgpuCapabilities(void *conn) {
    nvmlDevice_t device;
    nvmlDeviceVgpuCapability_t capability;
    unsigned int capResult;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &capability, sizeof(nvmlDeviceVgpuCapability_t)) < 0 ||
        rpc_read(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVgpuCapabilities(device, capability, &capResult);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the supported vGPU types on a physical GPU (device).
*
* An array of supported vGPU types for the physical GPU indicated by \a device is returned in the caller-supplied buffer
* pointed at by \a vgpuTypeIds. The element count of nvmlVgpuTypeId_t array is passed in \a vgpuCount, and \a vgpuCount
* is used to return the number of vGPU types written to the buffer.
*
* If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \a vgpuCount.
* To query the number of vGPU types supported for the GPU, call this function with *vgpuCount = 0.
* The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are supported.
*
* @param device                   The identifier of the target device
* @param vgpuCount                Pointer to caller-supplied array size, and returns number of vGPU types
* @param vgpuTypeIds              Pointer to caller-supplied array in which to return list of vGPU types
*
* @return
*         - \ref NVML_SUCCESS                      successful completion
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE      \a vgpuTypeIds buffer is too small, array element count is returned in \a vgpuCount
*         - \ref NVML_ERROR_INVALID_ARGUMENT       if \a vgpuCount is NULL or \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED          if vGPU is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN                on any unexpected error
*/
int handle_nvmlDeviceGetSupportedVgpus(void *conn) {
    nvmlDevice_t device;
    unsigned int vgpuCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &vgpuCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlVgpuTypeId_t* vgpuTypeIds = (nvmlVgpuTypeId_t*)malloc(vgpuCount * sizeof(nvmlVgpuTypeId_t));

    nvmlReturn_t result = nvmlDeviceGetSupportedVgpus(device, &vgpuCount, vgpuTypeIds);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, vgpuTypeIds, vgpuCount * sizeof(nvmlVgpuTypeId_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the currently creatable vGPU types on a physical GPU (device).
*
* An array of creatable vGPU types for the physical GPU indicated by \a device is returned in the caller-supplied buffer
* pointed at by \a vgpuTypeIds. The element count of nvmlVgpuTypeId_t array is passed in \a vgpuCount, and \a vgpuCount
* is used to return the number of vGPU types written to the buffer.
*
* The creatable vGPU types for a device may differ over time, as there may be restrictions on what type of vGPU types
* can concurrently run on a device.  For example, if only one vGPU type is allowed at a time on a device, then the creatable
* list will be restricted to whatever vGPU type is already running on the device.
*
* If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \a vgpuCount.
* To query the number of vGPU types createable for the GPU, call this function with *vgpuCount = 0.
* The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are creatable.
*
* @param device                   The identifier of the target device
* @param vgpuCount                Pointer to caller-supplied array size, and returns number of vGPU types
* @param vgpuTypeIds              Pointer to caller-supplied array in which to return list of vGPU types
*
* @return
*         - \ref NVML_SUCCESS                      successful completion
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE      \a vgpuTypeIds buffer is too small, array element count is returned in \a vgpuCount
*         - \ref NVML_ERROR_INVALID_ARGUMENT       if \a vgpuCount is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED          if vGPU is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN                on any unexpected error
*/
int handle_nvmlDeviceGetCreatableVgpus(void *conn) {
    nvmlDevice_t device;
    unsigned int vgpuCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &vgpuCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlVgpuTypeId_t* vgpuTypeIds = (nvmlVgpuTypeId_t*)malloc(vgpuCount * sizeof(nvmlVgpuTypeId_t));

    nvmlReturn_t result = nvmlDeviceGetCreatableVgpus(device, &vgpuCount, vgpuTypeIds);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, vgpuTypeIds, vgpuCount * sizeof(nvmlVgpuTypeId_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the class of a vGPU type. It will not exceed 64 characters in length (including the NUL terminator).
* See \ref nvmlConstants::NVML_DEVICE_NAME_BUFFER_SIZE.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param vgpuTypeClass            Pointer to string array to return class in
* @param size                     Size of string
*
* @return
*         - \ref NVML_SUCCESS                   successful completion
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a vgpuTypeId is invalid, or \a vgpuTypeClass is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE   if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlVgpuTypeGetClass(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    char vgpuTypeClass;
    unsigned int size;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &vgpuTypeClass, sizeof(char)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetClass(vgpuTypeId, &vgpuTypeClass, &size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuTypeClass, sizeof(char)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the vGPU type name.
*
* The name is an alphanumeric string that denotes a particular vGPU, e.g. GRID M60-2Q. It will not
* exceed 64 characters in length (including the NUL terminator).  See \ref
* nvmlConstants::NVML_DEVICE_NAME_BUFFER_SIZE.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param vgpuTypeName             Pointer to buffer to return name
* @param size                     Size of buffer
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a name is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetName(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int size;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* vgpuTypeName = (char*)malloc(size * sizeof(char));

    nvmlReturn_t result = nvmlVgpuTypeGetName(vgpuTypeId, vgpuTypeName, &size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, vgpuTypeName, size * sizeof(char)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the GPU Instance Profile ID for the given vGPU type ID.
* The API will return a valid GPU Instance Profile ID for the MIG capable vGPU types, else INVALID_GPU_INSTANCE_PROFILE_ID is
* returned.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param gpuInstanceProfileId     GPU Instance Profile ID
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_NOT_SUPPORTED     if \a device is not in vGPU Host virtualization mode
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a gpuInstanceProfileId is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetGpuInstanceProfileId(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int gpuInstanceProfileId;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &gpuInstanceProfileId, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetGpuInstanceProfileId(vgpuTypeId, &gpuInstanceProfileId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuInstanceProfileId, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the device ID of a vGPU type.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param deviceID                 Device ID and vendor ID of the device contained in single 32 bit value
* @param subsystemID              Subsystem ID and subsystem vendor ID of the device contained in single 32 bit value
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a deviceId or \a subsystemID are NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetDeviceID(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned long long deviceID;
    unsigned long long subsystemID;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &deviceID, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &subsystemID, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetDeviceID(vgpuTypeId, &deviceID, &subsystemID);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &deviceID, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &subsystemID, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the vGPU framebuffer size in bytes.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param fbSize                   Pointer to framebuffer size in bytes
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a fbSize is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetFramebufferSize(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned long long fbSize;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &fbSize, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetFramebufferSize(vgpuTypeId, &fbSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fbSize, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* Retrieve count of vGPU's supported display heads.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param numDisplayHeads          Pointer to number of display heads
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a numDisplayHeads is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetNumDisplayHeads(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int numDisplayHeads;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &numDisplayHeads, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetNumDisplayHeads(vgpuTypeId, &numDisplayHeads);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numDisplayHeads, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve vGPU display head's maximum supported resolution.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param displayIndex             Zero-based index of display head
* @param xdim                     Pointer to maximum number of pixels in X dimension
* @param ydim                     Pointer to maximum number of pixels in Y dimension
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a xdim or \a ydim are NULL, or \a displayIndex
*                                             is out of range.
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetResolution(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int displayIndex;
    unsigned int xdim;
    unsigned int ydim;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &displayIndex, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &xdim, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &ydim, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetResolution(vgpuTypeId, displayIndex, &xdim, &ydim);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &xdim, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &ydim, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve license requirements for a vGPU type
*
* The license type and version required to run the specified vGPU type is returned as an alphanumeric string, in the form
* "<license name>,<version>", for example "GRID-Virtual-PC,2.0". If a vGPU is runnable with* more than one type of license,
* the licenses are delimited by a semicolon, for example "GRID-Virtual-PC,2.0;GRID-Virtual-WS,2.0;GRID-Virtual-WS-Ext,2.0".
*
* The total length of the returned string will not exceed 128 characters, including the NUL terminator.
* See \ref nvmlVgpuConstants::NVML_GRID_LICENSE_BUFFER_SIZE.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param vgpuTypeLicenseString    Pointer to buffer to return license info
* @param size                     Size of \a vgpuTypeLicenseString buffer
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a vgpuTypeLicenseString is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetLicense(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int size;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* vgpuTypeLicenseString = (char*)malloc(size * sizeof(char));

    nvmlReturn_t result = nvmlVgpuTypeGetLicense(vgpuTypeId, vgpuTypeLicenseString, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, vgpuTypeLicenseString, size * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the static frame rate limit value of the vGPU type
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param frameRateLimit           Reference to return the frame rate limit value
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_NOT_SUPPORTED     if frame rate limiter is turned off for the vGPU type
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a frameRateLimit is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetFrameRateLimit(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int frameRateLimit;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &frameRateLimit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetFrameRateLimit(vgpuTypeId, &frameRateLimit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &frameRateLimit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the maximum number of vGPU instances creatable on a device for given vGPU type
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                   The identifier of the target device
* @param vgpuTypeId               Handle to vGPU type
* @param vgpuInstanceCount        Pointer to get the max number of vGPU instances
*                                 that can be created on a deicve for given vgpuTypeId
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid or is not supported on target device,
*                                             or \a vgpuInstanceCount is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetMaxInstances(void *conn) {
    nvmlDevice_t device;
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int vgpuInstanceCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &vgpuInstanceCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetMaxInstances(device, vgpuTypeId, &vgpuInstanceCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuInstanceCount, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the maximum number of vGPU instances supported per VM for given vGPU type
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuTypeId               Handle to vGPU type
* @param vgpuInstanceCountPerVm   Pointer to get the max number of vGPU instances supported per VM for given \a vgpuTypeId
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a vgpuInstanceCountPerVm is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetMaxInstancesPerVm(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    unsigned int vgpuInstanceCountPerVm;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &vgpuInstanceCountPerVm, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetMaxInstancesPerVm(vgpuTypeId, &vgpuInstanceCountPerVm);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuInstanceCountPerVm, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the active vGPU instances on a device.
*
* An array of active vGPU instances is returned in the caller-supplied buffer pointed at by \a vgpuInstances. The
* array elememt count is passed in \a vgpuCount, and \a vgpuCount is used to return the number of vGPU instances
* written to the buffer.
*
* If the supplied buffer is not large enough to accommodate the vGPU instance array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuInstance_t array required in \a vgpuCount.
* To query the number of active vGPU instances, call this function with *vgpuCount = 0.  The code will return
* NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU Types are supported.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param device                   The identifier of the target device
* @param vgpuCount                Pointer which passes in the array size as well as get
*                                 back the number of types
* @param vgpuInstances            Pointer to array in which to return list of vGPU instances
*
* @return
*         - \ref NVML_SUCCESS                  successful completion
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a device is invalid, or \a vgpuCount is NULL
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE  if \a size is too small
*         - \ref NVML_ERROR_NOT_SUPPORTED      if vGPU is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlDeviceGetActiveVgpus(void *conn) {
    nvmlDevice_t device;
    unsigned int vgpuCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &vgpuCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlVgpuInstance_t* vgpuInstances = (nvmlVgpuInstance_t*)malloc(vgpuCount * sizeof(nvmlVgpuInstance_t));

    nvmlReturn_t result = nvmlDeviceGetActiveVgpus(device, &vgpuCount, vgpuInstances);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, vgpuInstances, vgpuCount * sizeof(nvmlVgpuInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the VM ID associated with a vGPU instance.
*
* The VM ID is returned as a string, not exceeding 80 characters in length (including the NUL terminator).
* See \ref nvmlConstants::NVML_DEVICE_UUID_BUFFER_SIZE.
*
* The format of the VM ID varies by platform, and is indicated by the type identifier returned in \a vmIdType.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param vmId                     Pointer to caller-supplied buffer to hold VM ID
* @param size                     Size of buffer in bytes
* @param vmIdType                 Pointer to hold VM ID type
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vmId or \a vmIdType is NULL, or \a vgpuInstance is 0
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetVmID(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int size;
    nvmlVgpuVmIdType_t vmIdType;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &vmIdType, sizeof(nvmlVgpuVmIdType_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* vmId = (char*)malloc(size * sizeof(char));

    nvmlReturn_t result = nvmlVgpuInstanceGetVmID(vgpuInstance, vmId, size, &vmIdType);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, vmId, size * sizeof(char)) < 0 ||
        rpc_write(conn, &vmIdType, sizeof(nvmlVgpuVmIdType_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the UUID of a vGPU instance.
*
* The UUID is a globally unique identifier associated with the vGPU, and is returned as a 5-part hexadecimal string,
* not exceeding 80 characters in length (including the NULL terminator).
* See \ref nvmlConstants::NVML_DEVICE_UUID_BUFFER_SIZE.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param uuid                     Pointer to caller-supplied buffer to hold vGPU UUID
* @param size                     Size of buffer in bytes
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a uuid is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a size is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetUUID(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int size;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* uuid = (char*)malloc(size * sizeof(char));

    nvmlReturn_t result = nvmlVgpuInstanceGetUUID(vgpuInstance, uuid, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, uuid, size * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the NVIDIA driver version installed in the VM associated with a vGPU.
*
* The version is returned as an alphanumeric string in the caller-supplied buffer \a version. The length of the version
* string will not exceed 80 characters in length (including the NUL terminator).
* See \ref nvmlConstants::NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE.
*
* nvmlVgpuInstanceGetVmDriverVersion() may be called at any time for a vGPU instance. The guest VM driver version is
* returned as "Not Available" if no NVIDIA driver is installed in the VM, or the VM has not yet booted to the point where the
* NVIDIA driver is loaded and initialized.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param version                  Caller-supplied buffer to return driver version string
* @param length                   Size of \a version buffer
*
* @return
*         - \ref NVML_SUCCESS                 if \a version has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetVmDriverVersion(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int length;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* version = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlVgpuInstanceGetVmDriverVersion(vgpuInstance, version, length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, version, length * sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the framebuffer usage in bytes.
*
* Framebuffer usage is the amont of vGPU framebuffer memory that is currently in use by the VM.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             The identifier of the target instance
* @param fbUsage                  Pointer to framebuffer usage in bytes
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a fbUsage is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetFbUsage(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned long long fbUsage;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &fbUsage, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetFbUsage(vgpuInstance, &fbUsage);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fbUsage, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* @deprecated Use \ref nvmlVgpuInstanceGetLicenseInfo_v2.
*
* Retrieve the current licensing state of the vGPU instance.
*
* If the vGPU is currently licensed, \a licensed is set to 1, otherwise it is set to 0.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param licensed                 Reference to return the licensing status
*
* @return
*         - \ref NVML_SUCCESS                 if \a licensed has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a licensed is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetLicenseStatus(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int licensed;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &licensed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetLicenseStatus(vgpuInstance, &licensed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &licensed, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the vGPU type of a vGPU instance.
*
* Returns the vGPU type ID of vgpu assigned to the vGPU instance.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param vgpuTypeId               Reference to return the vgpuTypeId
*
* @return
*         - \ref NVML_SUCCESS                 if \a vgpuTypeId has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a vgpuTypeId is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetType(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlVgpuTypeId_t vgpuTypeId;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetType(vgpuInstance, &vgpuTypeId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the frame rate limit set for the vGPU instance.
*
* Returns the value of the frame rate limit set for the vGPU instance
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param frameRateLimit           Reference to return the frame rate limit
*
* @return
*         - \ref NVML_SUCCESS                 if \a frameRateLimit has been set
*         - \ref NVML_ERROR_NOT_SUPPORTED     if frame rate limiter is turned off for the vGPU type
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a frameRateLimit is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetFrameRateLimit(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int frameRateLimit;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &frameRateLimit, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetFrameRateLimit(vgpuInstance, &frameRateLimit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &frameRateLimit, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the current ECC mode of vGPU instance.
*
* @param vgpuInstance            The identifier of the target vGPU instance
* @param eccMode                 Reference in which to return the current ECC mode
*
* @return
*         - \ref NVML_SUCCESS                 if the vgpuInstance's ECC mode has been successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a mode is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the vGPU doesn't support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetEccMode(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlEnableState_t eccMode;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &eccMode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetEccMode(vgpuInstance, &eccMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &eccMode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the encoder capacity of a vGPU instance, as a percentage of maximum encoder capacity with valid values in the range 0-100.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param encoderCapacity          Reference to an unsigned int for the encoder capacity
*
* @return
*         - \ref NVML_SUCCESS                 if \a encoderCapacity has been retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a encoderQueryType is invalid
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetEncoderCapacity(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int encoderCapacity;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &encoderCapacity, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetEncoderCapacity(vgpuInstance, &encoderCapacity);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &encoderCapacity, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Set the encoder capacity of a vGPU instance, as a percentage of maximum encoder capacity with valid values in the range 0-100.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance             Identifier of the target vGPU instance
* @param encoderCapacity          Unsigned int for the encoder capacity value
*
* @return
*         - \ref NVML_SUCCESS                 if \a encoderCapacity has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a encoderCapacity is out of range of 0-100.
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceSetEncoderCapacity(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int encoderCapacity;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &encoderCapacity, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceSetEncoderCapacity(vgpuInstance, encoderCapacity);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieves the current encoder statistics of a vGPU Instance
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance                      Identifier of the target vGPU instance
* @param sessionCount                      Reference to an unsigned int for count of active encoder sessions
* @param averageFps                        Reference to an unsigned int for trailing average FPS of all active sessions
* @param averageLatency                    Reference to an unsigned int for encode latency in microseconds
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionCount, \a averageFps and \a averageLatency is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a sessionCount , or \a averageFps or \a averageLatency is NULL
*                                              or \a vgpuInstance is 0.
*         - \ref NVML_ERROR_NOT_FOUND          if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlVgpuInstanceGetEncoderStats(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int sessionCount;
    unsigned int averageFps;
    unsigned int averageLatency;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &averageFps, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &averageLatency, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetEncoderStats(vgpuInstance, &sessionCount, &averageFps, &averageLatency);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &averageFps, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &averageLatency, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves information about all active encoder sessions on a vGPU Instance.
*
* An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \a sessionInfo. The
* array element count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
* written to the buffer.
*
* If the supplied buffer is not large enough to accommodate the active session array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \a sessionCount.
* To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return
* NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance                      Identifier of the target vGPU instance
* @param sessionCount                      Reference to caller supplied array size, and returns
*                                          the number of sessions.
* @param sessionInfo                       Reference to caller supplied array in which the list
*                                          of session information us returned.
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionInfo is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE  if \a sessionCount is too small, array element count is
                                                returned in \a sessionCount
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a sessionCount is NULL, or \a vgpuInstance is 0.
*         - \ref NVML_ERROR_NOT_FOUND          if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlVgpuInstanceGetEncoderSessions(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int sessionCount;
    nvmlEncoderSessionInfo_t sessionInfo;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &sessionInfo, sizeof(nvmlEncoderSessionInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetEncoderSessions(vgpuInstance, &sessionCount, &sessionInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &sessionInfo, sizeof(nvmlEncoderSessionInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the active frame buffer capture sessions statistics of a vGPU Instance
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance                      Identifier of the target vGPU instance
* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats
*
* @return
*         - \ref NVML_SUCCESS                  if \a fbcStats is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a vgpuInstance is 0, or \a fbcStats is NULL
*         - \ref NVML_ERROR_NOT_FOUND          if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlVgpuInstanceGetFBCStats(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlFBCStats_t fbcStats;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &fbcStats, sizeof(nvmlFBCStats_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetFBCStats(vgpuInstance, &fbcStats);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fbcStats, sizeof(nvmlFBCStats_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves information about active frame buffer capture sessions on a vGPU Instance.
*
* An array of active FBC sessions is returned in the caller-supplied buffer pointed at by \a sessionInfo. The
* array element count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
* written to the buffer.
*
* If the supplied buffer is not large enough to accomodate the active session array, the function returns
* NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlFBCSessionInfo_t array required in \a sessionCount.
* To query the number of active FBC sessions, call this function with *sessionCount = 0.  The code will return
* NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @note hResolution, vResolution, averageFPS and averageLatency data for a FBC session returned in \a sessionInfo may
*       be zero if there are no new frames captured since the session started.
*
* @param vgpuInstance                      Identifier of the target vGPU instance
* @param sessionCount                      Reference to caller supplied array size, and returns the number of sessions.
* @param sessionInfo                       Reference in which to return the session information
*
* @return
*         - \ref NVML_SUCCESS                  if \a sessionInfo is fetched
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a vgpuInstance is 0, or \a sessionCount is NULL.
*         - \ref NVML_ERROR_NOT_FOUND          if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE  if \a sessionCount is too small, array element count is returned in \a sessionCount
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlVgpuInstanceGetFBCSessions(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int sessionCount;
    nvmlFBCSessionInfo_t sessionInfo;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &sessionInfo, sizeof(nvmlFBCSessionInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetFBCSessions(vgpuInstance, &sessionCount, &sessionInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &sessionInfo, sizeof(nvmlFBCSessionInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the GPU Instance ID for the given vGPU Instance.
* The API will return a valid GPU Instance ID for MIG backed vGPU Instance, else INVALID_GPU_INSTANCE_ID is returned.
*
* For Kepler &tm; or newer fully supported devices.
*
* @param vgpuInstance                      Identifier of the target vGPU instance
* @param gpuInstanceId                     GPU Instance ID
*
* @return
*         - \ref NVML_SUCCESS                  successful completion
*         - \ref NVML_ERROR_UNINITIALIZED      if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a vgpuInstance is 0, or \a gpuInstanceId is NULL.
*         - \ref NVML_ERROR_NOT_FOUND          if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN            on any unexpected error
*/
int handle_nvmlVgpuInstanceGetGpuInstanceId(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int gpuInstanceId;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &gpuInstanceId, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetGpuInstanceId(vgpuInstance, &gpuInstanceId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuInstanceId, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the PCI Id of the given vGPU Instance i.e. the PCI Id of the GPU as seen inside the VM.
*
* The vGPU PCI id is returned as "00000000:00:00.0" if NVIDIA driver is not installed on the vGPU instance.
*
* @param vgpuInstance                         Identifier of the target vGPU instance
* @param vgpuPciId                            Caller-supplied buffer to return vGPU PCI Id string
* @param length                               Size of the vgpuPciId buffer
*
* @return
*         - \ref NVML_SUCCESS                 if vGPU PCI Id is sucessfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a vgpuPciId is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_DRIVER_NOT_LOADED if NVIDIA driver is not running on the vGPU instance
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a length is too small, \a length is set to required length
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetGpuPciId(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int length;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* vgpuPciId = (char*)malloc(length * sizeof(char));

    nvmlReturn_t result = nvmlVgpuInstanceGetGpuPciId(vgpuInstance, vgpuPciId, &length);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, vgpuPciId, length * sizeof(char)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the requested capability for a given vGPU type. Refer to the \a nvmlVgpuCapability_t structure
* for the specific capabilities that can be queried. The return value in \a capResult should be treated as
* a boolean, with a non-zero value indicating that the capability is supported.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuTypeId                           Handle to vGPU type
* @param capability                           Specifies the \a nvmlVgpuCapability_t to be queried
* @param capResult                            A boolean for the queried capability indicating that feature is supported
*
* @return
*         - \ref NVML_SUCCESS                 successful completion
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuTypeId is invalid, or \a capability is invalid, or \a capResult is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuTypeGetCapabilities(void *conn) {
    nvmlVgpuTypeId_t vgpuTypeId;
    nvmlVgpuCapability_t capability;
    unsigned int capResult;

    if (rpc_read(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &capability, sizeof(nvmlVgpuCapability_t)) < 0 ||
        rpc_read(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuTypeGetCapabilities(vgpuTypeId, capability, &capResult);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &capResult, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Returns vGPU metadata structure for a running vGPU. The structure contains information about the vGPU and its associated VM
* such as the currently installed NVIDIA guest driver version, together with host driver version and an opaque data section
* containing internal state.
*
* nvmlVgpuInstanceGetMetadata() may be called at any time for a vGPU instance. Some fields in the returned structure are
* dependent on information obtained from the guest VM, which may not yet have reached a state where that information
* is available. The current state of these dependent fields is reflected in the info structure's \ref nvmlVgpuGuestInfoState_t field.
*
* The VMM may choose to read and save the vGPU's VM info as persistent metadata associated with the VM, and provide
* it to Virtual GPU Manager when creating a vGPU for subsequent instances of the VM.
*
* The caller passes in a buffer via \a vgpuMetadata, with the size of the buffer in \a bufferSize. If the vGPU Metadata structure
* is too large to fit in the supplied buffer, the function returns NVML_ERROR_INSUFFICIENT_SIZE with the size needed
* in \a bufferSize.
*
* @param vgpuInstance             vGPU instance handle
* @param vgpuMetadata             Pointer to caller-supplied buffer into which vGPU metadata is written
* @param bufferSize               Size of vgpuMetadata buffer
*
* @return
*         - \ref NVML_SUCCESS                   vGPU metadata structure was successfully returned
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE   vgpuMetadata buffer is too small, required size is returned in \a bufferSize
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a bufferSize is NULL or \a vgpuInstance is 0; if \a vgpuMetadata is NULL and the value of \a bufferSize is not 0.
*         - \ref NVML_ERROR_NOT_FOUND           if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlVgpuInstanceGetMetadata(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlVgpuMetadata_t vgpuMetadata;
    unsigned int bufferSize;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_read(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetMetadata(vgpuInstance, &vgpuMetadata, &bufferSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_write(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Returns a vGPU metadata structure for the physical GPU indicated by \a device. The structure contains information about
* the GPU and the currently installed NVIDIA host driver version that's controlling it, together with an opaque data section
* containing internal state.
*
* The caller passes in a buffer via \a pgpuMetadata, with the size of the buffer in \a bufferSize. If the \a pgpuMetadata
* structure is too large to fit in the supplied buffer, the function returns NVML_ERROR_INSUFFICIENT_SIZE with the size needed
* in \a bufferSize.
*
* @param device                The identifier of the target device
* @param pgpuMetadata          Pointer to caller-supplied buffer into which \a pgpuMetadata is written
* @param bufferSize            Pointer to size of \a pgpuMetadata buffer
*
* @return
*         - \ref NVML_SUCCESS                   GPU metadata structure was successfully returned
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE   pgpuMetadata buffer is too small, required size is returned in \a bufferSize
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a bufferSize is NULL or \a device is invalid; if \a pgpuMetadata is NULL and the value of \a bufferSize is not 0.
*         - \ref NVML_ERROR_NOT_SUPPORTED       vGPU is not supported by the system
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetVgpuMetadata(void *conn) {
    nvmlDevice_t device;
    nvmlVgpuPgpuMetadata_t pgpuMetadata;
    unsigned int bufferSize;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pgpuMetadata, sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_read(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVgpuMetadata(device, &pgpuMetadata, &bufferSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pgpuMetadata, sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_write(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Takes a vGPU instance metadata structure read from \ref nvmlVgpuInstanceGetMetadata(), and a vGPU metadata structure for a
* physical GPU read from \ref nvmlDeviceGetVgpuMetadata(), and returns compatibility information of the vGPU instance and the
* physical GPU.
*
* The caller passes in a buffer via \a compatibilityInfo, into which a compatibility information structure is written. The
* structure defines the states in which the vGPU / VM may be booted on the physical GPU. If the vGPU / VM compatibility
* with the physical GPU is limited, a limit code indicates the factor limiting compatibility.
* (see \ref nvmlVgpuPgpuCompatibilityLimitCode_t for details).
*
* Note: vGPU compatibility does not take into account dynamic capacity conditions that may limit a system's ability to
*       boot a given vGPU or associated VM.
*
* @param vgpuMetadata          Pointer to caller-supplied vGPU metadata structure
* @param pgpuMetadata          Pointer to caller-supplied GPU metadata structure
* @param compatibilityInfo     Pointer to caller-supplied buffer to hold compatibility info
*
* @return
*         - \ref NVML_SUCCESS                   vGPU metadata structure was successfully returned
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a vgpuMetadata or \a pgpuMetadata or \a bufferSize are NULL
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlGetVgpuCompatibility(void *conn) {
    nvmlVgpuMetadata_t vgpuMetadata;
    nvmlVgpuPgpuMetadata_t pgpuMetadata;
    nvmlVgpuPgpuCompatibility_t compatibilityInfo;

    if (rpc_read(conn, &vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_read(conn, &pgpuMetadata, sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_read(conn, &compatibilityInfo, sizeof(nvmlVgpuPgpuCompatibility_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGetVgpuCompatibility(&vgpuMetadata, &pgpuMetadata, &compatibilityInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_write(conn, &pgpuMetadata, sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_write(conn, &compatibilityInfo, sizeof(nvmlVgpuPgpuCompatibility_t)) < 0)
        return -1;

    return result;
}

/**
* Returns the properties of the physical GPU indicated by the device in an ascii-encoded string format.
*
* The caller passes in a buffer via \a pgpuMetadata, with the size of the buffer in \a bufferSize. If the
* string is too large to fit in the supplied buffer, the function returns NVML_ERROR_INSUFFICIENT_SIZE with the size needed
* in \a bufferSize.
*
* @param device                The identifier of the target device
* @param pgpuMetadata          Pointer to caller-supplied buffer into which \a pgpuMetadata is written
* @param bufferSize            Pointer to size of \a pgpuMetadata buffer
*
* @return
*         - \ref NVML_SUCCESS                   GPU metadata structure was successfully returned
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE   \a pgpuMetadata buffer is too small, required size is returned in \a bufferSize
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a bufferSize is NULL or \a device is invalid; if \a pgpuMetadata is NULL and the value of \a bufferSize is not 0.
*         - \ref NVML_ERROR_NOT_SUPPORTED       if vGPU is not supported by the system
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetPgpuMetadataString(void *conn) {
    nvmlDevice_t device;
    unsigned int bufferSize;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    char* pgpuMetadata = (char*)malloc(bufferSize * sizeof(char));

    nvmlReturn_t result = nvmlDeviceGetPgpuMetadataString(device, pgpuMetadata, &bufferSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, pgpuMetadata, bufferSize * sizeof(char)) < 0 ||
        rpc_write(conn, &bufferSize, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Returns the vGPU Software scheduler logs.
* \a pSchedulerLog points to a caller-allocated structure to contain the logs. The number of elements returned will
* never exceed \a NVML_SCHEDULER_SW_MAX_LOG_ENTRIES.
*
* To get the entire logs, call the function atleast 5 times a second.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                The identifier of the target \a device
* @param pSchedulerLog         Reference in which \a pSchedulerLog is written
*
* @return
*         - \ref NVML_SUCCESS                   vGPU scheduler logs were successfully obtained
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a pSchedulerLog is NULL or \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED       The API is not supported in current state or \a device not in vGPU host mode
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetVgpuSchedulerLog(void *conn) {
    nvmlDevice_t device;
    nvmlVgpuSchedulerLog_t pSchedulerLog;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pSchedulerLog, sizeof(nvmlVgpuSchedulerLog_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVgpuSchedulerLog(device, &pSchedulerLog);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pSchedulerLog, sizeof(nvmlVgpuSchedulerLog_t)) < 0)
        return -1;

    return result;
}

/**
* Returns the vGPU scheduler state.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                The identifier of the target \a device
* @param pSchedulerState       Reference in which \a pSchedulerState is returned
*
* @return
*         - \ref NVML_SUCCESS                   vGPU scheduler state is successfully obtained
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a pSchedulerState is NULL or \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED       The API is not supported in current state or \a device not in vGPU host mode
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetVgpuSchedulerState(void *conn) {
    nvmlDevice_t device;
    nvmlVgpuSchedulerGetState_t pSchedulerState;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pSchedulerState, sizeof(nvmlVgpuSchedulerGetState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVgpuSchedulerState(device, &pSchedulerState);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pSchedulerState, sizeof(nvmlVgpuSchedulerGetState_t)) < 0)
        return -1;

    return result;
}

/**
* Returns the vGPU scheduler capabilities.
* The list of supported vGPU schedulers returned in \a nvmlVgpuSchedulerCapabilities_t is from
* the NVML_VGPU_SCHEDULER_POLICY_*. This list enumerates the supported scheduler policies
* if the engine is Graphics type.
* The other values in \a nvmlVgpuSchedulerCapabilities_t are also applicable if the engine is
* Graphics type. For other engine types, it is BEST EFFORT policy.
* If ARR is supported and enabled, scheduling frequency and averaging factor are applicable
* else timeSlice is applicable.
*
* For Pascal &tm; or newer fully supported devices.
*
* @param device                The identifier of the target \a device
* @param pCapabilities         Reference in which \a pCapabilities is written
*
* @return
*         - \ref NVML_SUCCESS                   vGPU scheduler capabilities were successfully obtained
*         - \ref NVML_ERROR_INVALID_ARGUMENT    if \a pCapabilities is NULL or \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED       The API is not supported in current state or \a device not in vGPU host mode
*         - \ref NVML_ERROR_UNKNOWN             on any unexpected error
*/
int handle_nvmlDeviceGetVgpuSchedulerCapabilities(void *conn) {
    nvmlDevice_t device;
    nvmlVgpuSchedulerCapabilities_t pCapabilities;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pCapabilities, sizeof(nvmlVgpuSchedulerCapabilities_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetVgpuSchedulerCapabilities(device, &pCapabilities);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pCapabilities, sizeof(nvmlVgpuSchedulerCapabilities_t)) < 0)
        return -1;

    return result;
}

/**
* Query the ranges of supported vGPU versions.
*
* This function gets the linear range of supported vGPU versions that is preset for the NVIDIA vGPU Manager and the range set by an administrator.
* If the preset range has not been overridden by \ref nvmlSetVgpuVersion, both ranges are the same.
*
* The caller passes pointers to the following \ref nvmlVgpuVersion_t structures, into which the NVIDIA vGPU Manager writes the ranges:
* 1. \a supported structure that represents the preset range of vGPU versions supported by the NVIDIA vGPU Manager.
* 2. \a current structure that represents the range of supported vGPU versions set by an administrator. By default, this range is the same as the preset range.
*
* @param supported  Pointer to the structure in which the preset range of vGPU versions supported by the NVIDIA vGPU Manager is written
* @param current    Pointer to the structure in which the range of supported vGPU versions set by an administrator is written
*
* @return
* - \ref NVML_SUCCESS                 The vGPU version range structures were successfully obtained.
* - \ref NVML_ERROR_NOT_SUPPORTED     The API is not supported.
* - \ref NVML_ERROR_INVALID_ARGUMENT  The \a supported parameter or the \a current parameter is NULL.
* - \ref NVML_ERROR_UNKNOWN           An error occurred while the data was being fetched.
*/
int handle_nvmlGetVgpuVersion(void *conn) {
    nvmlVgpuVersion_t supported;
    nvmlVgpuVersion_t current;

    if (rpc_read(conn, &supported, sizeof(nvmlVgpuVersion_t)) < 0 ||
        rpc_read(conn, &current, sizeof(nvmlVgpuVersion_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGetVgpuVersion(&supported, &current);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &supported, sizeof(nvmlVgpuVersion_t)) < 0 ||
        rpc_write(conn, &current, sizeof(nvmlVgpuVersion_t)) < 0)
        return -1;

    return result;
}

/**
* Override the preset range of vGPU versions supported by the NVIDIA vGPU Manager with a range set by an administrator.
*
* This function configures the NVIDIA vGPU Manager with a range of supported vGPU versions set by an administrator. This range must be a subset of the
* preset range that the NVIDIA vGPU Manager supports. The custom range set by an administrator takes precedence over the preset range and is advertised to
* the guest VM for negotiating the vGPU version. See \ref nvmlGetVgpuVersion for details of how to query the preset range of versions supported.
*
* This function takes a pointer to vGPU version range structure \ref nvmlVgpuVersion_t as input to override the preset vGPU version range that the NVIDIA vGPU Manager supports.
*
* After host system reboot or driver reload, the range of supported versions reverts to the range that is preset for the NVIDIA vGPU Manager.
*
* @note 1. The range set by the administrator must be a subset of the preset range that the NVIDIA vGPU Manager supports. Otherwise, an error is returned.
*       2. If the range of supported guest driver versions does not overlap the range set by the administrator, the guest driver fails to load.
*       3. If the range of supported guest driver versions overlaps the range set by the administrator, the guest driver will load with a negotiated
*          vGPU version that is the maximum value in the overlapping range.
*       4. No VMs must be running on the host when this function is called. If a VM is running on the host, the call to this function fails.
*
* @param vgpuVersion   Pointer to a caller-supplied range of supported vGPU versions.
*
* @return
* - \ref NVML_SUCCESS                 The preset range of supported vGPU versions was successfully overridden.
* - \ref NVML_ERROR_NOT_SUPPORTED     The API is not supported.
* - \ref NVML_ERROR_IN_USE            The range was not overridden because a VM is running on the host.
* - \ref NVML_ERROR_INVALID_ARGUMENT  The \a vgpuVersion parameter specifies a range that is outside the range supported by the NVIDIA vGPU Manager or if \a vgpuVersion is NULL.
*/
int handle_nvmlSetVgpuVersion(void *conn) {
    nvmlVgpuVersion_t vgpuVersion;

    if (rpc_read(conn, &vgpuVersion, sizeof(nvmlVgpuVersion_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlSetVgpuVersion(&vgpuVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuVersion, sizeof(nvmlVgpuVersion_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves current utilization for vGPUs on a physical GPU (device).
*
* For Kepler &tm; or newer fully supported devices.
*
* Reads recent utilization of GPU SM (3D/Compute), framebuffer, video encoder, and video decoder for vGPU instances running
* on a device. Utilization values are returned as an array of utilization sample structures in the caller-supplied buffer
* pointed at by \a utilizationSamples. One utilization sample structure is returned per vGPU instance, and includes the
* CPU timestamp at which the samples were recorded. Individual utilization values are returned as "unsigned int" values
* in nvmlValue_t unions. The function sets the caller-supplied \a sampleValType to NVML_VALUE_TYPE_UNSIGNED_INT to
* indicate the returned value type.
*
* To read utilization values, first determine the size of buffer required to hold the samples by invoking the function with
* \a utilizationSamples set to NULL. The function will return NVML_ERROR_INSUFFICIENT_SIZE, with the current vGPU instance
* count in \a vgpuInstanceSamplesCount, or NVML_SUCCESS if the current vGPU instance count is zero. The caller should allocate
* a buffer of size vgpuInstanceSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t). Invoke the function again with
* the allocated buffer passed in \a utilizationSamples, and \a vgpuInstanceSamplesCount set to the number of entries the
* buffer is sized for.
*
* On successful return, the function updates \a vgpuInstanceSampleCount with the number of vGPU utilization sample
* structures that were actually written. This may differ from a previously read value as vGPU instances are created or
* destroyed.
*
* lastSeenTimeStamp represents the CPU timestamp in microseconds at which utilization samples were last read. Set it to 0
* to read utilization based on all the samples maintained by the driver's internal sample buffer. Set lastSeenTimeStamp
* to a timeStamp retrieved from a previous query to read utilization since the previous query.
*
* @param device                        The identifier for the target device
* @param lastSeenTimeStamp             Return only samples with timestamp greater than lastSeenTimeStamp.
* @param sampleValType                 Pointer to caller-supplied buffer to hold the type of returned sample values
* @param vgpuInstanceSamplesCount      Pointer to caller-supplied array size, and returns number of vGPU instances
* @param utilizationSamples            Pointer to caller-supplied buffer in which vGPU utilization samples are returned
* @return
*         - \ref NVML_SUCCESS                 if utilization samples are successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a vgpuInstanceSamplesCount or \a sampleValType is
*                                             NULL, or a sample count of 0 is passed with a non-NULL \a utilizationSamples
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if supplied \a vgpuInstanceSamplesCount is too small to return samples for all
*                                             vGPU instances currently executing on the device
*         - \ref NVML_ERROR_NOT_SUPPORTED     if vGPU is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_FOUND         if sample entries are not found
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetVgpuUtilization(void *conn) {
    nvmlDevice_t device;
    unsigned long long lastSeenTimeStamp;
    nvmlValueType_t sampleValType;
    unsigned int vgpuInstanceSamplesCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_read(conn, &vgpuInstanceSamplesCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlVgpuInstanceUtilizationSample_t* utilizationSamples = (nvmlVgpuInstanceUtilizationSample_t*)malloc(vgpuInstanceSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t));

    nvmlReturn_t result = nvmlDeviceGetVgpuUtilization(device, lastSeenTimeStamp, &sampleValType, &vgpuInstanceSamplesCount, utilizationSamples);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_write(conn, &vgpuInstanceSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, utilizationSamples, vgpuInstanceSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves current utilization for processes running on vGPUs on a physical GPU (device).
*
* For Maxwell &tm; or newer fully supported devices.
*
* Reads recent utilization of GPU SM (3D/Compute), framebuffer, video encoder, and video decoder for processes running on
* vGPU instances active on a device. Utilization values are returned as an array of utilization sample structures in the
* caller-supplied buffer pointed at by \a utilizationSamples. One utilization sample structure is returned per process running
* on vGPU instances, that had some non-zero utilization during the last sample period. It includes the CPU timestamp at which
* the samples were recorded. Individual utilization values are returned as "unsigned int" values.
*
* To read utilization values, first determine the size of buffer required to hold the samples by invoking the function with
* \a utilizationSamples set to NULL. The function will return NVML_ERROR_INSUFFICIENT_SIZE, with the current vGPU instance
* count in \a vgpuProcessSamplesCount. The caller should allocate a buffer of size
* vgpuProcessSamplesCount * sizeof(nvmlVgpuProcessUtilizationSample_t). Invoke the function again with
* the allocated buffer passed in \a utilizationSamples, and \a vgpuProcessSamplesCount set to the number of entries the
* buffer is sized for.
*
* On successful return, the function updates \a vgpuSubProcessSampleCount with the number of vGPU sub process utilization sample
* structures that were actually written. This may differ from a previously read value depending on the number of processes that are active
* in any given sample period.
*
* lastSeenTimeStamp represents the CPU timestamp in microseconds at which utilization samples were last read. Set it to 0
* to read utilization based on all the samples maintained by the driver's internal sample buffer. Set lastSeenTimeStamp
* to a timeStamp retrieved from a previous query to read utilization since the previous query.
*
* @param device                        The identifier for the target device
* @param lastSeenTimeStamp             Return only samples with timestamp greater than lastSeenTimeStamp.
* @param vgpuProcessSamplesCount       Pointer to caller-supplied array size, and returns number of processes running on vGPU instances
* @param utilizationSamples            Pointer to caller-supplied buffer in which vGPU sub process utilization samples are returned
* @return
*         - \ref NVML_SUCCESS                 if utilization samples are successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid, \a vgpuProcessSamplesCount or a sample count of 0 is
*                                             passed with a non-NULL \a utilizationSamples
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if supplied \a vgpuProcessSamplesCount is too small to return samples for all
*                                             vGPU instances currently executing on the device
*         - \ref NVML_ERROR_NOT_SUPPORTED     if vGPU is not supported by the device
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_NOT_FOUND         if sample entries are not found
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetVgpuProcessUtilization(void *conn) {
    nvmlDevice_t device;
    unsigned long long lastSeenTimeStamp;
    unsigned int vgpuProcessSamplesCount;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &vgpuProcessSamplesCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlVgpuProcessUtilizationSample_t* utilizationSamples = (nvmlVgpuProcessUtilizationSample_t*)malloc(vgpuProcessSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t));

    nvmlReturn_t result = nvmlDeviceGetVgpuProcessUtilization(device, lastSeenTimeStamp, &vgpuProcessSamplesCount, utilizationSamples);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &vgpuProcessSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, utilizationSamples, vgpuProcessSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t)) < 0)
        return -1;

    return result;
}

/**
* Queries the state of per process accounting mode on vGPU.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance            The identifier of the target vGPU instance
* @param mode                    Reference in which to return the current accounting mode
*
* @return
*         - \ref NVML_SUCCESS                 if the mode has been successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a mode is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the vGPU doesn't support this feature
*         - \ref NVML_ERROR_DRIVER_NOT_LOADED if NVIDIA driver is not running on the vGPU instance
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetAccountingMode(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlEnableState_t mode;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetAccountingMode(vgpuInstance, &mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0)
        return -1;

    return result;
}

/**
* Queries list of processes running on vGPU that can be queried for accounting stats. The list of processes
* returned can be in running or terminated state.
*
* For Maxwell &tm; or newer fully supported devices.
*
* To just query the maximum number of processes that can be queried, call this function with *count = 0 and
* pids=NULL. The return code will be NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if list is empty.
*
* For more details see \ref nvmlVgpuInstanceGetAccountingStats.
*
* @note In case of PID collision some processes might not be accessible before the circular buffer is full.
*
* @param vgpuInstance            The identifier of the target vGPU instance
* @param count                   Reference in which to provide the \a pids array size, and
*                                to return the number of elements ready to be queried
* @param pids                    Reference in which to return list of process ids
*
* @return
*         - \ref NVML_SUCCESS                 if pids were successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a count is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the vGPU doesn't support this feature or accounting mode is disabled
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if \a count is too small (\a count is set to expected value)
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*
* @see nvmlVgpuInstanceGetAccountingPids
*/
int handle_nvmlVgpuInstanceGetAccountingPids(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int count;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    unsigned int* pids = (unsigned int*)malloc(count * sizeof(unsigned int));

    nvmlReturn_t result = nvmlVgpuInstanceGetAccountingPids(vgpuInstance, &count, pids);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, pids, count * sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Queries process's accounting stats.
*
* For Maxwell &tm; or newer fully supported devices.
*
* Accounting stats capture GPU utilization and other statistics across the lifetime of a process, and
* can be queried during life time of the process or after its termination.
* The time field in \ref nvmlAccountingStats_t is reported as 0 during the lifetime of the process and
* updated to actual running time after its termination.
* Accounting stats are kept in a circular buffer, newly created processes overwrite information about old
* processes.
*
* See \ref nvmlAccountingStats_t for description of each returned metric.
* List of processes that can be queried can be retrieved from \ref nvmlVgpuInstanceGetAccountingPids.
*
* @note Accounting Mode needs to be on. See \ref nvmlVgpuInstanceGetAccountingMode.
* @note Only compute and graphics applications stats can be queried. Monitoring applications stats can't be
*         queried since they don't contribute to GPU utilization.
* @note In case of pid collision stats of only the latest process (that terminated last) will be reported
*
* @param vgpuInstance            The identifier of the target vGPU instance
* @param pid                     Process Id of the target process to query stats for
* @param stats                   Reference in which to return the process's accounting stats
*
* @return
*         - \ref NVML_SUCCESS                 if stats have been successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a stats is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*                                             or \a stats is not found
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the vGPU doesn't support this feature or accounting mode is disabled
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetAccountingStats(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    unsigned int pid;
    nvmlAccountingStats_t stats;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stats, sizeof(nvmlAccountingStats_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetAccountingStats(vgpuInstance, pid, &stats);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &stats, sizeof(nvmlAccountingStats_t)) < 0)
        return -1;

    return result;
}

/**
* Clears accounting information of the vGPU instance that have already terminated.
*
* For Maxwell &tm; or newer fully supported devices.
* Requires root/admin permissions.
*
* @note Accounting Mode needs to be on. See \ref nvmlVgpuInstanceGetAccountingMode.
* @note Only compute and graphics applications stats are reported and can be cleared since monitoring applications
*         stats don't contribute to GPU utilization.
*
* @param vgpuInstance            The identifier of the target vGPU instance
*
* @return
*         - \ref NVML_SUCCESS                 if accounting information has been cleared
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is invalid
*         - \ref NVML_ERROR_NO_PERMISSION     if the user doesn't have permission to perform this operation
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the vGPU doesn't support this feature or accounting mode is disabled
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceClearAccountingPids(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceClearAccountingPids(vgpuInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Query the license information of the vGPU instance.
*
* For Maxwell &tm; or newer fully supported devices.
*
* @param vgpuInstance              Identifier of the target vGPU instance
* @param licenseInfo               Pointer to vGPU license information structure
*
* @return
*         - \ref NVML_SUCCESS                 if information is successfully retrieved
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a licenseInfo is NULL
*         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
*         - \ref NVML_ERROR_DRIVER_NOT_LOADED if NVIDIA driver is not running on the vGPU instance
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlVgpuInstanceGetLicenseInfo_v2(void *conn) {
    nvmlVgpuInstance_t vgpuInstance;
    nvmlVgpuLicenseInfo_t licenseInfo;

    if (rpc_read(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &licenseInfo, sizeof(nvmlVgpuLicenseInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance, &licenseInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &licenseInfo, sizeof(nvmlVgpuLicenseInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieves the number of excluded GPU devices in the system.
*
* For all products.
*
* @param deviceCount                          Reference in which to return the number of excluded devices
*
* @return
*         - \ref NVML_SUCCESS                 if \a deviceCount has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a deviceCount is NULL
*/
int handle_nvmlGetExcludedDeviceCount(void *conn) {
    unsigned int deviceCount;

    if (rpc_read(conn, &deviceCount, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGetExcludedDeviceCount(&deviceCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &deviceCount, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Acquire the device information for an excluded GPU device, based on its index.
*
* For all products.
*
* Valid indices are derived from the \a deviceCount returned by
*   \ref nvmlGetExcludedDeviceCount(). For example, if \a deviceCount is 2 the valid indices
*   are 0 and 1, corresponding to GPU 0 and GPU 1.
*
* @param index                                The index of the target GPU, >= 0 and < \a deviceCount
* @param info                                 Reference in which to return the device information
*
* @return
*         - \ref NVML_SUCCESS                  if \a device has been set
*         - \ref NVML_ERROR_INVALID_ARGUMENT   if \a index is invalid or \a info is NULL
*
* @see nvmlGetExcludedDeviceCount
*/
int handle_nvmlGetExcludedDeviceInfoByIndex(void *conn) {
    unsigned int index;
    nvmlExcludedDeviceInfo_t info;

    if (rpc_read(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlExcludedDeviceInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGetExcludedDeviceInfoByIndex(index, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlExcludedDeviceInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Set MIG mode for the device.
*
* For Ampere &tm; or newer fully supported devices.
* Requires root user.
*
* This mode determines whether a GPU instance can be created.
*
* This API may unbind or reset the device to activate the requested mode. Thus, the attributes associated with the
* device, such as minor number, might change. The caller of this API is expected to query such attributes again.
*
* On certain platforms like pass-through virtualization, where reset functionality may not be exposed directly, VM
* reboot is required. \a activationStatus would return \ref NVML_ERROR_RESET_REQUIRED for such cases.
*
* \a activationStatus would return the appropriate error code upon unsuccessful activation. For example, if device
* unbind fails because the device isn't idle, \ref NVML_ERROR_IN_USE would be returned. The caller of this API
* is expected to idle the device and retry setting the \a mode.
*
* @note On Windows, only disabling MIG mode is supported. \a activationStatus would return \ref
*       NVML_ERROR_NOT_SUPPORTED as GPU reset is not supported on Windows through this API.
*
* @param device                               The identifier of the target device
* @param mode                                 The mode to be set, \ref NVML_DEVICE_MIG_DISABLE or
*                                             \ref NVML_DEVICE_MIG_ENABLE
* @param activationStatus                     The activationStatus status
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device,\a mode or \a activationStatus are invalid
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't support MIG mode
*/
int handle_nvmlDeviceSetMigMode(void *conn) {
    nvmlDevice_t device;
    unsigned int mode;
    nvmlReturn_t activationStatus;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &activationStatus, sizeof(nvmlReturn_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetMigMode(device, mode, &activationStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &activationStatus, sizeof(nvmlReturn_t)) < 0)
        return -1;

    return result;
}

/**
* Get MIG mode for the device.
*
* For Ampere &tm; or newer fully supported devices.
*
* Changing MIG modes may require device unbind or reset. The "pending" MIG mode refers to the target mode following the
* next activation trigger.
*
* @param device                               The identifier of the target device
* @param currentMode                          Returns the current mode, \ref NVML_DEVICE_MIG_DISABLE or
*                                             \ref NVML_DEVICE_MIG_ENABLE
* @param pendingMode                          Returns the pending mode, \ref NVML_DEVICE_MIG_DISABLE or
*                                             \ref NVML_DEVICE_MIG_ENABLE
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a currentMode or \a pendingMode are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't support MIG mode
*/
int handle_nvmlDeviceGetMigMode(void *conn) {
    nvmlDevice_t device;
    unsigned int currentMode;
    unsigned int pendingMode;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &currentMode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &pendingMode, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMigMode(device, &currentMode, &pendingMode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &currentMode, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &pendingMode, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instance profile information.
*
* Information provided by this API is immutable throughout the lifetime of a MIG mode.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               The identifier of the target device
* @param profile                              One of the NVML_GPU_INSTANCE_PROFILE_*
* @param info                                 Returns detailed profile information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a profile or \a info are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or \a profile isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlDeviceGetGpuInstanceProfileInfo(void *conn) {
    nvmlDevice_t device;
    unsigned int profile;
    nvmlGpuInstanceProfileInfo_t info;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlGpuInstanceProfileInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuInstanceProfileInfo(device, profile, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlGpuInstanceProfileInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Versioned wrapper around \ref nvmlDeviceGetGpuInstanceProfileInfo that accepts a versioned
* \ref nvmlGpuInstanceProfileInfo_v2_t or later output structure.
*
* @note The caller must set the \ref nvmlGpuInstanceProfileInfo_v2_t.version field to the
* appropriate version prior to calling this function. For example:
* \code
*     nvmlGpuInstanceProfileInfo_v2_t profileInfo =
*         { .version = nvmlGpuInstanceProfileInfo_v2 };
*     nvmlReturn_t result = nvmlDeviceGetGpuInstanceProfileInfoV(device,
*                                                                profile,
*                                                                &profileInfo);
* \endcode
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               The identifier of the target device
* @param profile                              One of the NVML_GPU_INSTANCE_PROFILE_*
* @param info                                 Returns detailed profile information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a profile, \a info, or \a info->version are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or \a profile isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlDeviceGetGpuInstanceProfileInfoV(void *conn) {
    nvmlDevice_t device;
    unsigned int profile;
    nvmlGpuInstanceProfileInfo_v2_t info;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlGpuInstanceProfileInfo_v2_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuInstanceProfileInfoV(device, profile, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlGpuInstanceProfileInfo_v2_t)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instance placements.
*
* A placement represents the location of a GPU instance within a device. This API only returns all the possible
* placements for the given profile.
* A created GPU instance occupies memory slices described by its placement. Creation of new GPU instance will
* fail if there is overlap with the already occupied memory slices.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param device                               The identifier of the target device
* @param profileId                            The GPU instance profile ID. See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param placements                           Returns placements allowed for the profile. Can be NULL to discover number
*                                             of allowed placements for this profile. If non-NULL must be large enough
*                                             to accommodate the placements supported by the profile.
* @param count                                Returns number of allowed placemenets for the profile.
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a profileId or \a count are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlDeviceGetGpuInstancePossiblePlacements_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int profileId;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlGpuInstancePlacement_t* placements = (nvmlGpuInstancePlacement_t*)malloc(count * sizeof(nvmlGpuInstancePlacement_t));

    nvmlReturn_t result = nvmlDeviceGetGpuInstancePossiblePlacements_v2(device, profileId, placements, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, placements, count * sizeof(nvmlGpuInstancePlacement_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instance profile capacity.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param device                               The identifier of the target device
* @param profileId                            The GPU instance profile ID. See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param count                                Returns remaining instance count for the profile ID
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a profileId or \a count are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlDeviceGetGpuInstanceRemainingCapacity(void *conn) {
    nvmlDevice_t device;
    unsigned int profileId;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuInstanceRemainingCapacity(device, profileId, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Create GPU instance.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* If the parent device is unbound, reset or the GPU instance is destroyed explicitly, the GPU instance handle would
* become invalid. The GPU instance must be recreated to acquire a valid handle.
*
* @param device                               The identifier of the target device
* @param profileId                            The GPU instance profile ID. See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param gpuInstance                          Returns the GPU instance handle
*
* @return
*         - \ref NVML_SUCCESS                       Upon success
*         - \ref NVML_ERROR_UNINITIALIZED           If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT        If \a device, \a profile, \a profileId or \a gpuInstance are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED           If \a device doesn't have MIG mode enabled or in vGPU guest
*         - \ref NVML_ERROR_NO_PERMISSION           If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_INSUFFICIENT_RESOURCES  If the requested GPU instance could not be created
*/
int handle_nvmlDeviceCreateGpuInstance(void *conn) {
    nvmlDevice_t device;
    unsigned int profileId;
    nvmlGpuInstance_t gpuInstance;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceCreateGpuInstance(device, profileId, &gpuInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Create GPU instance with the specified placement.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* If the parent device is unbound, reset or the GPU instance is destroyed explicitly, the GPU instance handle would
* become invalid. The GPU instance must be recreated to acquire a valid handle.
*
* @param device                               The identifier of the target device
* @param profileId                            The GPU instance profile ID. See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param placement                            The requested placement. See \ref nvmlDeviceGetGpuInstancePossiblePlacements_v2
* @param gpuInstance                          Returns the GPU instance handle
*
* @return
*         - \ref NVML_SUCCESS                       Upon success
*         - \ref NVML_ERROR_UNINITIALIZED           If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT        If \a device, \a profile, \a profileId, \a placement or \a gpuInstance
*                                                   are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED           If \a device doesn't have MIG mode enabled or in vGPU guest
*         - \ref NVML_ERROR_NO_PERMISSION           If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_INSUFFICIENT_RESOURCES  If the requested GPU instance could not be created
*/
int handle_nvmlDeviceCreateGpuInstanceWithPlacement(void *conn) {
    nvmlDevice_t device;
    unsigned int profileId;
    nvmlGpuInstancePlacement_t placement;
    nvmlGpuInstance_t gpuInstance;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &placement, sizeof(nvmlGpuInstancePlacement_t)) < 0 ||
        rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceCreateGpuInstanceWithPlacement(device, profileId, &placement, &gpuInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Destroy GPU instance.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param gpuInstance                          The GPU instance handle
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or in vGPU guest
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_IN_USE            If the GPU instance is in use. This error would be returned if processes
*                                             (e.g. CUDA application) or compute instances are active on the
*                                             GPU instance.
*/
int handle_nvmlGpuInstanceDestroy(void *conn) {
    nvmlGpuInstance_t gpuInstance;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceDestroy(gpuInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Get GPU instances for given profile ID.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param device                               The identifier of the target device
* @param profileId                            The GPU instance profile ID. See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param gpuInstances                         Returns pre-exiting GPU instances, the buffer must be large enough to
*                                             accommodate the instances supported by the profile.
*                                             See \ref nvmlDeviceGetGpuInstanceProfileInfo
* @param count                                The count of returned GPU instances
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a profileId, \a gpuInstances or \a count are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlDeviceGetGpuInstances(void *conn) {
    nvmlDevice_t device;
    unsigned int profileId;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlGpuInstance_t* gpuInstances = (nvmlGpuInstance_t*)malloc(count * sizeof(nvmlGpuInstance_t));

    nvmlReturn_t result = nvmlDeviceGetGpuInstances(device, profileId, gpuInstances, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, gpuInstances, count * sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instances for given instance ID.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param device                               The identifier of the target device
* @param id                                   The GPU instance ID
* @param gpuInstance                          Returns GPU instance
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a id or \a gpuInstance are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_NOT_FOUND         If the GPU instance is not found.
*/
int handle_nvmlDeviceGetGpuInstanceById(void *conn) {
    nvmlDevice_t device;
    unsigned int id;
    nvmlGpuInstance_t gpuInstance;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &id, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuInstanceById(device, id, &gpuInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instance information.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param gpuInstance                          The GPU instance handle
* @param info                                 Return GPU instance information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance or \a info are invalid
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetInfo(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    nvmlGpuInstanceInfo_t info;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlGpuInstanceInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetInfo(gpuInstance, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlGpuInstanceInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance profile information.
*
* Information provided by this API is immutable throughout the lifetime of a MIG mode.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profile                              One of the NVML_COMPUTE_INSTANCE_PROFILE_*
* @param engProfile                           One of the NVML_COMPUTE_INSTANCE_ENGINE_PROFILE_*
* @param info                                 Returns detailed profile information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance, \a profile, \a engProfile or \a info are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a profile isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetComputeInstanceProfileInfo(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profile;
    unsigned int engProfile;
    nvmlComputeInstanceProfileInfo_t info;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &engProfile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlComputeInstanceProfileInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceProfileInfo(gpuInstance, profile, engProfile, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlComputeInstanceProfileInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Versioned wrapper around \ref nvmlGpuInstanceGetComputeInstanceProfileInfo that accepts a versioned
* \ref nvmlComputeInstanceProfileInfo_v2_t or later output structure.
*
* @note The caller must set the \ref nvmlGpuInstanceProfileInfo_v2_t.version field to the
* appropriate version prior to calling this function. For example:
* \code
*     nvmlComputeInstanceProfileInfo_v2_t profileInfo =
*         { .version = nvmlComputeInstanceProfileInfo_v2 };
*     nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceProfileInfoV(gpuInstance,
*                                                                         profile,
*                                                                         engProfile,
*                                                                         &profileInfo);
* \endcode
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profile                              One of the NVML_COMPUTE_INSTANCE_PROFILE_*
* @param engProfile                           One of the NVML_COMPUTE_INSTANCE_ENGINE_PROFILE_*
* @param info                                 Returns detailed profile information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance, \a profile, \a engProfile, \a info, or \a info->version are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a profile isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetComputeInstanceProfileInfoV(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profile;
    unsigned int engProfile;
    nvmlComputeInstanceProfileInfo_v2_t info;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &engProfile, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlComputeInstanceProfileInfo_v2_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceProfileInfoV(gpuInstance, profile, engProfile, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlComputeInstanceProfileInfo_v2_t)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance profile capacity.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profileId                            The compute instance profile ID.
*                                             See \ref nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param count                                Returns remaining instance count for the profile ID
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance, \a profileId or \a availableCount are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetComputeInstanceRemainingCapacity(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profileId;
    unsigned int count;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceRemainingCapacity(gpuInstance, profileId, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance placements.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* A placement represents the location of a compute instance within a GPU instance. This API only returns all the possible
* placements for the given profile.
* A created compute instance occupies compute slices described by its placement. Creation of new compute instance will
* fail if there is overlap with the already occupied compute slices.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profileId                            The compute instance profile ID. See \ref  nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param placements                           Returns placements allowed for the profile. Can be NULL to discover number
*                                             of allowed placements for this profile. If non-NULL must be large enough
*                                             to accommodate the placements supported by the profile.
* @param count                                Returns number of allowed placemenets for the profile.
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance, \a profileId or \a count are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled or \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetComputeInstancePossiblePlacements(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profileId;
    nvmlComputeInstancePlacement_t placements;
    unsigned int count;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &placements, sizeof(nvmlComputeInstancePlacement_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstancePossiblePlacements(gpuInstance, profileId, &placements, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &placements, sizeof(nvmlComputeInstancePlacement_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Create compute instance.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* If the parent device is unbound, reset or the parent GPU instance is destroyed or the compute instance is destroyed
* explicitly, the compute instance handle would become invalid. The compute instance must be recreated to acquire
* a valid handle.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profileId                            The compute instance profile ID.
*                                             See \ref nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param computeInstance                      Returns the compute instance handle
*
* @return
*         - \ref NVML_SUCCESS                       Upon success
*         - \ref NVML_ERROR_UNINITIALIZED           If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT        If \a gpuInstance, \a profile, \a profileId or \a computeInstance
*                                                   are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED           If \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION           If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_INSUFFICIENT_RESOURCES  If the requested compute instance could not be created
*/
int handle_nvmlGpuInstanceCreateComputeInstance(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profileId;
    nvmlComputeInstance_t computeInstance;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceCreateComputeInstance(gpuInstance, profileId, &computeInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Create compute instance with the specified placement.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* If the parent device is unbound, reset or the parent GPU instance is destroyed or the compute instance is destroyed
* explicitly, the compute instance handle would become invalid. The compute instance must be recreated to acquire
* a valid handle.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profileId                            The compute instance profile ID.
*                                             See \ref nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param placement                            The requested placement. See \ref nvmlGpuInstanceGetComputeInstancePossiblePlacements
* @param computeInstance                      Returns the compute instance handle
*
* @return
*         - \ref NVML_SUCCESS                       Upon success
*         - \ref NVML_ERROR_UNINITIALIZED           If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT        If \a gpuInstance, \a profile, \a profileId or \a computeInstance
*                                                   are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED           If \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION           If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_INSUFFICIENT_RESOURCES  If the requested compute instance could not be created
*/
int handle_nvmlGpuInstanceCreateComputeInstanceWithPlacement(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profileId;
    nvmlComputeInstancePlacement_t placement;
    nvmlComputeInstance_t computeInstance;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &placement, sizeof(nvmlComputeInstancePlacement_t)) < 0 ||
        rpc_read(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceCreateComputeInstanceWithPlacement(gpuInstance, profileId, &placement, &computeInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Destroy compute instance.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param computeInstance                      The compute instance handle
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a computeInstance is invalid
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_IN_USE            If the compute instance is in use. This error would be returned if
*                                             processes (e.g. CUDA application) are active on the compute instance.
*/
int handle_nvmlComputeInstanceDestroy(void *conn) {
    nvmlComputeInstance_t computeInstance;

    if (rpc_read(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlComputeInstanceDestroy(computeInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Get compute instances for given profile ID.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param profileId                            The compute instance profile ID.
*                                             See \ref nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param computeInstances                     Returns pre-exiting compute instances, the buffer must be large enough to
*                                             accommodate the instances supported by the profile.
*                                             See \ref nvmlGpuInstanceGetComputeInstanceProfileInfo
* @param count                                The count of returned compute instances
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a gpuInstance, \a profileId, \a computeInstances or \a count
*                                             are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a profileId isn't supported
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlGpuInstanceGetComputeInstances(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int profileId;
    unsigned int count;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlComputeInstance_t* computeInstances = (nvmlComputeInstance_t*)malloc(count * sizeof(nvmlComputeInstance_t));

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstances(gpuInstance, profileId, computeInstances, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, computeInstances, count * sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance for given instance ID.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
* Requires privileged user.
*
* @param gpuInstance                          The identifier of the target GPU instance
* @param id                                   The compute instance ID
* @param computeInstance                      Returns compute instance
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a device, \a ID or \a computeInstance are invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't have MIG mode enabled
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*         - \ref NVML_ERROR_NOT_FOUND         If the compute instance is not found.
*/
int handle_nvmlGpuInstanceGetComputeInstanceById(void *conn) {
    nvmlGpuInstance_t gpuInstance;
    unsigned int id;
    nvmlComputeInstance_t computeInstance;

    if (rpc_read(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &id, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpuInstanceGetComputeInstanceById(gpuInstance, id, &computeInstance);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance information.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param computeInstance                      The compute instance handle
* @param info                                 Return compute instance information
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_UNINITIALIZED     If library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  If \a computeInstance or \a info are invalid
*         - \ref NVML_ERROR_NO_PERMISSION     If user doesn't have permission to perform the operation
*/
int handle_nvmlComputeInstanceGetInfo_v2(void *conn) {
    nvmlComputeInstance_t computeInstance;
    nvmlComputeInstanceInfo_t info;

    if (rpc_read(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlComputeInstanceInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlComputeInstanceGetInfo_v2(computeInstance, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlComputeInstanceInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Test if the given handle refers to a MIG device.
*
* A MIG device handle is an NVML abstraction which maps to a MIG compute instance.
* These overloaded references can be used (with some restrictions) interchangeably
* with a GPU device handle to execute queries at a per-compute instance granularity.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               NVML handle to test
* @param isMigDevice                          True when handle refers to a MIG device
*
* @return
*         - \ref NVML_SUCCESS                 if \a device status was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device handle or \a isMigDevice reference is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this check is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceIsMigDeviceHandle(void *conn) {
    nvmlDevice_t device;
    unsigned int isMigDevice;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &isMigDevice, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceIsMigDeviceHandle(device, &isMigDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isMigDevice, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get GPU instance ID for the given MIG device handle.
*
* GPU instance IDs are unique per device and remain valid until the GPU instance is destroyed.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               Target MIG device handle
* @param id                                   GPU instance ID
*
* @return
*         - \ref NVML_SUCCESS                 if instance ID was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a id reference is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGpuInstanceId(void *conn) {
    nvmlDevice_t device;
    unsigned int id;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &id, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuInstanceId(device, &id);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &id, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get compute instance ID for the given MIG device handle.
*
* Compute instance IDs are unique per GPU instance and remain valid until the compute instance
* is destroyed.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               Target MIG device handle
* @param id                                   Compute instance ID
*
* @return
*         - \ref NVML_SUCCESS                 if instance ID was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a id reference is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetComputeInstanceId(void *conn) {
    nvmlDevice_t device;
    unsigned int id;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &id, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetComputeInstanceId(device, &id);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &id, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get the maximum number of MIG devices that can exist under a given parent NVML device.
*
* Returns zero if MIG is not supported or enabled.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               Target device handle
* @param count                                Count of MIG devices
*
* @return
*         - \ref NVML_SUCCESS                 if \a count was successfully retrieved
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a count reference is invalid
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMaxMigDeviceCount(void *conn) {
    nvmlDevice_t device;
    unsigned int count;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMaxMigDeviceCount(device, &count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get MIG device handle for the given index under its parent NVML device.
*
* If the compute instance is destroyed either explicitly or by destroying,
* resetting or unbinding the parent GPU instance or the GPU device itself
* the MIG device handle would remain invalid and must be requested again
* using this API. Handles may be reused and their properties can change in
* the process.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param device                               Reference to the parent GPU device handle
* @param index                                Index of the MIG device
* @param migDevice                            Reference to the MIG device handle
*
* @return
*         - \ref NVML_SUCCESS                 if \a migDevice handle was successfully created
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a index or \a migDevice reference is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_NOT_FOUND         if no valid MIG device was found at \a index
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMigDeviceHandleByIndex(void *conn) {
    nvmlDevice_t device;
    unsigned int index;
    nvmlDevice_t migDevice;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &migDevice, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMigDeviceHandleByIndex(device, index, &migDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &migDevice, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Get parent device handle from a MIG device handle.
*
* For Ampere &tm; or newer fully supported devices.
* Supported on Linux only.
*
* @param migDevice                            MIG device handle
* @param device                               Device handle
*
* @return
*         - \ref NVML_SUCCESS                 if \a device handle was successfully created
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a migDevice or \a device is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetDeviceHandleFromMigDeviceHandle(void *conn) {
    nvmlDevice_t migDevice;
    nvmlDevice_t device;

    if (rpc_read(conn, &migDevice, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDeviceHandleFromMigDeviceHandle(migDevice, &device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0)
        return -1;

    return result;
}

/**
* Get the type of the GPU Bus (PCIe, PCI, ...)
*
* @param device                               The identifier of the target device
* @param type                                 The PCI Bus type
*
* return
*         - \ref NVML_SUCCESS                 if the bus \a type is successfully retreived
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \device is invalid or \type is NULL
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetBusType(void *conn) {
    nvmlDevice_t device;
    nvmlBusType_t type;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlBusType_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetBusType(device, &type);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &type, sizeof(nvmlBusType_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve performance monitor samples from the associated subdevice.
*
* @param device
* @param pDynamicPstatesInfo
*
* @return
*         - \ref NVML_SUCCESS                 if \a pDynamicPstatesInfo has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a pDynamicPstatesInfo is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetDynamicPstatesInfo(void *conn) {
    nvmlDevice_t device;
    nvmlGpuDynamicPstatesInfo_t pDynamicPstatesInfo;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pDynamicPstatesInfo, sizeof(nvmlGpuDynamicPstatesInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetDynamicPstatesInfo(device, &pDynamicPstatesInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pDynamicPstatesInfo, sizeof(nvmlGpuDynamicPstatesInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Sets the speed of a specified fan.
*
* WARNING: This function changes the fan control policy to manual. It means that YOU have to monitor
*          the temperature and adjust the fan speed accordingly.
*          If you set the fan speed too low you can burn your GPU!
*          Use nvmlDeviceSetDefaultFanSpeed_v2 to restore default control policy.
*
* For all cuda-capable discrete products with fans that are Maxwell or Newer.
*
* device                                The identifier of the target device
* fan                                   The index of the fan, starting at zero
* speed                                 The target speed of the fan [0-100] in % of max speed
*
* return
*        NVML_SUCCESS                   if the fan speed has been set
*        NVML_ERROR_UNINITIALIZED       if the library has not been successfully initialized
*        NVML_ERROR_INVALID_ARGUMENT    if the device is not valid, or the speed is outside acceptable ranges,
*                                              or if the fan index doesn't reference an actual fan.
*        NVML_ERROR_NOT_SUPPORTED       if the device is older than Maxwell.
*        NVML_ERROR_UNKNOWN             if there was an unexpected error.
*/
int handle_nvmlDeviceSetFanSpeed_v2(void *conn) {
    nvmlDevice_t device;
    unsigned int fan;
    unsigned int speed;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &speed, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetFanSpeed_v2(device, fan, speed);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieve the GPCCLK VF offset value
* @param[in]   device                         The identifier of the target device
* @param[out]  offset                         The retrieved GPCCLK VF offset value
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been successfully queried
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGpcClkVfOffset(void *conn) {
    nvmlDevice_t device;
    int offset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpcClkVfOffset(device, &offset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &offset, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Set the GPCCLK VF offset value
* @param[in]   device                         The identifier of the target device
* @param[in]   offset                         The GPCCLK VF offset value to set
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetGpcClkVfOffset(void *conn) {
    nvmlDevice_t device;
    int offset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetGpcClkVfOffset(device, offset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieve the MemClk (Memory Clock) VF offset value.
* @param[in]   device                         The identifier of the target device
* @param[out]  offset                         The retrieved MemClk VF offset value
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been successfully queried
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMemClkVfOffset(void *conn) {
    nvmlDevice_t device;
    int offset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemClkVfOffset(device, &offset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &offset, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Set the MemClk (Memory Clock) VF offset value. It requires elevated privileges.
* @param[in]   device                         The identifier of the target device
* @param[in]   offset                         The MemClk VF offset value to set
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been set
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_GPU_IS_LOST       if the target GPU has fallen off the bus or is otherwise inaccessible
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceSetMemClkVfOffset(void *conn) {
    nvmlDevice_t device;
    int offset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetMemClkVfOffset(device, offset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Retrieve min and max clocks of some clock domain for a given PState
*
* @param device                               The identifier of the target device
* @param type                                 Clock domain
* @param pstate                               PState to query
* @param minClockMHz                          Reference in which to return min clock frequency
* @param maxClockMHz                          Reference in which to return max clock frequency
*
* @return
*         - \ref NVML_SUCCESS                 if everything worked
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device, \a type or \a pstate are invalid or both
*                                                  \a minClockMHz and \a maxClockMHz are NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*/
int handle_nvmlDeviceGetMinMaxClockOfPState(void *conn) {
    nvmlDevice_t device;
    nvmlClockType_t type;
    nvmlPstates_t pstate;
    unsigned int minClockMHz;
    unsigned int maxClockMHz;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_read(conn, &pstate, sizeof(nvmlPstates_t)) < 0 ||
        rpc_read(conn, &minClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &maxClockMHz, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMinMaxClockOfPState(device, type, pstate, &minClockMHz, &maxClockMHz);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &maxClockMHz, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* Get all supported Performance States (P-States) for the device.
*
* The returned array would contain a contiguous list of valid P-States supported by
* the device. If the number of supported P-States is fewer than the size of the array
* supplied missing elements would contain \a NVML_PSTATE_UNKNOWN.
*
* The number of elements in the returned list will never exceed \a NVML_MAX_GPU_PERF_PSTATES.
*
* @param device                               The identifier of the target device
* @param pstates                              Container to return the list of performance states
*                                             supported by device
* @param size                                 Size of the supplied \a pstates array in bytes
*
* @return
*         - \ref NVML_SUCCESS                 if \a pstates array has been retrieved
*         - \ref NVML_ERROR_INSUFFICIENT_SIZE if the the container supplied was not large enough to
*                                             hold the resulting list
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device or \a pstates is invalid
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support performance state readings
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetSupportedPerformanceStates(void *conn) {
    nvmlDevice_t device;
    nvmlPstates_t pstates;
    unsigned int size;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &pstates, sizeof(nvmlPstates_t)) < 0 ||
        rpc_read(conn, &size, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetSupportedPerformanceStates(device, &pstates, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pstates, sizeof(nvmlPstates_t)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the GPCCLK min max VF offset value.
* @param[in]   device                         The identifier of the target device
* @param[out]  minOffset                      The retrieved GPCCLK VF min offset value
* @param[out]  maxOffset                      The retrieved GPCCLK VF max offset value
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been successfully queried
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetGpcClkMinMaxVfOffset(void *conn) {
    nvmlDevice_t device;
    int minOffset;
    int maxOffset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minOffset, sizeof(int)) < 0 ||
        rpc_read(conn, &maxOffset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpcClkMinMaxVfOffset(device, &minOffset, &maxOffset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minOffset, sizeof(int)) < 0 ||
        rpc_write(conn, &maxOffset, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Retrieve the MemClk (Memory Clock) min max VF offset value.
* @param[in]   device                         The identifier of the target device
* @param[out]  minOffset                      The retrieved MemClk VF min offset value
* @param[out]  maxOffset                      The retrieved MemClk VF max offset value
*
* @return
*         - \ref NVML_SUCCESS                 if \a offset has been successfully queried
*         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a offset is NULL
*         - \ref NVML_ERROR_NOT_SUPPORTED     if the device does not support this feature
*         - \ref NVML_ERROR_UNKNOWN           on any unexpected error
*/
int handle_nvmlDeviceGetMemClkMinMaxVfOffset(void *conn) {
    nvmlDevice_t device;
    int minOffset;
    int maxOffset;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &minOffset, sizeof(int)) < 0 ||
        rpc_read(conn, &maxOffset, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetMemClkMinMaxVfOffset(device, &minOffset, &maxOffset);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &minOffset, sizeof(int)) < 0 ||
        rpc_write(conn, &maxOffset, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* Get fabric information associated with the device.
*
* %HOPPER_OR_NEWER%
*
* On Hopper + NVSwitch systems, GPU is registered with the NVIDIA Fabric Manager
* Upon successful registration, the GPU is added to the NVLink fabric to enable
* peer-to-peer communication.
* This API reports the current state of the GPU in the NVLink fabric
* along with other useful information.
*
* @param device                               The identifier of the target device
* @param gpuFabricInfo                        Information about GPU fabric state
*
* @return
*         - \ref NVML_SUCCESS                 Upon success
*         - \ref NVML_ERROR_NOT_SUPPORTED     If \a device doesn't support gpu fabric
*/
int handle_nvmlDeviceGetGpuFabricInfo(void *conn) {
    nvmlDevice_t device;
    nvmlGpuFabricInfo_t gpuFabricInfo;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &gpuFabricInfo, sizeof(nvmlGpuFabricInfo_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceGetGpuFabricInfo(device, &gpuFabricInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpuFabricInfo, sizeof(nvmlGpuFabricInfo_t)) < 0)
        return -1;

    return result;
}

/**
* Calculate GPM metrics from two samples.
*
* For Hopper &tm; or newer fully supported devices.
*
* @param metricsGet             IN/OUT: populated \a nvmlGpmMetricsGet_t struct
*
* @return
*         - \ref NVML_SUCCESS on success
*         - Nonzero NVML_ERROR_? enum on error
*/
int handle_nvmlGpmMetricsGet(void *conn) {
    nvmlGpmMetricsGet_t metricsGet;

    if (rpc_read(conn, &metricsGet, sizeof(nvmlGpmMetricsGet_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmMetricsGet(&metricsGet);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &metricsGet, sizeof(nvmlGpmMetricsGet_t)) < 0)
        return -1;

    return result;
}

/**
* Free an allocated sample buffer that was allocated with \ref nvmlGpmSampleAlloc()
*
* For Hopper &tm; or newer fully supported devices.
*
* @param gpmSample              Sample to free
*
* @return
*         - \ref NVML_SUCCESS                on success
*         - \ref NVML_ERROR_INVALID_ARGUMENT if an invalid pointer is provided
*/
int handle_nvmlGpmSampleFree(void *conn) {
    nvmlGpmSample_t gpmSample;

    if (rpc_read(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmSampleFree(gpmSample);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Allocate a sample buffer to be used with NVML GPM . You will need to allocate
* at least two of these buffers to use with the NVML GPM feature
*
* For Hopper &tm; or newer fully supported devices.
*
* @param gpmSample             Where  the allocated sample will be stored
*
* @return
*         - \ref NVML_SUCCESS                on success
*         - \ref NVML_ERROR_INVALID_ARGUMENT if an invalid pointer is provided
*         - \ref NVML_ERROR_MEMORY           if system memory is insufficient
*/
int handle_nvmlGpmSampleAlloc(void *conn) {
    nvmlGpmSample_t gpmSample;

    if (rpc_read(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmSampleAlloc(&gpmSample);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0)
        return -1;

    return result;
}

/**
* Read a sample of GPM metrics into the provided \a gpmSample buffer. After
* two samples are gathered, you can call nvmlGpmMetricGet on those samples to
* retrive metrics
*
* For Hopper &tm; or newer fully supported devices.
*
* @param device                Device to get samples for
* @param gpmSample             Buffer to read samples into
*
* @return
*         - \ref NVML_SUCCESS on success
*         - Nonzero NVML_ERROR_? enum on error
*/
int handle_nvmlGpmSampleGet(void *conn) {
    nvmlDevice_t device;
    nvmlGpmSample_t gpmSample;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmSampleGet(device, gpmSample);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Read a sample of GPM metrics into the provided \a gpmSample buffer for a MIG GPU Instance.
*
* After two samples are gathered, you can call nvmlGpmMetricGet on those
* samples to retrive metrics
*
* For Hopper &tm; or newer fully supported devices.
*
* @param device                Device to get samples for
* @param gpuInstanceId         MIG GPU Instance ID
* @param gpmSample             Buffer to read samples into
*
* @return
*         - \ref NVML_SUCCESS on success
*         - Nonzero NVML_ERROR_? enum on error
*/
int handle_nvmlGpmMigSampleGet(void *conn) {
    nvmlDevice_t device;
    unsigned int gpuInstanceId;
    nvmlGpmSample_t gpmSample;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &gpuInstanceId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmMigSampleGet(device, gpuInstanceId, gpmSample);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* Indicate whether the supplied device supports GPM
*
* @param device                NVML device to query for
* @param gpmSupport            Structure to indicate GPM support \a nvmlGpmSupport_t. Indicates
*                              GPM support per system for the supplied device
*
* @return
*         - NVML_SUCCESS on success
*         - Nonzero NVML_ERROR_? enum if there is an error in processing the query
*/
int handle_nvmlGpmQueryDeviceSupport(void *conn) {
    nvmlDevice_t device;
    nvmlGpmSupport_t gpmSupport;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &gpmSupport, sizeof(nvmlGpmSupport_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlGpmQueryDeviceSupport(device, &gpmSupport);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &gpmSupport, sizeof(nvmlGpmSupport_t)) < 0)
        return -1;

    return result;
}

/**
* Set NvLink Low Power Threshold for device.
*
* %HOPPER_OR_NEWER%
*
* @param device                               The identifier of the target device
* @param info                                 Reference to \a nvmlNvLinkPowerThres_t struct
*                                             input parameters
*
* @return
*        - \ref NVML_SUCCESS                 if the \a Threshold is successfully set
*        - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
*        - \ref NVML_ERROR_INVALID_ARGUMENT  if \a device is invalid or \a Threshold is not within range
*        - \ref NVML_ERROR_NOT_SUPPORTED     if this query is not supported by the device
*
**/
int handle_nvmlDeviceSetNvLinkDeviceLowPowerThreshold(void *conn) {
    nvmlDevice_t device;
    nvmlNvLinkPowerThres_t info;

    if (rpc_read(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &info, sizeof(nvmlNvLinkPowerThres_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    nvmlReturn_t result = nvmlDeviceSetNvLinkDeviceLowPowerThreshold(device, &info);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &info, sizeof(nvmlNvLinkPowerThres_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the string description of an error code
*
* Sets \p *pStr to the address of a NULL-terminated string description
* of the error code \p error.
* If the error code is not recognized, ::CUDA_ERROR_INVALID_VALUE
* will be returned and \p *pStr will be set to the NULL address.
*
* \param error - Error code to convert to string
* \param pStr - Address of the string pointer.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::CUresult,
* ::cudaGetErrorString
*/
int handle_cuGetErrorString(void *conn) {
    CUresult error;
    const char* pStr;

    if (rpc_read(conn, &error, sizeof(CUresult)) < 0 ||
        rpc_read(conn, &pStr, sizeof(const char*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGetErrorString(error, &pStr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pStr, sizeof(const char*)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the string representation of an error code enum name
*
* Sets \p *pStr to the address of a NULL-terminated string representation
* of the name of the enum error code \p error.
* If the error code is not recognized, ::CUDA_ERROR_INVALID_VALUE
* will be returned and \p *pStr will be set to the NULL address.
*
* \param error - Error code to convert to string
* \param pStr - Address of the string pointer.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::CUresult,
* ::cudaGetErrorName
*/
int handle_cuGetErrorName(void *conn) {
    CUresult error;
    const char* pStr;

    if (rpc_read(conn, &error, sizeof(CUresult)) < 0 ||
        rpc_read(conn, &pStr, sizeof(const char*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGetErrorName(error, &pStr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pStr, sizeof(const char*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initialize the CUDA driver API
* Initializes the driver API and must be called before any other function from
* the driver API in the current process. Currently, the \p Flags parameter must be 0. If ::cuInit()
* has not been called, any function from the driver API will return
* ::CUDA_ERROR_NOT_INITIALIZED.
*
* \param Flags - Initialization flag for CUDA.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_SYSTEM_DRIVER_MISMATCH,
* ::CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE
* \notefnerr
*/
int handle_cuInit(void *conn) {
    unsigned int Flags;

    if (rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuInit(Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the latest CUDA version supported by driver
*
* Returns in \p *driverVersion the version of CUDA supported by
* the driver.  The version is returned as
* (1000 &times; major + 10 &times; minor). For example, CUDA 9.2
* would be represented by 9020.
*
* This function automatically returns ::CUDA_ERROR_INVALID_VALUE if
* \p driverVersion is NULL.
*
* \param driverVersion - Returns the CUDA driver version
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cudaDriverGetVersion,
* ::cudaRuntimeGetVersion
*/
int handle_cuDriverGetVersion(void *conn) {
    int driverVersion;

    if (rpc_read(conn, &driverVersion, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDriverGetVersion(&driverVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &driverVersion, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a handle to a compute device
*
* Returns in \p *device a device handle given an ordinal in the range <b>[0,
* ::cuDeviceGetCount()-1]</b>.
*
* \param device  - Returned device handle
* \param ordinal - Device number to get handle for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGetLuid,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport
*/
int handle_cuDeviceGet(void *conn) {
    CUdevice device;
    int ordinal;

    if (rpc_read(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &ordinal, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGet(&device, ordinal);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the number of compute-capable devices
*
* Returns in \p *count the number of devices with compute capability greater
* than or equal to 2.0 that are available for execution. If there is no such
* device, ::cuDeviceGetCount() returns 0.
*
* \param count - Returned number of compute-capable devices
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGetLuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaGetDeviceCount
*/
int handle_cuDeviceGetCount(void *conn) {
    int count;

    if (rpc_read(conn, &count, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetCount(&count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns an identifier string for the device
*
* Returns an ASCII string identifying the device \p dev in the NULL-terminated
* string pointed to by \p name. \p len specifies the maximum length of the
* string that may be returned.
*
* \param name - Returned identifier string for the device
* \param len  - Maximum length of string to store in \p name
* \param dev  - Device to get identifier string for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetUuid,
* ::cuDeviceGetLuid,
* ::cuDeviceGetCount,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaGetDeviceProperties
*/
int handle_cuDeviceGetName(void *conn) {
    char name;
    int len;
    CUdevice dev;

    if (rpc_read(conn, &name, sizeof(char)) < 0 ||
        rpc_read(conn, &len, sizeof(int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetName(&name, len, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &name, sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* \brief Return an UUID for the device
*
* Note there is a later version of this API, ::cuDeviceGetUuid_v2. It will
* supplant this version in 12.0, which is retained for minor version compatibility.
*
* Returns 16-octets identifing the device \p dev in the structure
* pointed by the \p uuid.
*
* \param uuid - Returned UUID
* \param dev  - Device to get identifier string for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetUuid_v2
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetLuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaGetDeviceProperties
*/
int handle_cuDeviceGetUuid(void *conn) {
    CUuuid uuid;
    CUdevice dev;

    if (rpc_read(conn, &uuid, sizeof(CUuuid)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetUuid(&uuid, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &uuid, sizeof(CUuuid)) < 0)
        return -1;

    return result;
}

/**
* \brief Return an UUID for the device (11.4+)
*
* Returns 16-octets identifing the device \p dev in the structure
* pointed by the \p uuid. If the device is in MIG mode, returns its
* MIG UUID which uniquely identifies the subscribed MIG compute instance.
*
* \param uuid - Returned UUID
* \param dev  - Device to get identifier string for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetLuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cudaGetDeviceProperties
*/
int handle_cuDeviceGetUuid_v2(void *conn) {
    CUuuid uuid;
    CUdevice dev;

    if (rpc_read(conn, &uuid, sizeof(CUuuid)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetUuid_v2(&uuid, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &uuid, sizeof(CUuuid)) < 0)
        return -1;

    return result;
}

/**
* \brief Return an LUID and device node mask for the device
*
* Return identifying information (\p luid and \p deviceNodeMask) to allow
* matching device with graphics APIs.
*
* \param luid - Returned LUID
* \param deviceNodeMask - Returned device node mask
* \param dev  - Device to get identifier string for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaGetDeviceProperties
*/
int handle_cuDeviceGetLuid(void *conn) {
    char luid;
    unsigned int deviceNodeMask;
    CUdevice dev;

    if (rpc_read(conn, &luid, sizeof(char)) < 0 ||
        rpc_read(conn, &deviceNodeMask, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetLuid(&luid, &deviceNodeMask, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &luid, sizeof(char)) < 0 ||
        rpc_write(conn, &deviceNodeMask, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the total amount of memory on the device
*
* Returns in \p *bytes the total amount of memory available on the device
* \p dev in bytes.
*
* \param bytes - Returned memory available on device in bytes
* \param dev   - Device handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaMemGetInfo
*/
int handle_cuDeviceTotalMem_v2(void *conn) {
    size_t bytes;
    CUdevice dev;

    if (rpc_read(conn, &bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceTotalMem_v2(&bytes, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &bytes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the maximum number of elements allocatable in a 1D linear texture for a given texture element size.
*
* Returns in \p maxWidthInElements the maximum number of texture elements allocatable in a 1D linear texture
* for given \p format and \p numChannels.
*
* \param maxWidthInElements    - Returned maximum number of texture elements allocatable for given \p format and \p numChannels.
* \param format                - Texture format.
* \param numChannels           - Number of channels per texture element.
* \param dev                   - Device handle.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cudaMemGetInfo,
* ::cuDeviceTotalMem
*/
int handle_cuDeviceGetTexture1DLinearMaxWidth(void *conn) {
    size_t maxWidthInElements;
    CUarray_format format;
    unsigned numChannels;
    CUdevice dev;

    if (rpc_read(conn, &maxWidthInElements, sizeof(size_t)) < 0 ||
        rpc_read(conn, &format, sizeof(CUarray_format)) < 0 ||
        rpc_read(conn, &numChannels, sizeof(unsigned)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetTexture1DLinearMaxWidth(&maxWidthInElements, format, numChannels, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxWidthInElements, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about the device
*
* Returns in \p *pi the integer value of the attribute \p attrib on device
* \p dev. The supported attributes are:
* - ::CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK: Maximum number of threads per
*   block;
* - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X: Maximum x-dimension of a block
* - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y: Maximum y-dimension of a block
* - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z: Maximum z-dimension of a block
* - ::CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X: Maximum x-dimension of a grid
* - ::CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y: Maximum y-dimension of a grid
* - ::CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z: Maximum z-dimension of a grid
* - ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK: Maximum amount of
*   shared memory available to a thread block in bytes
* - ::CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY: Memory available on device for
*   __constant__ variables in a CUDA C kernel in bytes
* - ::CU_DEVICE_ATTRIBUTE_WARP_SIZE: Warp size in threads
* - ::CU_DEVICE_ATTRIBUTE_MAX_PITCH: Maximum pitch in bytes allowed by the
*   memory copy functions that involve memory regions allocated through
*   ::cuMemAllocPitch()
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH: Maximum 1D
*  texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH: Maximum width
*  for a 1D texture bound to linear memory
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: Maximum
*  mipmapped 1D texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH: Maximum 2D
*  texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT: Maximum 2D
*  texture height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH: Maximum width
*  for a 2D texture bound to linear memory
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: Maximum height
*  for a 2D texture bound to linear memory
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH: Maximum pitch
*  in bytes for a 2D texture bound to linear memory
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: Maximum
*  mipmapped 2D texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: Maximum
*  mipmapped 2D texture height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH: Maximum 3D
*  texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT: Maximum 3D
*  texture height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH: Maximum 3D
*  texture depth
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE:
*  Alternate maximum 3D texture width, 0 if no alternate
*  maximum 3D texture size is supported
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE:
*  Alternate maximum 3D texture height, 0 if no alternate
*  maximum 3D texture size is supported
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE:
*  Alternate maximum 3D texture depth, 0 if no alternate
*  maximum 3D texture size is supported
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_WIDTH:
*  Maximum cubemap texture width or height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH:
*  Maximum 1D layered texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS:
*   Maximum layers in a 1D layered texture
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH:
*  Maximum 2D layered texture width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT:
*   Maximum 2D layered texture height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS:
*   Maximum layers in a 2D layered texture
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH:
*   Maximum cubemap layered texture width or height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS:
*   Maximum layers in a cubemap layered texture
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_WIDTH:
*   Maximum 1D surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_WIDTH:
*   Maximum 2D surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_HEIGHT:
*   Maximum 2D surface height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_WIDTH:
*   Maximum 3D surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_HEIGHT:
*   Maximum 3D surface height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_DEPTH:
*   Maximum 3D surface depth
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_WIDTH:
*   Maximum 1D layered surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_LAYERS:
*   Maximum layers in a 1D layered surface
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_WIDTH:
*   Maximum 2D layered surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_HEIGHT:
*   Maximum 2D layered surface height
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_LAYERS:
*   Maximum layers in a 2D layered surface
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_WIDTH:
*   Maximum cubemap surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH:
*   Maximum cubemap layered surface width
* - ::CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS:
*   Maximum layers in a cubemap layered surface
* - ::CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK: Maximum number of 32-bit
*   registers available to a thread block
* - ::CU_DEVICE_ATTRIBUTE_CLOCK_RATE: The typical clock frequency in kilohertz
* - ::CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT: Alignment requirement; texture
*   base addresses aligned to ::textureAlign bytes do not need an offset
*   applied to texture fetches
* - ::CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT: Pitch alignment requirement
*   for 2D texture references bound to pitched memory
* - ::CU_DEVICE_ATTRIBUTE_GPU_OVERLAP: 1 if the device can concurrently copy
*   memory between host and device while executing a kernel, or 0 if not
* - ::CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT: Number of multiprocessors on
*   the device
* - ::CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT: 1 if there is a run time limit
*   for kernels executed on the device, or 0 if not
* - ::CU_DEVICE_ATTRIBUTE_INTEGRATED: 1 if the device is integrated with the
*   memory subsystem, or 0 if not
* - ::CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY: 1 if the device can map host
*   memory into the CUDA address space, or 0 if not
* - ::CU_DEVICE_ATTRIBUTE_COMPUTE_MODE: Compute mode that device is currently
*   in. Available modes are as follows:
*   - ::CU_COMPUTEMODE_DEFAULT: Default mode - Device is not restricted and
*     can have multiple CUDA contexts present at a single time.
*   - ::CU_COMPUTEMODE_PROHIBITED: Compute-prohibited mode - Device is
*     prohibited from creating new CUDA contexts.
*   - ::CU_COMPUTEMODE_EXCLUSIVE_PROCESS:  Compute-exclusive-process mode - Device
*     can have only one context used by a single process at a time.
* - ::CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS: 1 if the device supports
*   executing multiple kernels within the same context simultaneously, or 0 if
*   not. It is not guaranteed that multiple kernels will be resident
*   on the device concurrently so this feature should not be relied upon for
*   correctness.
* - ::CU_DEVICE_ATTRIBUTE_ECC_ENABLED: 1 if error correction is enabled on the
*    device, 0 if error correction is disabled or not supported by the device
* - ::CU_DEVICE_ATTRIBUTE_PCI_BUS_ID: PCI bus identifier of the device
* - ::CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID: PCI device (also known as slot) identifier
*   of the device
* - ::CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID: PCI domain identifier of the device
* - ::CU_DEVICE_ATTRIBUTE_TCC_DRIVER: 1 if the device is using a TCC driver. TCC
*    is only available on Tesla hardware running Windows Vista or later
* - ::CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE: Peak memory clock frequency in kilohertz
* - ::CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH: Global memory bus width in bits
* - ::CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE: Size of L2 cache in bytes. 0 if the device doesn't have L2 cache
* - ::CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR: Maximum resident threads per multiprocessor
* - ::CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING: 1 if the device shares a unified address space with
*   the host, or 0 if not
* - ::CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR: Major compute capability version number
* - ::CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR: Minor compute capability version number
* - ::CU_DEVICE_ATTRIBUTE_GLOBAL_L1_CACHE_SUPPORTED: 1 if device supports caching globals
*    in L1 cache, 0 if caching globals in L1 cache is not supported by the device
* - ::CU_DEVICE_ATTRIBUTE_LOCAL_L1_CACHE_SUPPORTED: 1 if device supports caching locals
*    in L1 cache, 0 if caching locals in L1 cache is not supported by the device
* - ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: Maximum amount of
*   shared memory available to a multiprocessor in bytes; this amount is shared
*   by all thread blocks simultaneously resident on a multiprocessor
* - ::CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR: Maximum number of 32-bit
*   registers available to a multiprocessor; this number is shared by all thread
*   blocks simultaneously resident on a multiprocessor
* - ::CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY: 1 if device supports allocating managed memory
*   on this system, 0 if allocating managed memory is not supported by the device on this system.
* - ::CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD: 1 if device is on a multi-GPU board, 0 if not.
* - ::CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD_GROUP_ID: Unique identifier for a group of devices
*   associated with the same board. Devices on the same multi-GPU board will share the same identifier.
* - ::CU_DEVICE_ATTRIBUTE_HOST_NATIVE_ATOMIC_SUPPORTED: 1 if Link between the device and the host
*   supports native atomic operations.
* - ::CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: Ratio of single precision performance
*   (in floating-point operations per second) to double precision performance.
* - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS: Device supports coherently accessing
*   pageable memory without calling cudaHostRegister on it.
* - ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS: Device can coherently access managed memory
*   concurrently with the CPU.
* - ::CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED: Device supports Compute Preemption.
* - ::CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: Device can access host registered
*   memory at the same virtual address as the CPU.
* -  ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: The maximum per block shared memory size
*    supported on this device. This is the maximum value that can be opted into when using the cuFuncSetAttribute() or cuKernelSetAttribute() call.
*    For more details see ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES
* - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: Device accesses pageable memory via the host's
*   page tables.
* - ::CU_DEVICE_ATTRIBUTE_DIRECT_MANAGED_MEM_ACCESS_FROM_HOST: The host can directly access managed memory on the device without migration.
* - ::CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED:  Device supports virtual memory management APIs like ::cuMemAddressReserve, ::cuMemCreate, ::cuMemMap and related APIs
* - ::CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: Device supports exporting memory to a posix file descriptor with ::cuMemExportToShareableHandle, if requested via ::cuMemCreate
* - ::CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED:  Device supports exporting memory to a Win32 NT handle with ::cuMemExportToShareableHandle, if requested via ::cuMemCreate
* - ::CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: Device supports exporting memory to a Win32 KMT handle with ::cuMemExportToShareableHandle, if requested via ::cuMemCreate
* - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCKS_PER_MULTIPROCESSOR: Maximum number of thread blocks that can reside on a multiprocessor
* - ::CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED: Device supports compressible memory allocation via ::cuMemCreate
* - ::CU_DEVICE_ATTRIBUTE_MAX_PERSISTING_L2_CACHE_SIZE: Maximum L2 persisting lines capacity setting in bytes
* - ::CU_DEVICE_ATTRIBUTE_MAX_ACCESS_POLICY_WINDOW_SIZE: Maximum value of CUaccessPolicyWindow::num_bytes 
* - ::CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WITH_CUDA_VMM_SUPPORTED: Device supports specifying the GPUDirect RDMA flag with ::cuMemCreate.
* - ::CU_DEVICE_ATTRIBUTE_RESERVED_SHARED_MEMORY_PER_BLOCK: Amount of shared memory per block reserved by CUDA driver in bytes
* - ::CU_DEVICE_ATTRIBUTE_SPARSE_CUDA_ARRAY_SUPPORTED: Device supports sparse CUDA arrays and sparse CUDA mipmapped arrays. 
* - ::CU_DEVICE_ATTRIBUTE_READ_ONLY_HOST_REGISTER_SUPPORTED: Device supports using the ::cuMemHostRegister flag ::CU_MEMHOSTERGISTER_READ_ONLY to register memory that must be mapped as read-only to the GPU
* - ::CU_DEVICE_ATTRIBUTE_MEMORY_POOLS_SUPPORTED: Device supports using the ::cuMemAllocAsync and ::cuMemPool family of APIs
* - ::CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_SUPPORTED: Device supports GPUDirect RDMA APIs, like nvidia_p2p_get_pages (see https://docs.nvidia.com/cuda/gpudirect-rdma for more information)
* - ::CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_FLUSH_WRITES_OPTIONS: The returned attribute shall be interpreted as a bitmask, where the individual bits are described by the ::CUflushGPUDirectRDMAWritesOptions enum
* - ::CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WRITES_ORDERING: GPUDirect RDMA writes to the device do not need to be flushed for consumers within the scope indicated by the returned attribute. See ::CUGPUDirectRDMAWritesOrdering for the numerical values returned here.
* - ::CU_DEVICE_ATTRIBUTE_MEMPOOL_SUPPORTED_HANDLE_TYPES: Bitmask of handle types supported with mempool based IPC
* - ::CU_DEVICE_ATTRIBUTE_DEFERRED_MAPPING_CUDA_ARRAY_SUPPORTED: Device supports deferred mapping CUDA arrays and CUDA mipmapped arrays.
*
* \param pi     - Returned device attribute value
* \param attrib - Device attribute to query
* \param dev    - Device handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem,
* ::cuDeviceGetExecAffinitySupport,
* ::cudaDeviceGetAttribute,
* ::cudaGetDeviceProperties
*/
int handle_cuDeviceGetAttribute(void *conn) {
    int pi;
    CUdevice_attribute attrib;
    CUdevice dev;

    if (rpc_read(conn, &pi, sizeof(int)) < 0 ||
        rpc_read(conn, &attrib, sizeof(CUdevice_attribute)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetAttribute(&pi, attrib, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pi, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Return NvSciSync attributes that this device can support.
*
* Returns in \p nvSciSyncAttrList, the properties of NvSciSync that
* this CUDA device, \p dev can support. The returned \p nvSciSyncAttrList
* can be used to create an NvSciSync object that matches this device's capabilities.
* 
* If NvSciSyncAttrKey_RequiredPerm field in \p nvSciSyncAttrList is
* already set this API will return ::CUDA_ERROR_INVALID_VALUE.
* 
* The applications should set \p nvSciSyncAttrList to a valid 
* NvSciSyncAttrList failing which this API will return
* ::CUDA_ERROR_INVALID_HANDLE.
* 
* The \p flags controls how applications intends to use
* the NvSciSync created from the \p nvSciSyncAttrList. The valid flags are:
* - ::CUDA_NVSCISYNC_ATTR_SIGNAL, specifies that the applications intends to 
* signal an NvSciSync on this CUDA device.
* - ::CUDA_NVSCISYNC_ATTR_WAIT, specifies that the applications intends to 
* wait on an NvSciSync on this CUDA device.
*
* At least one of these flags must be set, failing which the API
* returns ::CUDA_ERROR_INVALID_VALUE. Both the flags are orthogonal
* to one another: a developer may set both these flags that allows one to
* set both wait and signal specific attributes in the same \p nvSciSyncAttrList.
*
* \param nvSciSyncAttrList     - Return NvSciSync attributes supported.
* \param dev                   - Valid Cuda Device to get NvSciSync attributes for.
* \param flags                 - flags describing NvSciSync usage.
*
* \return
*
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa
* ::cuImportExternalSemaphore,
* ::cuDestroyExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuDeviceGetNvSciSyncAttributes(void *conn) {
    void* nvSciSyncAttrList;
    CUdevice dev;
    int flags;

    if (rpc_read(conn, &nvSciSyncAttrList, sizeof(void*)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &flags, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetNvSciSyncAttributes(&nvSciSyncAttrList, dev, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nvSciSyncAttrList, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the current memory pool of a device
*
* The memory pool must be local to the specified device.
* ::cuMemAllocAsync allocates from the current mempool of the provided stream's device.
* By default, a device's current memory pool is its default memory pool.
*
* \note Use ::cuMemAllocFromPoolAsync to specify asynchronous allocations from a device different
* than the one the stream runs on. 
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuDeviceGetDefaultMemPool, ::cuDeviceGetMemPool, ::cuMemPoolCreate, ::cuMemPoolDestroy, ::cuMemAllocFromPoolAsync
*/
int handle_cuDeviceSetMemPool(void *conn) {
    CUdevice dev;
    CUmemoryPool pool;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceSetMemPool(dev, pool);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the current mempool for a device
*
* Returns the last pool provided to ::cuDeviceSetMemPool for this device
* or the device's default memory pool if ::cuDeviceSetMemPool has never been called.
* By default the current mempool is the default mempool for a device.
* Otherwise the returned pool must have been set with ::cuDeviceSetMemPool.
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuDeviceGetDefaultMemPool, ::cuMemPoolCreate, ::cuDeviceSetMemPool
*/
int handle_cuDeviceGetMemPool(void *conn) {
    CUmemoryPool pool;
    CUdevice dev;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetMemPool(&pool, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the default mempool of a device
*
* The default mempool of a device contains device memory from that device.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuMemAllocAsync, ::cuMemPoolTrimTo, ::cuMemPoolGetAttribute, ::cuMemPoolSetAttribute, cuMemPoolSetAccess, ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuDeviceGetDefaultMemPool(void *conn) {
    CUmemoryPool pool_out;
    CUdevice dev;

    if (rpc_read(conn, &pool_out, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetDefaultMemPool(&pool_out, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pool_out, sizeof(CUmemoryPool)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about the execution affinity support of the device.
*
* Returns in \p *pi whether execution affinity type \p type is supported by device \p dev.
* The supported types are:
* - ::CU_EXEC_AFFINITY_TYPE_SM_COUNT: 1 if context with limited SMs is supported by the device,
*   or 0 if not;
*
* \param pi   - 1 if the execution affinity type \p type is supported by the device, or 0 if not
* \param type - Execution affinity type to query
* \param dev  - Device handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem
*/
int handle_cuDeviceGetExecAffinitySupport(void *conn) {
    int pi;
    CUexecAffinityType type;
    CUdevice dev;

    if (rpc_read(conn, &pi, sizeof(int)) < 0 ||
        rpc_read(conn, &type, sizeof(CUexecAffinityType)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetExecAffinitySupport(&pi, type, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pi, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Blocks until remote writes are visible to the specified scope
*
* Blocks until GPUDirect RDMA writes to the target context via mappings
* created through APIs like nvidia_p2p_get_pages (see
* https://docs.nvidia.com/cuda/gpudirect-rdma for more information), are
* visible to the specified scope.
*
* If the scope equals or lies within the scope indicated by
* ::CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WRITES_ORDERING, the call
* will be a no-op and can be safely omitted for performance. This can be
* determined by comparing the numerical values between the two enums, with
* smaller scopes having smaller values.
*
* Users may query support for this API via
* ::CU_DEVICE_ATTRIBUTE_FLUSH_FLUSH_GPU_DIRECT_RDMA_OPTIONS.
*
* \param target - The target of the operation, see ::CUflushGPUDirectRDMAWritesTarget
* \param scope  - The scope of the operation, see ::CUflushGPUDirectRDMAWritesScope
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
*/
int handle_cuFlushGPUDirectRDMAWrites(void *conn) {
    CUflushGPUDirectRDMAWritesTarget target;
    CUflushGPUDirectRDMAWritesScope scope;

    if (rpc_read(conn, &target, sizeof(CUflushGPUDirectRDMAWritesTarget)) < 0 ||
        rpc_read(conn, &scope, sizeof(CUflushGPUDirectRDMAWritesScope)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFlushGPUDirectRDMAWrites(target, scope);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns properties for a selected device
*
* \deprecated
*
* This function was deprecated as of CUDA 5.0 and replaced by ::cuDeviceGetAttribute().
*
* Returns in \p *prop the properties of device \p dev. The ::CUdevprop
* structure is defined as:
*
* \code
     typedef struct CUdevprop_st {
     int maxThreadsPerBlock;
     int maxThreadsDim[3];
     int maxGridSize[3];
     int sharedMemPerBlock;
     int totalConstantMemory;
     int SIMDWidth;
     int memPitch;
     int regsPerBlock;
     int clockRate;
     int textureAlign
  } CUdevprop;
* \endcode
* where:
*
* - ::maxThreadsPerBlock is the maximum number of threads per block;
* - ::maxThreadsDim[3] is the maximum sizes of each dimension of a block;
* - ::maxGridSize[3] is the maximum sizes of each dimension of a grid;
* - ::sharedMemPerBlock is the total amount of shared memory available per
*   block in bytes;
* - ::totalConstantMemory is the total amount of constant memory available on
*   the device in bytes;
* - ::SIMDWidth is the warp size;
* - ::memPitch is the maximum pitch allowed by the memory copy functions that
*   involve memory regions allocated through ::cuMemAllocPitch();
* - ::regsPerBlock is the total number of registers available per block;
* - ::clockRate is the clock frequency in kilohertz;
* - ::textureAlign is the alignment requirement; texture base addresses that
*   are aligned to ::textureAlign bytes do not need an offset applied to
*   texture fetches.
*
* \param prop - Returned properties of device
* \param dev  - Device to get properties for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem
*/
int handle_cuDeviceGetProperties(void *conn) {
    CUdevprop prop;
    CUdevice dev;

    if (rpc_read(conn, &prop, sizeof(CUdevprop)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetProperties(&prop, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &prop, sizeof(CUdevprop)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the compute capability of the device
*
* \deprecated
*
* This function was deprecated as of CUDA 5.0 and its functionality superseded
* by ::cuDeviceGetAttribute().
*
* Returns in \p *major and \p *minor the major and minor revision numbers that
* define the compute capability of the device \p dev.
*
* \param major - Major revision number
* \param minor - Minor revision number
* \param dev   - Device handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGetAttribute,
* ::cuDeviceGetCount,
* ::cuDeviceGetName,
* ::cuDeviceGetUuid,
* ::cuDeviceGet,
* ::cuDeviceTotalMem
*/
int handle_cuDeviceComputeCapability(void *conn) {
    int major;
    int minor;
    CUdevice dev;

    if (rpc_read(conn, &major, sizeof(int)) < 0 ||
        rpc_read(conn, &minor, sizeof(int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceComputeCapability(&major, &minor, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &major, sizeof(int)) < 0 ||
        rpc_write(conn, &minor, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Retain the primary context on the GPU
*
* Retains the primary context on the device.
* Once the user successfully retains the primary context, the primary context
* will be active and available to the user until the user releases it
* with ::cuDevicePrimaryCtxRelease() or resets it with ::cuDevicePrimaryCtxReset().
* Unlike ::cuCtxCreate() the newly retained context is not pushed onto the stack.
*
* Retaining the primary context for the first time will fail with ::CUDA_ERROR_UNKNOWN
* if the compute mode of the device is ::CU_COMPUTEMODE_PROHIBITED. The function
* ::cuDeviceGetAttribute() can be used with ::CU_DEVICE_ATTRIBUTE_COMPUTE_MODE to
* determine the compute mode  of the device.
* The <i>nvidia-smi</i> tool can be used to set the compute mode for
* devices. Documentation for <i>nvidia-smi</i> can be obtained by passing a
* -h option to it.
*
* Please note that the primary context always supports pinned allocations. Other
* flags can be specified by ::cuDevicePrimaryCtxSetFlags().
*
* \param pctx  - Returned context handle of the new context
* \param dev   - Device for which primary context is requested
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuDevicePrimaryCtxRelease,
* ::cuDevicePrimaryCtxSetFlags,
* ::cuCtxCreate,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuDevicePrimaryCtxRetain(void *conn) {
    CUcontext pctx;
    CUdevice dev;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDevicePrimaryCtxRetain(&pctx, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Release the primary context on the GPU
*
* Releases the primary context interop on the device.
* A retained context should always be released once the user is done using
* it. The context is automatically reset once the last reference to it is
* released. This behavior is different when the primary context was retained
* by the CUDA runtime from CUDA 4.0 and earlier. In this case, the primary
* context remains always active.
*
* Releasing a primary context that has not been previously retained will
* fail with ::CUDA_ERROR_INVALID_CONTEXT.
*
* Please note that unlike ::cuCtxDestroy() this method does not pop the context
* from stack in any circumstances.
*
* \param dev - Device which primary context is released
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuDevicePrimaryCtxRetain,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuDevicePrimaryCtxRelease_v2(void *conn) {
    CUdevice dev;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDevicePrimaryCtxRelease_v2(dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set flags for the primary context
*
* Sets the flags for the primary context on the device overwriting previously
* set ones.
*
* The three LSBs of the \p flags parameter can be used to control how the OS
* thread, which owns the CUDA context at the time of an API call, interacts
* with the OS scheduler when waiting for results from the GPU. Only one of
* the scheduling flags can be set when creating a context.
*
* - ::CU_CTX_SCHED_SPIN: Instruct CUDA to actively spin when waiting for
* results from the GPU. This can decrease latency when waiting for the GPU,
* but may lower the performance of CPU threads if they are performing work in
* parallel with the CUDA thread.
*
* - ::CU_CTX_SCHED_YIELD: Instruct CUDA to yield its thread when waiting for
* results from the GPU. This can increase latency when waiting for the GPU,
* but can increase the performance of CPU threads performing work in parallel
* with the GPU.
*
* - ::CU_CTX_SCHED_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work.
*
* - ::CU_CTX_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work. <br>
* <b>Deprecated:</b> This flag was deprecated as of CUDA 4.0 and was
* replaced with ::CU_CTX_SCHED_BLOCKING_SYNC.
*
* - ::CU_CTX_SCHED_AUTO: The default value if the \p flags parameter is zero,
* uses a heuristic based on the number of active CUDA contexts in the
* process \e C and the number of logical processors in the system \e P. If
* \e C > \e P, then CUDA will yield to other OS threads when waiting for
* the GPU (::CU_CTX_SCHED_YIELD), otherwise CUDA will not yield while
* waiting for results and actively spin on the processor (::CU_CTX_SCHED_SPIN).
* Additionally, on Tegra devices, ::CU_CTX_SCHED_AUTO uses a heuristic based on
* the power profile of the platform and may choose ::CU_CTX_SCHED_BLOCKING_SYNC
* for low-powered devices.
*
* - ::CU_CTX_LMEM_RESIZE_TO_MAX: Instruct CUDA to not reduce local memory
* after resizing local memory for a kernel. This can prevent thrashing by
* local memory allocations when launching many kernels with high local
* memory usage at the cost of potentially increased memory usage. <br>
* <b>Deprecated:</b> This flag is deprecated and the behavior enabled
* by this flag is now the default and cannot be disabled.
*
* \param dev   - Device for which the primary context flags are set
* \param flags - New flags for the device
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa ::cuDevicePrimaryCtxRetain,
* ::cuDevicePrimaryCtxGetState,
* ::cuCtxCreate,
* ::cuCtxGetFlags,
* ::cudaSetDeviceFlags
*/
int handle_cuDevicePrimaryCtxSetFlags_v2(void *conn) {
    CUdevice dev;
    unsigned int flags;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDevicePrimaryCtxSetFlags_v2(dev, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Get the state of the primary context
*
* Returns in \p *flags the flags for the primary context of \p dev, and in
* \p *active whether it is active.  See ::cuDevicePrimaryCtxSetFlags for flag
* values.
*
* \param dev    - Device to get primary context flags for
* \param flags  - Pointer to store flags
* \param active - Pointer to store context state; 0 = inactive, 1 = active
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa
* ::cuDevicePrimaryCtxSetFlags,
* ::cuCtxGetFlags,
* ::cudaGetDeviceFlags
*/
int handle_cuDevicePrimaryCtxGetState(void *conn) {
    CUdevice dev;
    unsigned int flags;
    int active;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &active, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDevicePrimaryCtxGetState(dev, &flags, &active);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &active, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroy all allocations and reset all state on the primary context
*
* Explicitly destroys and cleans up all resources associated with the current
* device in the current process.
*
* Note that it is responsibility of the calling function to ensure that no
* other module in the process is using the device any more. For that reason
* it is recommended to use ::cuDevicePrimaryCtxRelease() in most cases.
* However it is safe for other modules to call ::cuDevicePrimaryCtxRelease()
* even after resetting the device.
* Resetting the primary context does not release it, an application that has
* retained the primary context should explicitly release its usage.
*
* \param dev - Device for which primary context is destroyed
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE
* \notefnerr
*
* \sa ::cuDevicePrimaryCtxRetain,
* ::cuDevicePrimaryCtxRelease,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cudaDeviceReset
*/
int handle_cuDevicePrimaryCtxReset_v2(void *conn) {
    CUdevice dev;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDevicePrimaryCtxReset_v2(dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Create a CUDA context
*
* \note In most cases it is recommended to use ::cuDevicePrimaryCtxRetain.
*
* Creates a new CUDA context and associates it with the calling thread. The
* \p flags parameter is described below. The context is created with a usage
* count of 1 and the caller of ::cuCtxCreate() must call ::cuCtxDestroy()
* when done using the context. If a context is already current to the thread,
* it is supplanted by the newly created context and may be restored by a subsequent
* call to ::cuCtxPopCurrent().
*
* The three LSBs of the \p flags parameter can be used to control how the OS
* thread, which owns the CUDA context at the time of an API call, interacts
* with the OS scheduler when waiting for results from the GPU. Only one of
* the scheduling flags can be set when creating a context.
*
* - ::CU_CTX_SCHED_SPIN: Instruct CUDA to actively spin when waiting for
* results from the GPU. This can decrease latency when waiting for the GPU,
* but may lower the performance of CPU threads if they are performing work in
* parallel with the CUDA thread.
*
* - ::CU_CTX_SCHED_YIELD: Instruct CUDA to yield its thread when waiting for
* results from the GPU. This can increase latency when waiting for the GPU,
* but can increase the performance of CPU threads performing work in parallel
* with the GPU.
*
* - ::CU_CTX_SCHED_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work.
*
* - ::CU_CTX_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work. <br>
* <b>Deprecated:</b> This flag was deprecated as of CUDA 4.0 and was
* replaced with ::CU_CTX_SCHED_BLOCKING_SYNC.
*
* - ::CU_CTX_SCHED_AUTO: The default value if the \p flags parameter is zero,
* uses a heuristic based on the number of active CUDA contexts in the
* process \e C and the number of logical processors in the system \e P. If
* \e C > \e P, then CUDA will yield to other OS threads when waiting for
* the GPU (::CU_CTX_SCHED_YIELD), otherwise CUDA will not yield while
* waiting for results and actively spin on the processor (::CU_CTX_SCHED_SPIN).
* Additionally, on Tegra devices, ::CU_CTX_SCHED_AUTO uses a heuristic based on
* the power profile of the platform and may choose ::CU_CTX_SCHED_BLOCKING_SYNC
* for low-powered devices.
*
* - ::CU_CTX_MAP_HOST: Instruct CUDA to support mapped pinned allocations.
* This flag must be set in order to allocate pinned host memory that is
* accessible to the GPU.
*
* - ::CU_CTX_LMEM_RESIZE_TO_MAX: Instruct CUDA to not reduce local memory
* after resizing local memory for a kernel. This can prevent thrashing by
* local memory allocations when launching many kernels with high local
* memory usage at the cost of potentially increased memory usage. <br>
* <b>Deprecated:</b> This flag is deprecated and the behavior enabled
* by this flag is now the default and cannot be disabled.
* Instead, the per-thread stack size can be controlled with ::cuCtxSetLimit().
*
* Context creation will fail with ::CUDA_ERROR_UNKNOWN if the compute mode of
* the device is ::CU_COMPUTEMODE_PROHIBITED. The function ::cuDeviceGetAttribute()
* can be used with ::CU_DEVICE_ATTRIBUTE_COMPUTE_MODE to determine the
* compute mode of the device. The <i>nvidia-smi</i> tool can be used to set
* the compute mode for * devices.
* Documentation for <i>nvidia-smi</i> can be obtained by passing a
* -h option to it.
*
* \param pctx  - Returned context handle of the new context
* \param flags - Context creation flags
* \param dev   - Device to create context on
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxCreate_v2(void *conn) {
    CUcontext pctx;
    unsigned int flags;
    CUdevice dev;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxCreate_v2(&pctx, flags, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Create a CUDA context with execution affinity
*
* Creates a new CUDA context with execution affinity and associates it with
* the calling thread. The \p paramsArray and \p flags parameter are described below.
* The context is created with a usage count of 1 and the caller of ::cuCtxCreate() must
* call ::cuCtxDestroy() when done using the context. If a context is already
* current to the thread, it is supplanted by the newly created context and may
* be restored by a subsequent call to ::cuCtxPopCurrent().
*
* The type and the amount of execution resource the context can use is limited by \p paramsArray
* and \p numParams. The \p paramsArray is an array of \p CUexecAffinityParam and the \p numParams
* describes the size of the array. If two \p CUexecAffinityParam in the array have the same type,
* the latter execution affinity parameter overrides the former execution affinity parameter.
* The supported execution affinity types are:
* - ::CU_EXEC_AFFINITY_TYPE_SM_COUNT limits the portion of SMs that the context can use. The portion
*   of SMs is specified as the number of SMs via \p CUexecAffinitySmCount. This limit will be internally
*   rounded up to the next hardware-supported amount. Hence, it is imperative to query the actual execution
*   affinity of the context via \p cuCtxGetExecAffinity after context creation. Currently, this attribute
*   is only supported under Volta+ MPS.
*
* The three LSBs of the \p flags parameter can be used to control how the OS
* thread, which owns the CUDA context at the time of an API call, interacts
* with the OS scheduler when waiting for results from the GPU. Only one of
* the scheduling flags can be set when creating a context.
*
* - ::CU_CTX_SCHED_SPIN: Instruct CUDA to actively spin when waiting for
* results from the GPU. This can decrease latency when waiting for the GPU,
* but may lower the performance of CPU threads if they are performing work in
* parallel with the CUDA thread.
*
* - ::CU_CTX_SCHED_YIELD: Instruct CUDA to yield its thread when waiting for
* results from the GPU. This can increase latency when waiting for the GPU,
* but can increase the performance of CPU threads performing work in parallel
* with the GPU.
*
* - ::CU_CTX_SCHED_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work.
*
* - ::CU_CTX_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
* synchronization primitive when waiting for the GPU to finish work. <br>
* <b>Deprecated:</b> This flag was deprecated as of CUDA 4.0 and was
* replaced with ::CU_CTX_SCHED_BLOCKING_SYNC.
*
* - ::CU_CTX_SCHED_AUTO: The default value if the \p flags parameter is zero,
* uses a heuristic based on the number of active CUDA contexts in the
* process \e C and the number of logical processors in the system \e P. If
* \e C > \e P, then CUDA will yield to other OS threads when waiting for
* the GPU (::CU_CTX_SCHED_YIELD), otherwise CUDA will not yield while
* waiting for results and actively spin on the processor (::CU_CTX_SCHED_SPIN).
* Additionally, on Tegra devices, ::CU_CTX_SCHED_AUTO uses a heuristic based on
* the power profile of the platform and may choose ::CU_CTX_SCHED_BLOCKING_SYNC
* for low-powered devices.
*
* - ::CU_CTX_MAP_HOST: Instruct CUDA to support mapped pinned allocations.
* This flag must be set in order to allocate pinned host memory that is
* accessible to the GPU.
*
* - ::CU_CTX_LMEM_RESIZE_TO_MAX: Instruct CUDA to not reduce local memory
* after resizing local memory for a kernel. This can prevent thrashing by
* local memory allocations when launching many kernels with high local
* memory usage at the cost of potentially increased memory usage. <br>
* <b>Deprecated:</b> This flag is deprecated and the behavior enabled
* by this flag is now the default and cannot be disabled.
* Instead, the per-thread stack size can be controlled with ::cuCtxSetLimit().
*
* Context creation will fail with ::CUDA_ERROR_UNKNOWN if the compute mode of
* the device is ::CU_COMPUTEMODE_PROHIBITED. The function ::cuDeviceGetAttribute()
* can be used with ::CU_DEVICE_ATTRIBUTE_COMPUTE_MODE to determine the
* compute mode of the device. The <i>nvidia-smi</i> tool can be used to set
* the compute mode for * devices.
* Documentation for <i>nvidia-smi</i> can be obtained by passing a
* -h option to it.
*
* \param pctx        - Returned context handle of the new context
* \param paramsArray - Execution affinity parameters
* \param numParams   - Number of execution affinity parameters
* \param flags       - Context creation flags
* \param dev         - Device to create context on
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNSUPPORTED_EXEC_AFFINITY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::CUexecAffinityParam
*/
int handle_cuCtxCreate_v3(void *conn) {
    CUcontext pctx;
    CUexecAffinityParam paramsArray;
    int numParams;
    unsigned int flags;
    CUdevice dev;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &paramsArray, sizeof(CUexecAffinityParam)) < 0 ||
        rpc_read(conn, &numParams, sizeof(int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxCreate_v3(&pctx, &paramsArray, numParams, flags, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &paramsArray, sizeof(CUexecAffinityParam)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroy a CUDA context
*
* Destroys the CUDA context specified by \p ctx.  The context \p ctx will be
* destroyed regardless of how many threads it is current to.
* It is the responsibility of the calling function to ensure that no API
* call issues using \p ctx while ::cuCtxDestroy() is executing.
*
* Destroys and cleans up all resources associated with the context.
* It is the caller's responsibility to ensure that the context or its resources
* are not accessed or passed in subsequent API calls and doing so will result in undefined behavior.
* These resources include CUDA types such as ::CUmodule, ::CUfunction, ::CUstream, ::CUevent,
* ::CUarray, ::CUmipmappedArray, ::CUtexObject, ::CUsurfObject, ::CUtexref, ::CUsurfref,
* ::CUgraphicsResource, ::CUlinkState, ::CUexternalMemory and ::CUexternalSemaphore.
*
* If \p ctx is current to the calling thread then \p ctx will also be
* popped from the current thread's context stack (as though ::cuCtxPopCurrent()
* were called).  If \p ctx is current to other threads, then \p ctx will
* remain current to those threads, and attempting to access \p ctx from
* those threads will result in the error ::CUDA_ERROR_CONTEXT_IS_DESTROYED.
*
* \param ctx - Context to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxDestroy_v2(void *conn) {
    CUcontext ctx;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxDestroy_v2(ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Pushes a context on the current CPU thread
*
* Pushes the given context \p ctx onto the CPU thread's stack of current
* contexts. The specified context becomes the CPU thread's current context, so
* all CUDA functions that operate on the current context are affected.
*
* The previous current context may be made current again by calling
* ::cuCtxDestroy() or ::cuCtxPopCurrent().
*
* \param ctx - Context to push
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxPushCurrent_v2(void *conn) {
    CUcontext ctx;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxPushCurrent_v2(ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Pops the current CUDA context from the current CPU thread.
*
* Pops the current CUDA context from the CPU thread and passes back the
* old context handle in \p *pctx. That context may then be made current
* to a different CPU thread by calling ::cuCtxPushCurrent().
*
* If a context was current to the CPU thread before ::cuCtxCreate() or
* ::cuCtxPushCurrent() was called, this function makes that context current to
* the CPU thread again.
*
* \param pctx - Returned popped context handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxPopCurrent_v2(void *conn) {
    CUcontext pctx;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxPopCurrent_v2(&pctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Binds the specified CUDA context to the calling CPU thread
*
* Binds the specified CUDA context to the calling CPU thread.
* If \p ctx is NULL then the CUDA context previously bound to the
* calling CPU thread is unbound and ::CUDA_SUCCESS is returned.
*
* If there exists a CUDA context stack on the calling CPU thread, this
* will replace the top of that stack with \p ctx.
* If \p ctx is NULL then this will be equivalent to popping the top
* of the calling CPU thread's CUDA context stack (or a no-op if the
* calling CPU thread's CUDA context stack is empty).
*
* \param ctx - Context to bind to the calling CPU thread
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa
* ::cuCtxGetCurrent,
* ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cudaSetDevice
*/
int handle_cuCtxSetCurrent(void *conn) {
    CUcontext ctx;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxSetCurrent(ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the CUDA context bound to the calling CPU thread.
*
* Returns in \p *pctx the CUDA context bound to the calling CPU thread.
* If no context is bound to the calling CPU thread then \p *pctx is
* set to NULL and ::CUDA_SUCCESS is returned.
*
* \param pctx - Returned context handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* \notefnerr
*
* \sa
* ::cuCtxSetCurrent,
* ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cudaGetDevice
*/
int handle_cuCtxGetCurrent(void *conn) {
    CUcontext pctx;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetCurrent(&pctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the device ID for the current context
*
* Returns in \p *device the ordinal of the current context's device.
*
* \param device - Returned device ID for the current context
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cudaGetDevice
*/
int handle_cuCtxGetDevice(void *conn) {
    CUdevice device;

    if (rpc_read(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetDevice(&device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the flags for the current context
*
* Returns in \p *flags the flags of the current context. See ::cuCtxCreate
* for flag values.
*
* \param flags - Pointer to store flags of current context
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetCurrent,
* ::cuCtxGetDevice,
* ::cuCtxGetLimit,
* ::cuCtxGetSharedMemConfig,
* ::cuCtxGetStreamPriorityRange,
* ::cudaGetDeviceFlags
*/
int handle_cuCtxGetFlags(void *conn) {
    unsigned int flags;

    if (rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetFlags(&flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the unique Id associated with the context supplied
*
* Returns in \p ctxId the unique Id which is associated with a given context.
* The Id is unique for the life of the program for this instance of CUDA.
* If context is supplied as NULL and there is one current, the Id of the
* current context is returned.
*
* \param ctx - Context for which to obtain the Id
* \param ctxId - Pointer to store the Id of the context
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPushCurrent
*/
int handle_cuCtxGetId(void *conn) {
    CUcontext ctx;
    unsigned long long ctxId;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &ctxId, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetId(ctx, &ctxId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ctxId, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* \brief Block for a context's tasks to complete
*
* Blocks until the device has completed all preceding requested tasks.
* ::cuCtxSynchronize() returns an error if one of the preceding tasks failed.
* If the context was created with the ::CU_CTX_SCHED_BLOCKING_SYNC flag, the
* CPU thread will block until the GPU context has finished its work.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cudaDeviceSynchronize
*/
int handle_cuCtxSynchronize(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxSynchronize();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set resource limits
*
* Setting \p limit to \p value is a request by the application to update
* the current limit maintained by the context. The driver is free to
* modify the requested value to meet h/w requirements (this could be
* clamping to minimum or maximum values, rounding up to nearest element
* size, etc). The application can use ::cuCtxGetLimit() to find out exactly
* what the limit has been set to.
*
* Setting each ::CUlimit has its own specific restrictions, so each is
* discussed here.
*
* - ::CU_LIMIT_STACK_SIZE controls the stack size in bytes of each GPU thread.
*   The driver automatically increases the per-thread stack size
*   for each kernel launch as needed. This size isn't reset back to the
*   original value after each launch. Setting this value will take effect 
*   immediately, and if necessary, the device will block until all preceding 
*   requested tasks are complete.
*
* - ::CU_LIMIT_PRINTF_FIFO_SIZE controls the size in bytes of the FIFO used
*   by the ::printf() device system call. Setting ::CU_LIMIT_PRINTF_FIFO_SIZE
*   must be performed before launching any kernel that uses the ::printf()
*   device system call, otherwise ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* - ::CU_LIMIT_MALLOC_HEAP_SIZE controls the size in bytes of the heap used
*   by the ::malloc() and ::free() device system calls. Setting
*   ::CU_LIMIT_MALLOC_HEAP_SIZE must be performed before launching any kernel
*   that uses the ::malloc() or ::free() device system calls, otherwise
*   ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* - ::CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH controls the maximum nesting depth of
*   a grid at which a thread can safely call ::cudaDeviceSynchronize(). Setting
*   this limit must be performed before any launch of a kernel that uses the
*   device runtime and calls ::cudaDeviceSynchronize() above the default sync
*   depth, two levels of grids. Calls to ::cudaDeviceSynchronize() will fail
*   with error code ::cudaErrorSyncDepthExceeded if the limitation is
*   violated. This limit can be set smaller than the default or up the maximum
*   launch depth of 24. When setting this limit, keep in mind that additional
*   levels of sync depth require the driver to reserve large amounts of device
*   memory which can no longer be used for user allocations. If these
*   reservations of device memory fail, ::cuCtxSetLimit() will return
*   ::CUDA_ERROR_OUT_OF_MEMORY, and the limit can be reset to a lower value.
*   This limit is only applicable to devices of compute capability < 9.0.
*   Attempting to set this limit on devices of other compute capability
*   versions will result in the error ::CUDA_ERROR_UNSUPPORTED_LIMIT being
*   returned.
*
* - ::CU_LIMIT_DEV_RUNTIME_PENDING_LAUNCH_COUNT controls the maximum number of
*   outstanding device runtime launches that can be made from the current
*   context. A grid is outstanding from the point of launch up until the grid
*   is known to have been completed. Device runtime launches which violate
*   this limitation fail and return ::cudaErrorLaunchPendingCountExceeded when
*   ::cudaGetLastError() is called after launch. If more pending launches than
*   the default (2048 launches) are needed for a module using the device
*   runtime, this limit can be increased. Keep in mind that being able to
*   sustain additional pending launches will require the driver to reserve
*   larger amounts of device memory upfront which can no longer be used for
*   allocations. If these reservations fail, ::cuCtxSetLimit() will return
*   ::CUDA_ERROR_OUT_OF_MEMORY, and the limit can be reset to a lower value.
*   This limit is only applicable to devices of compute capability 3.5 and
*   higher. Attempting to set this limit on devices of compute capability less
*   than 3.5 will result in the error ::CUDA_ERROR_UNSUPPORTED_LIMIT being
*   returned.
*
* - ::CU_LIMIT_MAX_L2_FETCH_GRANULARITY controls the L2 cache fetch granularity.
*   Values can range from 0B to 128B. This is purely a performence hint and
*   it can be ignored or clamped depending on the platform.
*
* - ::CU_LIMIT_PERSISTING_L2_CACHE_SIZE controls size in bytes availabe for
*   persisting L2 cache. This is purely a performance hint and it can be
*   ignored or clamped depending on the platform.
*
* \param limit - Limit to set
* \param value - Size of limit
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNSUPPORTED_LIMIT,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSynchronize,
* ::cudaDeviceSetLimit
*/
int handle_cuCtxSetLimit(void *conn) {
    CUlimit limit;
    size_t value;

    if (rpc_read(conn, &limit, sizeof(CUlimit)) < 0 ||
        rpc_read(conn, &value, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxSetLimit(limit, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns resource limits
*
* Returns in \p *pvalue the current size of \p limit.  The supported
* ::CUlimit values are:
* - ::CU_LIMIT_STACK_SIZE: stack size in bytes of each GPU thread.
* - ::CU_LIMIT_PRINTF_FIFO_SIZE: size in bytes of the FIFO used by the
*   ::printf() device system call.
* - ::CU_LIMIT_MALLOC_HEAP_SIZE: size in bytes of the heap used by the
*   ::malloc() and ::free() device system calls.
* - ::CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH: maximum grid depth at which a thread
*   can issue the device runtime call ::cudaDeviceSynchronize() to wait on
*   child grid launches to complete.
* - ::CU_LIMIT_DEV_RUNTIME_PENDING_LAUNCH_COUNT: maximum number of outstanding
*   device runtime launches that can be made from this context.
* - ::CU_LIMIT_MAX_L2_FETCH_GRANULARITY: L2 cache fetch granularity.
* - ::CU_LIMIT_PERSISTING_L2_CACHE_SIZE: Persisting L2 cache size in bytes
*
* \param limit  - Limit to query
* \param pvalue - Returned size of limit
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNSUPPORTED_LIMIT
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cudaDeviceGetLimit
*/
int handle_cuCtxGetLimit(void *conn) {
    size_t pvalue;
    CUlimit limit;

    if (rpc_read(conn, &pvalue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(CUlimit)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetLimit(&pvalue, limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pvalue, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the preferred cache configuration for the current context.
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this function returns through \p pconfig the preferred cache configuration
* for the current context. This is only a preference. The driver will use
* the requested configuration if possible, but it is free to choose a different
* configuration if required to execute functions.
*
* This will return a \p pconfig of ::CU_FUNC_CACHE_PREFER_NONE on devices
* where the size of the L1 cache and shared memory are fixed.
*
* The supported cache configurations are:
* - ::CU_FUNC_CACHE_PREFER_NONE: no preference for shared memory or L1 (default)
* - ::CU_FUNC_CACHE_PREFER_SHARED: prefer larger shared memory and smaller L1 cache
* - ::CU_FUNC_CACHE_PREFER_L1: prefer larger L1 cache and smaller shared memory
* - ::CU_FUNC_CACHE_PREFER_EQUAL: prefer equal sized L1 cache and shared memory
*
* \param pconfig - Returned cache configuration
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cuFuncSetCacheConfig,
* ::cudaDeviceGetCacheConfig
*/
int handle_cuCtxGetCacheConfig(void *conn) {
    CUfunc_cache pconfig;

    if (rpc_read(conn, &pconfig, sizeof(CUfunc_cache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetCacheConfig(&pconfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pconfig, sizeof(CUfunc_cache)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for the current context.
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p config the preferred cache configuration for
* the current context. This is only a preference. The driver will use
* the requested configuration if possible, but it is free to choose a different
* configuration if required to execute the function. Any function preference
* set via ::cuFuncSetCacheConfig() or ::cuKernelSetCacheConfig() will be preferred over this context-wide
* setting. Setting the context-wide cache configuration to
* ::CU_FUNC_CACHE_PREFER_NONE will cause subsequent kernel launches to prefer
* to not change the cache configuration unless required to launch the kernel.
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
* The supported cache configurations are:
* - ::CU_FUNC_CACHE_PREFER_NONE: no preference for shared memory or L1 (default)
* - ::CU_FUNC_CACHE_PREFER_SHARED: prefer larger shared memory and smaller L1 cache
* - ::CU_FUNC_CACHE_PREFER_L1: prefer larger L1 cache and smaller shared memory
* - ::CU_FUNC_CACHE_PREFER_EQUAL: prefer equal sized L1 cache and shared memory
*
* \param config - Requested cache configuration
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cuFuncSetCacheConfig,
* ::cudaDeviceSetCacheConfig,
* ::cuKernelSetCacheConfig
*/
int handle_cuCtxSetCacheConfig(void *conn) {
    CUfunc_cache config;

    if (rpc_read(conn, &config, sizeof(CUfunc_cache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxSetCacheConfig(config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the current shared memory configuration for the current context.
*
* This function will return in \p pConfig the current size of shared memory banks
* in the current context. On devices with configurable shared memory banks,
* ::cuCtxSetSharedMemConfig can be used to change this setting, so that all
* subsequent kernel launches will by default use the new bank size. When
* ::cuCtxGetSharedMemConfig is called on devices without configurable shared
* memory, it will return the fixed bank size of the hardware.
*
* The returned bank configurations can be either:
* - ::CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE:  shared memory bank width is
*   four bytes.
* - ::CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE: shared memory bank width will
*   eight bytes.
*
* \param pConfig - returned shared memory configuration
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cuCtxGetSharedMemConfig,
* ::cuFuncSetCacheConfig,
* ::cudaDeviceGetSharedMemConfig
*/
int handle_cuCtxGetSharedMemConfig(void *conn) {
    CUsharedconfig pConfig;

    if (rpc_read(conn, &pConfig, sizeof(CUsharedconfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetSharedMemConfig(&pConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pConfig, sizeof(CUsharedconfig)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the shared memory configuration for the current context.
*
* On devices with configurable shared memory banks, this function will set
* the context's shared memory bank size which is used for subsequent kernel
* launches.
*
* Changed the shared memory configuration between launches may insert a device
* side synchronization point between those launches.
*
* Changing the shared memory bank size will not increase shared memory usage
* or affect occupancy of kernels, but may have major effects on performance.
* Larger bank sizes will allow for greater potential bandwidth to shared memory,
* but will change what kinds of accesses to shared memory will result in bank
* conflicts.
*
* This function will do nothing on devices with fixed shared memory bank size.
*
* The supported bank configurations are:
* - ::CU_SHARED_MEM_CONFIG_DEFAULT_BANK_SIZE: set bank width to the default initial
*   setting (currently, four bytes).
* - ::CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE: set shared memory bank width to
*   be natively four bytes.
* - ::CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE: set shared memory bank width to
*   be natively eight bytes.
*
* \param config - requested shared memory configuration
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cuCtxGetSharedMemConfig,
* ::cuFuncSetCacheConfig,
* ::cudaDeviceSetSharedMemConfig
*/
int handle_cuCtxSetSharedMemConfig(void *conn) {
    CUsharedconfig config;

    if (rpc_read(conn, &config, sizeof(CUsharedconfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxSetSharedMemConfig(config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the context's API version.
*
* Returns a version number in \p version corresponding to the capabilities of
* the context (e.g. 3010 or 3020), which library developers can use to direct
* callers to a specific API version. If \p ctx is NULL, returns the API version
* used to create the currently bound context.
*
* Note that new API versions are only introduced when context capabilities are
* changed that break binary compatibility, so the API version and driver version
* may be different. For example, it is valid for the API version to be 3020 while
* the driver version is 4020.
*
* \param ctx     - Context to check
* \param version - Pointer to version
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxGetApiVersion(void *conn) {
    CUcontext ctx;
    unsigned int version;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &version, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetApiVersion(ctx, &version);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &version, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns numerical values that correspond to the least and
* greatest stream priorities.
*
* Returns in \p *leastPriority and \p *greatestPriority the numerical values that correspond
* to the least and greatest stream priorities respectively. Stream priorities
* follow a convention where lower numbers imply greater priorities. The range of
* meaningful stream priorities is given by [\p *greatestPriority, \p *leastPriority].
* If the user attempts to create a stream with a priority value that is
* outside the meaningful range as specified by this API, the priority is
* automatically clamped down or up to either \p *leastPriority or \p *greatestPriority
* respectively. See ::cuStreamCreateWithPriority for details on creating a
* priority stream.
* A NULL may be passed in for \p *leastPriority or \p *greatestPriority if the value
* is not desired.
*
* This function will return '0' in both \p *leastPriority and \p *greatestPriority if
* the current context's device does not support stream priorities
* (see ::cuDeviceGetAttribute).
*
* \param leastPriority    - Pointer to an int in which the numerical value for least
*                           stream priority is returned
* \param greatestPriority - Pointer to an int in which the numerical value for greatest
*                           stream priority is returned
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa ::cuStreamCreateWithPriority,
* ::cuStreamGetPriority,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize,
* ::cudaDeviceGetStreamPriorityRange
*/
int handle_cuCtxGetStreamPriorityRange(void *conn) {
    int leastPriority;
    int greatestPriority;

    if (rpc_read(conn, &leastPriority, sizeof(int)) < 0 ||
        rpc_read(conn, &greatestPriority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetStreamPriorityRange(&leastPriority, &greatestPriority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &leastPriority, sizeof(int)) < 0 ||
        rpc_write(conn, &greatestPriority, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Resets all persisting lines in cache to normal status.
*
* ::cuCtxResetPersistingL2Cache Resets all persisting lines in cache to normal
* status. Takes effect on function return.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuCtxResetPersistingL2Cache(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxResetPersistingL2Cache();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the execution affinity setting for the current context.
*
* Returns in \p *pExecAffinity the current value of \p type. The supported
* ::CUexecAffinityType values are:
* - ::CU_EXEC_AFFINITY_TYPE_SM_COUNT: number of SMs the context is limited to use.
*
* \param type          - Execution affinity type to query
* \param pExecAffinity - Returned execution affinity
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNSUPPORTED_EXEC_AFFINITY
* \notefnerr
*
* \sa
* ::CUexecAffinityParam
*/
int handle_cuCtxGetExecAffinity(void *conn) {
    CUexecAffinityParam pExecAffinity;
    CUexecAffinityType type;

    if (rpc_read(conn, &pExecAffinity, sizeof(CUexecAffinityParam)) < 0 ||
        rpc_read(conn, &type, sizeof(CUexecAffinityType)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxGetExecAffinity(&pExecAffinity, type);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pExecAffinity, sizeof(CUexecAffinityParam)) < 0)
        return -1;

    return result;
}

/**
* \brief Increment a context's usage-count
*
* \deprecated
*
* Note that this function is deprecated and should not be used.
*
* Increments the usage count of the context and passes back a context handle
* in \p *pctx that must be passed to ::cuCtxDetach() when the application is
* done with the context. ::cuCtxAttach() fails if there is no context current
* to the thread.
*
* Currently, the \p flags parameter must be 0.
*
* \param pctx  - Returned context handle of the current context
* \param flags - Context attach flags (must be 0)
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxDetach,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxAttach(void *conn) {
    CUcontext pctx;
    unsigned int flags;

    if (rpc_read(conn, &pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxAttach(&pctx, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Decrement a context's usage-count
*
* \deprecated
*
* Note that this function is deprecated and should not be used.
*
* Decrements the usage count of the context \p ctx, and destroys the context
* if the usage count goes to 0. The context must be a handle that was passed
* back by ::cuCtxCreate() or ::cuCtxAttach(), and must be current to the
* calling thread.
*
* \param ctx - Context to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxCreate,
* ::cuCtxDestroy,
* ::cuCtxGetApiVersion,
* ::cuCtxGetCacheConfig,
* ::cuCtxGetDevice,
* ::cuCtxGetFlags,
* ::cuCtxGetLimit,
* ::cuCtxPopCurrent,
* ::cuCtxPushCurrent,
* ::cuCtxSetCacheConfig,
* ::cuCtxSetLimit,
* ::cuCtxSynchronize
*/
int handle_cuCtxDetach(void *conn) {
    CUcontext ctx;

    if (rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxDetach(ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Loads a compute module
*
* Takes a filename \p fname and loads the corresponding module \p module into
* the current context. The CUDA driver API does not attempt to lazily
* allocate the resources needed by a module; if the memory for functions and
* data (constant and global) needed by the module cannot be allocated,
* ::cuModuleLoad() fails. The file should be a \e cubin file as output by
* \b nvcc, or a \e PTX file either as output by \b nvcc or handwritten, or
* a \e fatbin file as output by \b nvcc from toolchain 4.0 or later.
*
* \param module - Returned module
* \param fname  - Filename of module to load
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_FILE_NOT_FOUND,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleLoad(void *conn) {
    CUmodule module;
    char fname;

    if (rpc_read(conn, &module, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &fname, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleLoad(&module, &fname);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &module, sizeof(CUmodule)) < 0)
        return -1;

    return result;
}

/**
* \brief Load a module's data
*
* Takes a pointer \p image and loads the corresponding module \p module into
* the current context. The pointer may be obtained by mapping a \e cubin or
* \e PTX or \e fatbin file, passing a \e cubin or \e PTX or \e fatbin file
* as a NULL-terminated text string, or incorporating a \e cubin or \e fatbin
* object into the executable resources and using operating system calls such
* as Windows \c FindResource() to obtain the pointer.
*
* \param module - Returned module
* \param image  - Module data to load
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleLoadData(void *conn) {
    CUmodule module;
    void* image;

    if (rpc_read(conn, &module, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &image, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleLoadData(&module, &image);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &module, sizeof(CUmodule)) < 0)
        return -1;

    return result;
}

/**
* \brief Load a module's data with options
*
* Takes a pointer \p image and loads the corresponding module \p module into
* the current context. The pointer may be obtained by mapping a \e cubin or
* \e PTX or \e fatbin file, passing a \e cubin or \e PTX or \e fatbin file
* as a NULL-terminated text string, or incorporating a \e cubin or \e fatbin
* object into the executable resources and using operating system calls such
* as Windows \c FindResource() to obtain the pointer. Options are passed as
* an array via \p options and any corresponding parameters are passed in
* \p optionValues. The number of total options is supplied via \p numOptions.
* Any outputs will be returned via \p optionValues.
*
* \param module       - Returned module
* \param image        - Module data to load
* \param numOptions   - Number of options
* \param options      - Options for JIT
* \param optionValues - Option values for JIT
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleLoadDataEx(void *conn) {
    CUmodule module;
    void* image;
    unsigned int numOptions;
    CUjit_option options;
    void* optionValues;

    if (rpc_read(conn, &module, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &image, sizeof(void*)) < 0 ||
        rpc_read(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleLoadDataEx(&module, &image, numOptions, &options, &optionValues);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &module, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Load a module's data
*
* Takes a pointer \p fatCubin and loads the corresponding module \p module
* into the current context. The pointer represents a <i>fat binary</i> object,
* which is a collection of different \e cubin and/or \e PTX files, all
* representing the same device code, but compiled and optimized for different
* architectures.
*
* Prior to CUDA 4.0, there was no documented API for constructing and using
* fat binary objects by programmers.  Starting with CUDA 4.0, fat binary
* objects can be constructed by providing the <i>-fatbin option</i> to \b nvcc.
* More information can be found in the \b nvcc document.
*
* \param module   - Returned module
* \param fatCubin - Fat binary to load
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleUnload
*/
int handle_cuModuleLoadFatBinary(void *conn) {
    CUmodule module;
    void* fatCubin;

    if (rpc_read(conn, &module, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &fatCubin, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleLoadFatBinary(&module, &fatCubin);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &module, sizeof(CUmodule)) < 0)
        return -1;

    return result;
}

/**
* \brief Unloads a module
*
* Unloads a module \p hmod from the current context.
*
* \param hmod - Module to unload
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_destroy_ub
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary
*/
int handle_cuModuleUnload(void *conn) {
    CUmodule hmod;

    if (rpc_read(conn, &hmod, sizeof(CUmodule)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleUnload(hmod);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Query lazy loading mode
*
* Returns lazy loading mode
* Module loading mode is controlled by CUDA_MODULE_LOADING env variable
*
* \param mode      - Returns the lazy loading mode
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \notefnerr
*
* \sa
* ::cuModuleLoad,
*/
int handle_cuModuleGetLoadingMode(void *conn) {
    CUmoduleLoadingMode mode;

    if (rpc_read(conn, &mode, sizeof(CUmoduleLoadingMode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleGetLoadingMode(&mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(CUmoduleLoadingMode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a function handle
*
* Returns in \p *hfunc the handle of the function of name \p name located in
* module \p hmod. If no function of that name exists, ::cuModuleGetFunction()
* returns ::CUDA_ERROR_NOT_FOUND.
*
* \param hfunc - Returned function handle
* \param hmod  - Module to retrieve function from
* \param name  - Name of function to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleGetFunction(void *conn) {
    CUfunction hfunc;
    CUmodule hmod;
    char name;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleGetFunction(&hfunc, hmod, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a global pointer from a module
*
* Returns in \p *dptr and \p *bytes the base pointer and size of the
* global of name \p name located in module \p hmod. If no variable of that name
* exists, ::cuModuleGetGlobal() returns ::CUDA_ERROR_NOT_FOUND. Both
* parameters \p dptr and \p bytes are optional. If one of them is
* NULL, it is ignored.
*
* \param dptr  - Returned global device pointer
* \param bytes - Returned global size in bytes
* \param hmod  - Module to retrieve global from
* \param name  - Name of global to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_FOUND
* \notefnerr
*
* \sa ::cuModuleGetFunction,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload,
* ::cudaGetSymbolAddress,
* ::cudaGetSymbolSize
*/
int handle_cuModuleGetGlobal_v2(void *conn) {
    CUdeviceptr dptr;
    size_t bytes;
    CUmodule hmod;
    char name;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleGetGlobal_v2(&dptr, &bytes, hmod, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a pending JIT linker invocation.
*
* If the call is successful, the caller owns the returned CUlinkState, which
* should eventually be destroyed with ::cuLinkDestroy.  The
* device code machine size (32 or 64 bit) will match the calling application.
*
* Both linker and compiler options may be specified.  Compiler options will
* be applied to inputs to this linker action which must be compiled from PTX.
* The options ::CU_JIT_WALL_TIME,
* ::CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES, and ::CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES
* will accumulate data until the CUlinkState is destroyed.
*
* \p optionValues must remain valid for the life of the CUlinkState if output
* options are used.  No other references to inputs are maintained after this
* call returns.
*
* \note For LTO-IR input, only LTO-IR compiled with toolkits prior to CUDA 12.0 will be accepted
*
* \param numOptions   Size of options arrays
* \param options      Array of linker and compiler options
* \param optionValues Array of option values, each cast to void *
* \param stateOut     On success, this will contain a CUlinkState to specify
*                     and complete this action
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
* \notefnerr
*
* \sa ::cuLinkAddData,
* ::cuLinkAddFile,
* ::cuLinkComplete,
* ::cuLinkDestroy
*/
int handle_cuLinkCreate_v2(void *conn) {
    unsigned int numOptions;
    CUjit_option options;
    void* optionValues;
    CUlinkState stateOut;

    if (rpc_read(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &optionValues, sizeof(void*)) < 0 ||
        rpc_read(conn, &stateOut, sizeof(CUlinkState)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLinkCreate_v2(numOptions, &options, &optionValues, &stateOut);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &optionValues, sizeof(void*)) < 0 ||
        rpc_write(conn, &stateOut, sizeof(CUlinkState)) < 0)
        return -1;

    return result;
}

/**
* \brief Add an input to a pending linker invocation
*
* Ownership of \p data is retained by the caller.  No reference is retained to any
* inputs after this call returns.
*
* This method accepts only compiler options, which are used if the data must
* be compiled from PTX, and does not accept any of
* ::CU_JIT_WALL_TIME, ::CU_JIT_INFO_LOG_BUFFER, ::CU_JIT_ERROR_LOG_BUFFER,
* ::CU_JIT_TARGET_FROM_CUCONTEXT, or ::CU_JIT_TARGET.
*
* \note For LTO-IR input, only LTO-IR compiled with toolkits prior to CUDA 12.0 will be accepted
*
* \param state        A pending linker action.
* \param type         The type of the input data.
* \param data         The input data.  PTX must be NULL-terminated.
* \param size         The length of the input data.
* \param name         An optional name for this input in log messages.
* \param numOptions   Size of options.
* \param options      Options to be applied only for this input (overrides options from ::cuLinkCreate).
* \param optionValues Array of option values, each cast to void *.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU
*
* \sa ::cuLinkCreate,
* ::cuLinkAddFile,
* ::cuLinkComplete,
* ::cuLinkDestroy
*/
int handle_cuLinkAddData_v2(void *conn) {
    CUlinkState state;
    CUjitInputType type;
    void* data;
    size_t size;
    char name;
    unsigned int numOptions;
    CUjit_option options;
    void* optionValues;

    if (rpc_read(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_read(conn, &type, sizeof(CUjitInputType)) < 0 ||
        rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0 ||
        rpc_read(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLinkAddData_v2(state, type, &data, size, &name, numOptions, &options, &optionValues);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0 ||
        rpc_write(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Add a file input to a pending linker invocation
*
* No reference is retained to any inputs after this call returns.
*
* This method accepts only compiler options, which are used if the input
* must be compiled from PTX, and does not accept any of
* ::CU_JIT_WALL_TIME, ::CU_JIT_INFO_LOG_BUFFER, ::CU_JIT_ERROR_LOG_BUFFER,
* ::CU_JIT_TARGET_FROM_CUCONTEXT, or ::CU_JIT_TARGET.
*
* This method is equivalent to invoking ::cuLinkAddData on the contents
* of the file.
*
* \note For LTO-IR input, only LTO-IR compiled with toolkits prior to CUDA 12.0 will be accepted
*
* \param state        A pending linker action
* \param type         The type of the input data
* \param path         Path to the input file
* \param numOptions   Size of options
* \param options      Options to be applied only for this input (overrides options from ::cuLinkCreate)
* \param optionValues Array of option values, each cast to void *
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_FILE_NOT_FOUND
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU
*
* \sa ::cuLinkCreate,
* ::cuLinkAddData,
* ::cuLinkComplete,
* ::cuLinkDestroy
*/
int handle_cuLinkAddFile_v2(void *conn) {
    CUlinkState state;
    CUjitInputType type;
    char path;
    unsigned int numOptions;
    CUjit_option options;
    void* optionValues;

    if (rpc_read(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_read(conn, &type, sizeof(CUjitInputType)) < 0 ||
        rpc_read(conn, &path, sizeof(char)) < 0 ||
        rpc_read(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLinkAddFile_v2(state, type, &path, numOptions, &options, &optionValues);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &options, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &optionValues, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Complete a pending linker invocation
*
* Completes the pending linker action and returns the cubin image for the linked
* device code, which can be used with ::cuModuleLoadData.  The cubin is owned by
* \p state, so it should be loaded before \p state is destroyed via ::cuLinkDestroy.
* This call does not destroy \p state.
*
* \param state    A pending linker invocation
* \param cubinOut On success, this will point to the output image
* \param sizeOut  Optional parameter to receive the size of the generated image
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuLinkCreate,
* ::cuLinkAddData,
* ::cuLinkAddFile,
* ::cuLinkDestroy,
* ::cuModuleLoadData
*/
int handle_cuLinkComplete(void *conn) {
    CUlinkState state;
    void* cubinOut;
    size_t sizeOut;

    if (rpc_read(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_read(conn, &cubinOut, sizeof(void*)) < 0 ||
        rpc_read(conn, &sizeOut, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLinkComplete(state, &cubinOut, &sizeOut);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &cubinOut, sizeof(void*)) < 0 ||
        rpc_write(conn, &sizeOut, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys state for a JIT linker invocation.
*
* \param state State object for the linker invocation
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_HANDLE
*
* \sa ::cuLinkCreate
*/
int handle_cuLinkDestroy(void *conn) {
    CUlinkState state;

    if (rpc_read(conn, &state, sizeof(CUlinkState)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLinkDestroy(state);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a handle to a texture reference
*
* \deprecated
*
* Returns in \p *pTexRef the handle of the texture reference of name \p name
* in the module \p hmod. If no texture reference of that name exists,
* ::cuModuleGetTexRef() returns ::CUDA_ERROR_NOT_FOUND. This texture reference
* handle should not be destroyed, since it will be destroyed when the module
* is unloaded.
*
* \param pTexRef  - Returned texture reference
* \param hmod     - Module to retrieve texture reference from
* \param name     - Name of texture reference to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_FOUND
* \notefnerr
*
* \sa
* ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetSurfRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleGetTexRef(void *conn) {
    CUtexref pTexRef;
    CUmodule hmod;
    char name;

    if (rpc_read(conn, &pTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleGetTexRef(&pTexRef, hmod, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexRef, sizeof(CUtexref)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a handle to a surface reference
*
* \deprecated
*
* Returns in \p *pSurfRef the handle of the surface reference of name \p name
* in the module \p hmod. If no surface reference of that name exists,
* ::cuModuleGetSurfRef() returns ::CUDA_ERROR_NOT_FOUND.
*
* \param pSurfRef  - Returned surface reference
* \param hmod     - Module to retrieve surface reference from
* \param name     - Name of surface reference to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_FOUND
* \notefnerr
*
* \sa
* ::cuModuleGetFunction,
* ::cuModuleGetGlobal,
* ::cuModuleGetTexRef,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx,
* ::cuModuleLoadFatBinary,
* ::cuModuleUnload
*/
int handle_cuModuleGetSurfRef(void *conn) {
    CUsurfref pSurfRef;
    CUmodule hmod;
    char name;

    if (rpc_read(conn, &pSurfRef, sizeof(CUsurfref)) < 0 ||
        rpc_read(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuModuleGetSurfRef(&pSurfRef, hmod, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pSurfRef, sizeof(CUsurfref)) < 0)
        return -1;

    return result;
}

/**
* \brief Load a library with specified code and options
*
* Takes a pointer \p code and loads the corresponding library \p library into
* all contexts existent at the time of the call and future contexts at the time
* of creation until the library is unloaded with ::cuLibraryUnload().
*
* The pointer may be obtained by mapping a \e cubin or \e PTX or \e fatbin file,
* passing a \e cubin or \e PTX or \e fatbin file as a NULL-terminated text string, or
* incorporating a \e cubin or \e fatbin object into the executable resources and
* using operating system calls such as Windows \c FindResource() to obtain the pointer.
* Options are passed as an array via \p jitOptions and any corresponding parameters are passed in
* \p jitOptionsValues. The number of total JTT options is supplied via \p numJitOptions.
* Any outputs will be returned via \p jitOptionsValues.
*
* \param library             - Returned library
* \param code                - Code to load
* \param jitOptions          - Options for JIT
* \param jitOptionsValues    - Option values for JIT
* \param numJitOptions       - Number of options
* \param libraryOptions      - Options for loading
* \param libraryOptionValues - Option values for loading
* \param numLibraryOptions   - Number of options for loading
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
*
* \sa ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx
*/
int handle_cuLibraryLoadData(void *conn) {
    CUlibrary library;
    void* code;
    CUjit_option jitOptions;
    void* jitOptionsValues;
    unsigned int numJitOptions;
    CUlibraryOption libraryOptions;
    void* libraryOptionValues;
    unsigned int numLibraryOptions;

    if (rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &code, sizeof(void*)) < 0 ||
        rpc_read(conn, &jitOptions, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &jitOptionsValues, sizeof(void*)) < 0 ||
        rpc_read(conn, &numJitOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &libraryOptions, sizeof(CUlibraryOption)) < 0 ||
        rpc_read(conn, &libraryOptionValues, sizeof(void*)) < 0 ||
        rpc_read(conn, &numLibraryOptions, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryLoadData(&library, &code, &jitOptions, &jitOptionsValues, numJitOptions, &libraryOptions, &libraryOptionValues, numLibraryOptions);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &jitOptions, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &jitOptionsValues, sizeof(void*)) < 0 ||
        rpc_write(conn, &libraryOptions, sizeof(CUlibraryOption)) < 0 ||
        rpc_write(conn, &libraryOptionValues, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Load a library with specified file and options
*
* Takes a filename \p fileName and loads the corresponding library \p library into
* all contexts existent at the time of the call and future contexts at the time of
* creation until the library is unloaded with ::cuLibraryUnload().
*
* The file should be a \e cubin file as output by \b nvcc, or a \e PTX file either
* as output by \b nvcc or handwritten, or a \e fatbin file as output by \b nvcc
* from toolchain 4.0 or later.
*
* Options are passed as an array via \p jitOptions and any corresponding parameters are
* passed in \p jitOptionsValues. The number of total options is supplied via \p numJitOptions.
* Any outputs will be returned via \p jitOptionsValues.
*
* \param library             - Returned library
* \param code                - Code to load
* \param jitOptions          - Options for JIT
* \param jitOptionsValues    - Option values for JIT
* \param numJitOptions       - Number of options
* \param libraryOptions      - Options for loading
* \param libraryOptionValues - Option values for loading
* \param numLibraryOptions   - Number of options for loading
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_PTX,
* ::CUDA_ERROR_UNSUPPORTED_PTX_VERSION,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NO_BINARY_FOR_GPU,
* ::CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_JIT_COMPILER_NOT_FOUND
*
* \sa ::cuLibraryLoadData,
* :: cuLibraryUnload,
* ::cuModuleLoad,
* ::cuModuleLoadData,
* ::cuModuleLoadDataEx
*/
int handle_cuLibraryLoadFromFile(void *conn) {
    CUlibrary library;
    char fileName;
    CUjit_option jitOptions;
    void* jitOptionsValues;
    unsigned int numJitOptions;
    CUlibraryOption libraryOptions;
    void* libraryOptionValues;
    unsigned int numLibraryOptions;

    if (rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &fileName, sizeof(char)) < 0 ||
        rpc_read(conn, &jitOptions, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, &jitOptionsValues, sizeof(void*)) < 0 ||
        rpc_read(conn, &numJitOptions, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &libraryOptions, sizeof(CUlibraryOption)) < 0 ||
        rpc_read(conn, &libraryOptionValues, sizeof(void*)) < 0 ||
        rpc_read(conn, &numLibraryOptions, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryLoadFromFile(&library, &fileName, &jitOptions, &jitOptionsValues, numJitOptions, &libraryOptions, &libraryOptionValues, numLibraryOptions);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &jitOptions, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, &jitOptionsValues, sizeof(void*)) < 0 ||
        rpc_write(conn, &libraryOptions, sizeof(CUlibraryOption)) < 0 ||
        rpc_write(conn, &libraryOptionValues, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Unloads a library
*
* Unloads the library specified with \p library
*
* \param library - Library to unload
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuLibraryLoadData
* ::cuLibraryLoadFromFile,
* ::cuModuleUnload
*/
int handle_cuLibraryUnload(void *conn) {
    CUlibrary library;

    if (rpc_read(conn, &library, sizeof(CUlibrary)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryUnload(library);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a kernel handle
*
* Returns in \p pKernel the handle of the kernel with name \p name located in library \p library.
* If kernel handle is not found, the call returns ::CUDA_ERROR_NOT_FOUND.
*
* \param pKernel - Returned kernel handle
* \param library - Library to retrieve kernel from
* \param name - Name of kernel to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuKernelGetFunction,
* ::cuLibraryGetModule,
* ::cuModuleGetFunction
*/
int handle_cuLibraryGetKernel(void *conn) {
    CUkernel pKernel;
    CUlibrary library;
    char name;

    if (rpc_read(conn, &pKernel, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryGetKernel(&pKernel, library, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pKernel, sizeof(CUkernel)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a module handle
*
* Returns in \p pMod the module handle associated with the current context located in
* library \p library. If module handle is not found, the call returns ::CUDA_ERROR_NOT_FOUND.
*
* \param pMod - Returned module handle
* \param library - Library to retrieve module from
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuModuleGetFunction
*/
int handle_cuLibraryGetModule(void *conn) {
    CUmodule pMod;
    CUlibrary library;

    if (rpc_read(conn, &pMod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &library, sizeof(CUlibrary)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryGetModule(&pMod, library);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pMod, sizeof(CUmodule)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a function handle
*
* Returns in \p pFunc the handle of the function for the requested kernel \p kernel and
* the current context. If function handle is not found, the call returns ::CUDA_ERROR_NOT_FOUND.
*
* \param pFunc - Returned function handle
* \param kernel - Kernel to retrieve function for the requested context
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuLibraryGetKernel,
* ::cuLibraryGetModule,
* ::cuModuleGetFunction
*/
int handle_cuKernelGetFunction(void *conn) {
    CUfunction pFunc;
    CUkernel kernel;

    if (rpc_read(conn, &pFunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &kernel, sizeof(CUkernel)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuKernelGetFunction(&pFunc, kernel);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pFunc, sizeof(CUfunction)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a global device pointer
*
* Returns in \p *dptr and \p *bytes the base pointer and size of the global with
* name \p name for the requested library \p library and the current context.
* If no global for the requested name \p name exists, the call returns ::CUDA_ERROR_NOT_FOUND.
* One of the parameters \p dptr or \p bytes (not both) can be NULL in which
* case it is ignored.
*
* \param dptr - Returned global device pointer for the requested context
* \param bytes - Returned global size in bytes
* \param library - Library to retrieve global from
* \param name - Name of global to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuLibraryGetModule,
* cuModuleGetGlobal
*/
int handle_cuLibraryGetGlobal(void *conn) {
    CUdeviceptr dptr;
    size_t bytes;
    CUlibrary library;
    char name;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryGetGlobal(&dptr, &bytes, library, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a pointer to managed memory
*
* Returns in \p *dptr and \p *bytes the base pointer and size of the managed memory with
* name \p name for the requested library \p library. If no managed memory with the
* requested name \p name exists, the call returns ::CUDA_ERROR_NOT_FOUND. One of the parameters
* \p dptr or \p bytes (not both) can be NULL in which case it is ignored.
* Note that managed memory for library \p library is shared across devices and is registered
* when the library is loaded into atleast one context.
*
* \note The API requires a CUDA context to be present and initialized on at least one device.
* If no context is present, the call returns ::CUDA_ERROR_NOT_FOUND.
*
* \param dptr - Returned pointer to the managed memory
* \param bytes - Returned memory size in bytes
* \param library - Library to retrieve managed memory from
* \param name - Name of managed memory to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
*/
int handle_cuLibraryGetManaged(void *conn) {
    CUdeviceptr dptr;
    size_t bytes;
    CUlibrary library;
    char name;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &name, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryGetManaged(&dptr, &bytes, library, &name);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a pointer to a universal function
*
* Returns in \p *fptr the function pointer to a global function denoted by \p symbol.
* If no universal function with name \p symbol exists, the call returns ::CUDA_ERROR_NOT_FOUND.
* If there is no device with attrubute ::CU_DEVICE_ATTRIBUTE_UNIFIED_FUNCTION_POINTERS present in the system,
* the call may return ::CUDA_ERROR_NOT_FOUND.
*
* \param fptr - Returned pointer to a universal function
* \param library - Library to retrieve function pointer memory from
* \param symbol - Name of function pointer to retrieve
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_FOUND,
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
*/
int handle_cuLibraryGetUnifiedFunction(void *conn) {
    void* fptr;
    CUlibrary library;
    char symbol;

    if (rpc_read(conn, &fptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &symbol, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLibraryGetUnifiedFunction(&fptr, library, &symbol);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &fptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about a kernel
*
* Returns in \p *pi the integer value of the attribute \p attrib for the kernel
* \p kernel for the requested device \p dev. The supported attributes are:
* - ::CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK: The maximum number of threads
*   per block, beyond which a launch of the kernel would fail. This number
*   depends on both the kernel and the requested device.
* - ::CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES: The size in bytes of
*   statically-allocated shared memory per block required by this kernel.
*   This does not include dynamically-allocated shared memory requested by
*   the user at runtime.
* - ::CU_FUNC_ATTRIBUTE_CONST_SIZE_BYTES: The size in bytes of user-allocated
*   constant memory required by this kernel.
* - ::CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES: The size in bytes of local memory
*   used by each thread of this kernel.
* - ::CU_FUNC_ATTRIBUTE_NUM_REGS: The number of registers used by each thread
*   of this kernel.
* - ::CU_FUNC_ATTRIBUTE_PTX_VERSION: The PTX virtual architecture version for
*   which the kernel was compiled. This value is the major PTX version * 10
*   + the minor PTX version, so a PTX version 1.3 function would return the
*   value 13. Note that this may return the undefined value of 0 for cubins
*   compiled prior to CUDA 3.0.
* - ::CU_FUNC_ATTRIBUTE_BINARY_VERSION: The binary architecture version for
*   which the kernel was compiled. This value is the major binary
*   version * 10 + the minor binary version, so a binary version 1.3 function
*   would return the value 13. Note that this will return a value of 10 for
*   legacy cubins that do not have a properly-encoded binary architecture
*   version.
* - ::CU_FUNC_CACHE_MODE_CA: The attribute to indicate whether the kernel has
*   been compiled with user specified option "-Xptxas --dlcm=ca" set.
* - ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES: The maximum size in bytes of
*   dynamically-allocated shared memory.
* - ::CU_FUNC_ATTRIBUTE_PREFERRED_SHARED_MEMORY_CARVEOUT: Preferred shared memory-L1
*   cache split ratio in percent of total shared memory.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SIZE_MUST_BE_SET: If this attribute is set, the
*   kernel must launch with a valid cluster size specified.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_WIDTH: The required cluster width in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_HEIGHT: The required cluster height in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_DEPTH: The required cluster depth in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_NON_PORTABLE_CLUSTER_SIZE_ALLOWED: Indicates whether
*   the function can be launched with non-portable cluster size. 1 is allowed,
*   0 is disallowed. A non-portable cluster size may only function on the
*   specific SKUs the program is tested on. The launch might fail if the
*   program is run on a different hardware platform. CUDA API provides
*   cudaOccupancyMaxActiveClusters to assist with checking whether the desired
*   size can be launched on the current device. A portable cluster size is
*   guaranteed to be functional on all compute capabilities higher than the
*   target compute capability. The portable cluster size for sm_90 is 8 blocks
*   per cluster. This value may increase for future compute capabilities. The
*   specific hardware unit may support higher cluster sizes that’s not
*   guaranteed to be portable.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE: The block
*   scheduling policy of a function. The value type is CUclusterSchedulingPolicy.
*
* \note If another thread is trying to set the same attribute on the same device using
* ::cuKernelSetAttribute() simultaneously, the attribute query will give the old or new
* value depending on the interleavings chosen by the OS scheduler and memory consistency.
*
* \param pi     - Returned attribute value
* \param attrib - Attribute requested
* \param kernel  - Kernel to query attribute of
* \param dev - Device to query attribute of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuKernelSetAttribute,
* ::cuLibraryGetKernel,
* ::cuLaunchKernel,
* ::cuKernelGetFunction,
* ::cuLibraryGetModule,
* ::cuModuleGetFunction,
* ::cuFuncGetAttribute
*/
int handle_cuKernelGetAttribute(void *conn) {
    int pi;
    CUfunction_attribute attrib;
    CUkernel kernel;
    CUdevice dev;

    if (rpc_read(conn, &pi, sizeof(int)) < 0 ||
        rpc_read(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_read(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuKernelGetAttribute(&pi, attrib, kernel, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pi, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets information about a kernel
*
* This call sets the value of a specified attribute \p attrib on the kernel \p kernel
* for the requested device \p dev to an integer value specified by \p val.
* This function returns CUDA_SUCCESS if the new value of the attribute could be
* successfully set. If the set fails, this call will return an error.
* Not all attributes can have values set. Attempting to set a value on a read-only
* attribute will result in an error (CUDA_ERROR_INVALID_VALUE)
*
* Note that attributes set using ::cuFuncSetAttribute() will override the attribute
* set by this API irrespective of whether the call to ::cuFuncSetAttribute() is made
* before or after this API call. However, ::cuKernelGetAttribute() will always
* return the attribute value set by this API.
*
* Supported attributes are:
* - ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES: This is the maximum size in bytes of
*   dynamically-allocated shared memory. The value should contain the requested
*   maximum size of dynamically-allocated shared memory. The sum of this value and
*   the function attribute ::CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES cannot exceed the
*   device attribute ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN.
*   The maximal size of requestable dynamic shared memory may differ by GPU
*   architecture.
* - ::CU_FUNC_ATTRIBUTE_PREFERRED_SHARED_MEMORY_CARVEOUT: On devices where the L1
*   cache and shared memory use the same hardware resources, this sets the shared memory
*   carveout preference, in percent of the total shared memory.
*   See ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR
*   This is only a hint, and the driver can choose a different ratio if required to execute the function.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_WIDTH: The required cluster width in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_HEIGHT: The required cluster height in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_DEPTH: The required cluster depth in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE: The block
*   scheduling policy of a function. The value type is CUclusterSchedulingPolicy.
*
* \note The API has stricter locking requirements in comparison to its legacy counterpart
* ::cuFuncSetAttribute() due to device-wide semantics. If multiple threads are trying to
* set the same attribute on the same device simultaneously, the attribute setting will depend
* on the interleavings chosen by the OS scheduler and memory consistency.
*
* \param attrib - Attribute requested
* \param val - Value to set
* \param kernel  - Kernel to set attribute of
* \param dev - Device to set attribute of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuKernelGetAttribute,
* ::cuLibraryGetKernel,
* ::cuLaunchKernel,
* ::cuKernelGetFunction,
* ::cuLibraryGetModule,
* ::cuModuleGetFunction,
* ::cuFuncSetAttribute
*/
int handle_cuKernelSetAttribute(void *conn) {
    CUfunction_attribute attrib;
    int val;
    CUkernel kernel;
    CUdevice dev;

    if (rpc_read(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_read(conn, &val, sizeof(int)) < 0 ||
        rpc_read(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuKernelSetAttribute(attrib, val, kernel, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for a device kernel.
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p config the preferred cache configuration for
* the device kernel \p kernel on the requested device \p dev. This is only a preference.
* The driver will use the requested configuration if possible, but it is free to choose a different
* configuration if required to execute \p kernel.  Any context-wide preference
* set via ::cuCtxSetCacheConfig() will be overridden by this per-kernel
* setting.
*
* Note that attributes set using ::cuFuncSetCacheConfig() will override the attribute
* set by this API irrespective of whether the call to ::cuFuncSetCacheConfig() is made
* before or after this API call.
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
*
* The supported cache configurations are:
* - ::CU_FUNC_CACHE_PREFER_NONE: no preference for shared memory or L1 (default)
* - ::CU_FUNC_CACHE_PREFER_SHARED: prefer larger shared memory and smaller L1 cache
* - ::CU_FUNC_CACHE_PREFER_L1: prefer larger L1 cache and smaller shared memory
* - ::CU_FUNC_CACHE_PREFER_EQUAL: prefer equal sized L1 cache and shared memory
*
* \note The API has stricter locking requirements in comparison to its legacy counterpart
* ::cuFuncSetCacheConfig() due to device-wide semantics. If multiple threads are trying to
* set a config on the same device simultaneously, the cache config setting will depend
* on the interleavings chosen by the OS scheduler and memory consistency.
*
* \param kernel  - Kernel to configure cache for
* \param config - Requested cache configuration
* \param dev - Device to set attribute of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuLibraryLoadData,
* ::cuLibraryLoadFromFile,
* ::cuLibraryUnload,
* ::cuLibraryGetKernel,
* ::cuKernelGetFunction,
* ::cuLibraryGetModule,
* ::cuModuleGetFunction,
* ::cuFuncSetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuLaunchKernel
*/
int handle_cuKernelSetCacheConfig(void *conn) {
    CUkernel kernel;
    CUfunc_cache config;
    CUdevice dev;

    if (rpc_read(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &config, sizeof(CUfunc_cache)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuKernelSetCacheConfig(kernel, config, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets free and total memory
*
* Returns in \p *total the total amount of memory available to the the current context.
* Returns in \p *free the amount of memory on the device that is free according to the OS.
* CUDA is not guaranteed to be able to allocate all of the memory that the OS reports as free.
* In a multi-tenet situation, free estimate returned is prone to race condition where
* a new allocation/free done by a different process or a different thread in the same
* process between the time when free memory was estimated and reported, will result in
* deviation in free value reported and actual free memory.
*
* The integrated GPU on Tegra shares memory with CPU and other component
* of the SoC. The free and total values returned by the API excludes
* the SWAP memory space maintained by the OS on some platforms.
* The OS may move some of the memory pages into swap area as the GPU or
* CPU allocate or access memory. See Tegra app note on how to calculate
* total and free memory on Tegra.
*
* \param free  - Returned free memory in bytes
* \param total - Returned total memory in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemGetInfo
*/
int handle_cuMemGetInfo_v2(void *conn) {
    size_t free;
    size_t total;

    if (rpc_read(conn, &free, sizeof(size_t)) < 0 ||
        rpc_read(conn, &total, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetInfo_v2(&free, &total);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &free, sizeof(size_t)) < 0 ||
        rpc_write(conn, &total, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates device memory
*
* Allocates \p bytesize bytes of linear memory on the device and returns in
* \p *dptr a pointer to the allocated memory. The allocated memory is suitably
* aligned for any kind of variable. The memory is not cleared. If \p bytesize
* is 0, ::cuMemAlloc() returns ::CUDA_ERROR_INVALID_VALUE.
*
* \param dptr     - Returned device pointer
* \param bytesize - Requested allocation size in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMalloc
*/
int handle_cuMemAlloc_v2(void *conn) {
    CUdeviceptr dptr;
    size_t bytesize;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAlloc_v2(&dptr, bytesize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates pitched device memory
*
* Allocates at least \p WidthInBytes * \p Height bytes of linear memory on
* the device and returns in \p *dptr a pointer to the allocated memory. The
* function may pad the allocation to ensure that corresponding pointers in
* any given row will continue to meet the alignment requirements for
* coalescing as the address is updated from row to row. \p ElementSizeBytes
* specifies the size of the largest reads and writes that will be performed
* on the memory range. \p ElementSizeBytes may be 4, 8 or 16 (since coalesced
* memory transactions are not possible on other data sizes). If
* \p ElementSizeBytes is smaller than the actual read/write size of a kernel,
* the kernel will run correctly, but possibly at reduced speed. The pitch
* returned in \p *pPitch by ::cuMemAllocPitch() is the width in bytes of the
* allocation. The intended usage of pitch is as a separate parameter of the
* allocation, used to compute addresses within the 2D array. Given the row
* and column of an array element of type \b T, the address is computed as:
* \code
   T* pElement = (T*)((char*)BaseAddress + Row * Pitch) + Column;
* \endcode
*
* The pitch returned by ::cuMemAllocPitch() is guaranteed to work with
* ::cuMemcpy2D() under all circumstances. For allocations of 2D arrays, it is
* recommended that programmers consider performing pitch allocations using
* ::cuMemAllocPitch(). Due to alignment restrictions in the hardware, this is
* especially true if the application will be performing 2D memory copies
* between different regions of device memory (whether linear memory or CUDA
* arrays).
*
* The byte alignment of the pitch returned by ::cuMemAllocPitch() is guaranteed
* to match or exceed the alignment requirement for texture binding with
* ::cuTexRefSetAddress2D().
*
* \param dptr             - Returned device pointer
* \param pPitch           - Returned pitch of allocation in bytes
* \param WidthInBytes     - Requested allocation width in bytes
* \param Height           - Requested allocation height in rows
* \param ElementSizeBytes - Size of largest reads/writes for range
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMallocPitch
*/
int handle_cuMemAllocPitch_v2(void *conn) {
    CUdeviceptr dptr;
    size_t pPitch;
    size_t WidthInBytes;
    size_t Height;
    unsigned int ElementSizeBytes;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &pPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &WidthInBytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ElementSizeBytes, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAllocPitch_v2(&dptr, &pPitch, WidthInBytes, Height, ElementSizeBytes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &pPitch, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees device memory
*
* Frees the memory space pointed to by \p dptr, which must have been returned
* by a previous call to one of the following memory allocation APIs - ::cuMemAlloc(), 
* ::cuMemAllocPitch(), ::cuMemAllocManaged(), ::cuMemAllocAsync(), ::cuMemAllocFromPoolAsync()
*
* Note - This API will not perform any implict synchronization when the pointer was allocated with
* ::cuMemAllocAsync or ::cuMemAllocFromPoolAsync. Callers must ensure that all accesses to the
* pointer have completed before invoking ::cuMemFree. For best performance and memory reuse, users
* should use ::cuMemFreeAsync to free memory allocated via the stream ordered memory allocator.
* 
* \param dptr - Pointer to memory to free
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemAllocManaged, ::cuMemAllocAsync, ::cuMemAllocFromPoolAsync, 
* ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned, ::cuMemcpy3D, ::cuMemcpy3DAsync,
* ::cuMemcpyAtoA, ::cuMemcpyAtoD, ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA,
* ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync, ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA,
* ::cuMemcpyHtoAAsync, ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc, ::cuMemFreeAsync,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaFree
*/
int handle_cuMemFree_v2(void *conn) {
    CUdeviceptr dptr;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemFree_v2(dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Get information on memory allocations
*
* Returns the base address in \p *pbase and size in \p *psize of the
* allocation by ::cuMemAlloc() or ::cuMemAllocPitch() that contains the input
* pointer \p dptr. Both parameters \p pbase and \p psize are optional. If one
* of them is NULL, it is ignored.
*
* \param pbase - Returned base address
* \param psize - Returned size of device memory allocation
* \param dptr  - Device pointer to query
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_NOT_FOUND,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32
*/
int handle_cuMemGetAddressRange_v2(void *conn) {
    CUdeviceptr pbase;
    size_t psize;
    CUdeviceptr dptr;

    if (rpc_read(conn, &pbase, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &psize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetAddressRange_v2(&pbase, &psize, dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pbase, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &psize, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates page-locked host memory
*
* Allocates \p bytesize bytes of host memory that is page-locked and
* accessible to the device. The driver tracks the virtual memory ranges
* allocated with this function and automatically accelerates calls to
* functions such as ::cuMemcpy(). Since the memory can be accessed directly by
* the device, it can be read or written with much higher bandwidth than
* pageable memory obtained with functions such as ::malloc(). Allocating
* excessive amounts of memory with ::cuMemAllocHost() may degrade system
* performance, since it reduces the amount of memory available to the system
* for paging. As a result, this function is best used sparingly to allocate
* staging areas for data exchange between host and device.
*
* Note all host memory allocated using ::cuMemHostAlloc() will automatically
* be immediately accessible to all contexts on all devices which support unified
* addressing (as may be queried using ::CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING).
* The device pointer that may be used to access this host memory from those
* contexts is always equal to the returned host pointer \p *pp.
* See \ref CUDA_UNIFIED for additional details.
*
* \param pp       - Returned host pointer to page-locked memory
* \param bytesize - Requested allocation size in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMallocHost
*/
int handle_cuMemAllocHost_v2(void *conn) {
    void* pp;
    size_t bytesize;

    if (rpc_read(conn, &pp, sizeof(void*)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAllocHost_v2(&pp, bytesize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pp, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees page-locked host memory
*
* Frees the memory space pointed to by \p p, which must have been returned by
* a previous call to ::cuMemAllocHost().
*
* \param p - Pointer to memory to free
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaFreeHost
*/
int handle_cuMemFreeHost(void *conn) {
    void* p;

    if (rpc_read(conn, &p, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemFreeHost(&p);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &p, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates page-locked host memory
*
* Allocates \p bytesize bytes of host memory that is page-locked and accessible
* to the device. The driver tracks the virtual memory ranges allocated with
* this function and automatically accelerates calls to functions such as
* ::cuMemcpyHtoD(). Since the memory can be accessed directly by the device,
* it can be read or written with much higher bandwidth than pageable memory
* obtained with functions such as ::malloc(). Allocating excessive amounts of
* pinned memory may degrade system performance, since it reduces the amount
* of memory available to the system for paging. As a result, this function is
* best used sparingly to allocate staging areas for data exchange between
* host and device.
*
* The \p Flags parameter enables different options to be specified that
* affect the allocation, as follows.
*
* - ::CU_MEMHOSTALLOC_PORTABLE: The memory returned by this call will be
*   considered as pinned memory by all CUDA contexts, not just the one that
*   performed the allocation.
*
* - ::CU_MEMHOSTALLOC_DEVICEMAP: Maps the allocation into the CUDA address
*   space. The device pointer to the memory may be obtained by calling
*   ::cuMemHostGetDevicePointer().
*
* - ::CU_MEMHOSTALLOC_WRITECOMBINED: Allocates the memory as write-combined
*   (WC). WC memory can be transferred across the PCI Express bus more
*   quickly on some system configurations, but cannot be read efficiently by
*   most CPUs. WC memory is a good option for buffers that will be written by
*   the CPU and read by the GPU via mapped pinned memory or host->device
*   transfers.
*
* All of these flags are orthogonal to one another: a developer may allocate
* memory that is portable, mapped and/or write-combined with no restrictions.
*
* The ::CU_MEMHOSTALLOC_DEVICEMAP flag may be specified on CUDA contexts for
* devices that do not support mapped pinned memory. The failure is deferred
* to ::cuMemHostGetDevicePointer() because the memory may be mapped into
* other CUDA contexts via the ::CU_MEMHOSTALLOC_PORTABLE flag.
*
* The memory allocated by this function must be freed with ::cuMemFreeHost().
*
* Note all host memory allocated using ::cuMemHostAlloc() will automatically
* be immediately accessible to all contexts on all devices which support unified
* addressing (as may be queried using ::CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING).
* Unless the flag ::CU_MEMHOSTALLOC_WRITECOMBINED is specified, the device pointer
* that may be used to access this host memory from those contexts is always equal
* to the returned host pointer \p *pp.  If the flag ::CU_MEMHOSTALLOC_WRITECOMBINED
* is specified, then the function ::cuMemHostGetDevicePointer() must be used
* to query the device pointer, even if the context supports unified addressing.
* See \ref CUDA_UNIFIED for additional details.
*
* \param pp       - Returned host pointer to page-locked memory
* \param bytesize - Requested allocation size in bytes
* \param Flags    - Flags for allocation request
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaHostAlloc
*/
int handle_cuMemHostAlloc(void *conn) {
    void* pp;
    size_t bytesize;
    unsigned int Flags;

    if (rpc_read(conn, &pp, sizeof(void*)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemHostAlloc(&pp, bytesize, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pp, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Passes back device pointer of mapped pinned memory
*
* Passes back the device pointer \p pdptr corresponding to the mapped, pinned
* host buffer \p p allocated by ::cuMemHostAlloc.
*
* ::cuMemHostGetDevicePointer() will fail if the ::CU_MEMHOSTALLOC_DEVICEMAP
* flag was not specified at the time the memory was allocated, or if the
* function is called on a GPU that does not support mapped pinned memory.
*
* For devices that have a non-zero value for the device attribute
* ::CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM, the memory
* can also be accessed from the device using the host pointer \p p.
* The device pointer returned by ::cuMemHostGetDevicePointer() may or may not
* match the original host pointer \p p and depends on the devices visible to the
* application. If all devices visible to the application have a non-zero value for the
* device attribute, the device pointer returned by ::cuMemHostGetDevicePointer()
* will match the original pointer \p p. If any device visible to the application
* has a zero value for the device attribute, the device pointer returned by
* ::cuMemHostGetDevicePointer() will not match the original host pointer \p p,
* but it will be suitable for use on all devices provided Unified Virtual Addressing
* is enabled. In such systems, it is valid to access the memory using either pointer
* on devices that have a non-zero value for the device attribute. Note however that
* such devices should access the memory using only one of the two pointers and not both.
*
* \p Flags provides for future releases. For now, it must be set to 0.
*
* \param pdptr - Returned device pointer
* \param p     - Host pointer
* \param Flags - Options (must be 0)
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaHostGetDevicePointer
*/
int handle_cuMemHostGetDevicePointer_v2(void *conn) {
    CUdeviceptr pdptr;
    void* p;
    unsigned int Flags;

    if (rpc_read(conn, &pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &p, sizeof(void*)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemHostGetDevicePointer_v2(&pdptr, &p, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &p, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Passes back flags that were used for a pinned allocation
*
* Passes back the flags \p pFlags that were specified when allocating
* the pinned host buffer \p p allocated by ::cuMemHostAlloc.
*
* ::cuMemHostGetFlags() will fail if the pointer does not reside in
* an allocation performed by ::cuMemAllocHost() or ::cuMemHostAlloc().
*
* \param pFlags - Returned flags word
* \param p     - Host pointer
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuMemAllocHost,
* ::cuMemHostAlloc,
* ::cudaHostGetFlags
*/
int handle_cuMemHostGetFlags(void *conn) {
    unsigned int pFlags;
    void* p;

    if (rpc_read(conn, &pFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &p, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemHostGetFlags(&pFlags, &p);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &p, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates memory that will be automatically managed by the Unified Memory system
*
* Allocates \p bytesize bytes of managed memory on the device and returns in
* \p *dptr a pointer to the allocated memory. If the device doesn't support
* allocating managed memory, ::CUDA_ERROR_NOT_SUPPORTED is returned. Support
* for managed memory can be queried using the device attribute
* ::CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY. The allocated memory is suitably
* aligned for any kind of variable. The memory is not cleared. If \p bytesize
* is 0, ::cuMemAllocManaged returns ::CUDA_ERROR_INVALID_VALUE. The pointer
* is valid on the CPU and on all GPUs in the system that support managed memory.
* All accesses to this pointer must obey the Unified Memory programming model.
*
* \p flags specifies the default stream association for this allocation.
* \p flags must be one of ::CU_MEM_ATTACH_GLOBAL or ::CU_MEM_ATTACH_HOST. If
* ::CU_MEM_ATTACH_GLOBAL is specified, then this memory is accessible from
* any stream on any device. If ::CU_MEM_ATTACH_HOST is specified, then the
* allocation should not be accessed from devices that have a zero value for the
* device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS; an explicit call to
* ::cuStreamAttachMemAsync will be required to enable access on such devices.
*
* If the association is later changed via ::cuStreamAttachMemAsync to
* a single stream, the default association as specified during ::cuMemAllocManaged
* is restored when that stream is destroyed. For __managed__ variables, the
* default association is always ::CU_MEM_ATTACH_GLOBAL. Note that destroying a
* stream is an asynchronous operation, and as a result, the change to default
* association won't happen until all work in the stream has completed.
*
* Memory allocated with ::cuMemAllocManaged should be released with ::cuMemFree.
*
* Device memory oversubscription is possible for GPUs that have a non-zero value for the
* device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS. Managed memory on
* such GPUs may be evicted from device memory to host memory at any time by the Unified
* Memory driver in order to make room for other allocations.
*
* In a multi-GPU system where all GPUs have a non-zero value for the device attribute
* ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS, managed memory may not be populated when this
* API returns and instead may be populated on access. In such systems, managed memory can
* migrate to any processor's memory at any time. The Unified Memory driver will employ heuristics to
* maintain data locality and prevent excessive page faults to the extent possible. The application
* can also guide the driver about memory usage patterns via ::cuMemAdvise. The application
* can also explicitly migrate memory to a desired processor's memory via
* ::cuMemPrefetchAsync.
*
* In a multi-GPU system where all of the GPUs have a zero value for the device attribute
* ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS and all the GPUs have peer-to-peer support
* with each other, the physical storage for managed memory is created on the GPU which is active
* at the time ::cuMemAllocManaged is called. All other GPUs will reference the data at reduced
* bandwidth via peer mappings over the PCIe bus. The Unified Memory driver does not migrate
* memory among such GPUs.
*
* In a multi-GPU system where not all GPUs have peer-to-peer support with each other and
* where the value of the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS
* is zero for at least one of those GPUs, the location chosen for physical storage of managed
* memory is system-dependent.
* - On Linux, the location chosen will be device memory as long as the current set of active
* contexts are on devices that either have peer-to-peer support with each other or have a
* non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS.
* If there is an active context on a GPU that does not have a non-zero value for that device
* attribute and it does not have peer-to-peer support with the other devices that have active
* contexts on them, then the location for physical storage will be 'zero-copy' or host memory.
* Note that this means that managed memory that is located in device memory is migrated to
* host memory if a new context is created on a GPU that doesn't have a non-zero value for
* the device attribute and does not support peer-to-peer with at least one of the other devices
* that has an active context. This in turn implies that context creation may fail if there is
* insufficient host memory to migrate all managed allocations.
* - On Windows, the physical storage is always created in 'zero-copy' or host memory.
* All GPUs will reference the data at reduced bandwidth over the PCIe bus. In these
* circumstances, use of the environment variable CUDA_VISIBLE_DEVICES is recommended to
* restrict CUDA to only use those GPUs that have peer-to-peer support.
* Alternatively, users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a
* non-zero value to force the driver to always use device memory for physical storage.
* When this environment variable is set to a non-zero value, all contexts created in
* that process on devices that support managed memory have to be peer-to-peer compatible
* with each other. Context creation will fail if a context is created on a device that
* supports managed memory and is not peer-to-peer compatible with any of the other
* managed memory supporting devices on which contexts were previously created, even if
* those contexts have been destroyed. These environment variables are described
* in the CUDA programming guide under the "CUDA environment variables" section.
* - On ARM, managed memory is not available on discrete gpu with Drive PX-2.
*
* \param dptr     - Returned device pointer
* \param bytesize - Requested allocation size in bytes
* \param flags    - Must be one of ::CU_MEM_ATTACH_GLOBAL or ::CU_MEM_ATTACH_HOST
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cuDeviceGetAttribute, ::cuStreamAttachMemAsync,
* ::cudaMallocManaged
*/
int handle_cuMemAllocManaged(void *conn) {
    CUdeviceptr dptr;
    size_t bytesize;
    unsigned int flags;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAllocManaged(&dptr, bytesize, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a handle to a compute device
*
* Returns in \p *device a device handle given a PCI bus ID string.
*
* \param dev      - Returned device handle
*
* \param pciBusId - String in one of the following forms:
* [domain]:[bus]:[device].[function]
* [domain]:[bus]:[device]
* [bus]:[device].[function]
* where \p domain, \p bus, \p device, and \p function are all hexadecimal values
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGet,
* ::cuDeviceGetAttribute,
* ::cuDeviceGetPCIBusId,
* ::cudaDeviceGetByPCIBusId
*/
int handle_cuDeviceGetByPCIBusId(void *conn) {
    CUdevice dev;
    char pciBusId;

    if (rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &pciBusId, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetByPCIBusId(&dev, &pciBusId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a PCI Bus Id string for the device
*
* Returns an ASCII string identifying the device \p dev in the NULL-terminated
* string pointed to by \p pciBusId. \p len specifies the maximum length of the
* string that may be returned.
*
* \param pciBusId - Returned identifier string for the device in the following format
* [domain]:[bus]:[device].[function]
* where \p domain, \p bus, \p device, and \p function are all hexadecimal values.
* pciBusId should be large enough to store 13 characters including the NULL-terminator.
*
* \param len      - Maximum length of string to store in \p name
*
* \param dev      - Device to get identifier string for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuDeviceGet,
* ::cuDeviceGetAttribute,
* ::cuDeviceGetByPCIBusId,
* ::cudaDeviceGetPCIBusId
*/
int handle_cuDeviceGetPCIBusId(void *conn) {
    char pciBusId;
    int len;
    CUdevice dev;

    if (rpc_read(conn, &pciBusId, sizeof(char)) < 0 ||
        rpc_read(conn, &len, sizeof(int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetPCIBusId(&pciBusId, len, dev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciBusId, sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets an interprocess handle for a previously allocated event
*
* Takes as input a previously allocated event. This event must have been
* created with the ::CU_EVENT_INTERPROCESS and ::CU_EVENT_DISABLE_TIMING
* flags set. This opaque handle may be copied into other processes and
* opened with ::cuIpcOpenEventHandle to allow efficient hardware
* synchronization between GPU work in different processes.
*
* After the event has been opened in the importing process,
* ::cuEventRecord, ::cuEventSynchronize, ::cuStreamWaitEvent and
* ::cuEventQuery may be used in either process. Performing operations
* on the imported event after the exported event has been freed
* with ::cuEventDestroy will result in undefined behavior.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode
* Users can test their device for IPC functionality by calling
* ::cuapiDeviceGetAttribute with ::CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED
*
* \param pHandle - Pointer to a user allocated CUipcEventHandle
*                    in which to return the opaque event handle
* \param event   - Event allocated with ::CU_EVENT_INTERPROCESS and
*                    ::CU_EVENT_DISABLE_TIMING flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_MAP_FAILED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuEventCreate,
* ::cuEventDestroy,
* ::cuEventSynchronize,
* ::cuEventQuery,
* ::cuStreamWaitEvent,
* ::cuIpcOpenEventHandle,
* ::cuIpcGetMemHandle,
* ::cuIpcOpenMemHandle,
* ::cuIpcCloseMemHandle,
* ::cudaIpcGetEventHandle
*/
int handle_cuIpcGetEventHandle(void *conn) {
    CUipcEventHandle pHandle;
    CUevent event;

    if (rpc_read(conn, &pHandle, sizeof(CUipcEventHandle)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuIpcGetEventHandle(&pHandle, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHandle, sizeof(CUipcEventHandle)) < 0)
        return -1;

    return result;
}

/**
* \brief Opens an interprocess event handle for use in the current process
*
* Opens an interprocess event handle exported from another process with
* ::cuIpcGetEventHandle. This function returns a ::CUevent that behaves like
* a locally created event with the ::CU_EVENT_DISABLE_TIMING flag specified.
* This event must be freed with ::cuEventDestroy.
*
* Performing operations on the imported event after the exported event has
* been freed with ::cuEventDestroy will result in undefined behavior.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode
* Users can test their device for IPC functionality by calling
* ::cuapiDeviceGetAttribute with ::CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED
*
* \param phEvent - Returns the imported event
* \param handle  - Interprocess handle to open
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_MAP_FAILED,
* ::CUDA_ERROR_PEER_ACCESS_UNSUPPORTED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuEventCreate,
* ::cuEventDestroy,
* ::cuEventSynchronize,
* ::cuEventQuery,
* ::cuStreamWaitEvent,
* ::cuIpcGetEventHandle,
* ::cuIpcGetMemHandle,
* ::cuIpcOpenMemHandle,
* ::cuIpcCloseMemHandle,
* ::cudaIpcOpenEventHandle
*/
int handle_cuIpcOpenEventHandle(void *conn) {
    CUevent phEvent;
    CUipcEventHandle handle;

    if (rpc_read(conn, &phEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &handle, sizeof(CUipcEventHandle)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuIpcOpenEventHandle(&phEvent, handle);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phEvent, sizeof(CUevent)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets an interprocess memory handle for an existing device memory
* allocation
*
* Takes a pointer to the base of an existing device memory allocation created
* with ::cuMemAlloc and exports it for use in another process. This is a
* lightweight operation and may be called multiple times on an allocation
* without adverse effects.
*
* If a region of memory is freed with ::cuMemFree and a subsequent call
* to ::cuMemAlloc returns memory with the same device address,
* ::cuIpcGetMemHandle will return a unique handle for the
* new memory.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode
* Users can test their device for IPC functionality by calling
* ::cuapiDeviceGetAttribute with ::CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED
*
* \param pHandle - Pointer to user allocated ::CUipcMemHandle to return
*                    the handle in.
* \param dptr    - Base pointer to previously allocated device memory
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_MAP_FAILED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuMemAlloc,
* ::cuMemFree,
* ::cuIpcGetEventHandle,
* ::cuIpcOpenEventHandle,
* ::cuIpcOpenMemHandle,
* ::cuIpcCloseMemHandle,
* ::cudaIpcGetMemHandle
*/
int handle_cuIpcGetMemHandle(void *conn) {
    CUipcMemHandle pHandle;
    CUdeviceptr dptr;

    if (rpc_read(conn, &pHandle, sizeof(CUipcMemHandle)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuIpcGetMemHandle(&pHandle, dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHandle, sizeof(CUipcMemHandle)) < 0)
        return -1;

    return result;
}

/**
* \brief Opens an interprocess memory handle exported from another process
* and returns a device pointer usable in the local process.
*
* Maps memory exported from another process with ::cuIpcGetMemHandle into
* the current device address space. For contexts on different devices
* ::cuIpcOpenMemHandle can attempt to enable peer access between the
* devices as if the user called ::cuCtxEnablePeerAccess. This behavior is
* controlled by the ::CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS flag.
* ::cuDeviceCanAccessPeer can determine if a mapping is possible.
*
* Contexts that may open ::CUipcMemHandles are restricted in the following way.
* ::CUipcMemHandles from each ::CUdevice in a given process may only be opened
* by one ::CUcontext per ::CUdevice per other process.
*
* If the memory handle has already been opened by the current context, the
* reference count on the handle is incremented by 1 and the existing device pointer
* is returned.
*
* Memory returned from ::cuIpcOpenMemHandle must be freed with
* ::cuIpcCloseMemHandle.
*
* Calling ::cuMemFree on an exported memory region before calling
* ::cuIpcCloseMemHandle in the importing context will result in undefined
* behavior.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode
* Users can test their device for IPC functionality by calling
* ::cuapiDeviceGetAttribute with ::CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED
*
* \param pdptr  - Returned device pointer
* \param handle - ::CUipcMemHandle to open
* \param Flags  - Flags for this operation. Must be specified as ::CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_MAP_FAILED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_TOO_MANY_PEERS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \note No guarantees are made about the address returned in \p *pdptr.
* In particular, multiple processes may not receive the same address for the same \p handle.
*
* \sa
* ::cuMemAlloc,
* ::cuMemFree,
* ::cuIpcGetEventHandle,
* ::cuIpcOpenEventHandle,
* ::cuIpcGetMemHandle,
* ::cuIpcCloseMemHandle,
* ::cuCtxEnablePeerAccess,
* ::cuDeviceCanAccessPeer,
* ::cudaIpcOpenMemHandle
*/
int handle_cuIpcOpenMemHandle_v2(void *conn) {
    CUdeviceptr pdptr;
    CUipcMemHandle handle;
    unsigned int Flags;

    if (rpc_read(conn, &pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &handle, sizeof(CUipcMemHandle)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuIpcOpenMemHandle_v2(&pdptr, handle, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pdptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Attempts to close memory mapped with ::cuIpcOpenMemHandle
*
* Decrements the reference count of the memory returned by ::cuIpcOpenMemHandle by 1.
* When the reference count reaches 0, this API unmaps the memory. The original allocation
* in the exporting process as well as imported mappings in other processes
* will be unaffected.
*
* Any resources used to enable peer access will be freed if this is the
* last mapping using them.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode
* Users can test their device for IPC functionality by calling
* ::cuapiDeviceGetAttribute with ::CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED
*
* \param dptr - Device pointer returned by ::cuIpcOpenMemHandle
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_MAP_FAILED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \sa
* ::cuMemAlloc,
* ::cuMemFree,
* ::cuIpcGetEventHandle,
* ::cuIpcOpenEventHandle,
* ::cuIpcGetMemHandle,
* ::cuIpcOpenMemHandle,
* ::cudaIpcCloseMemHandle
*/
int handle_cuIpcCloseMemHandle(void *conn) {
    CUdeviceptr dptr;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuIpcCloseMemHandle(dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Registers an existing host memory range for use by CUDA
*
* Page-locks the memory range specified by \p p and \p bytesize and maps it
* for the device(s) as specified by \p Flags. This memory range also is added
* to the same tracking mechanism as ::cuMemHostAlloc to automatically accelerate
* calls to functions such as ::cuMemcpyHtoD(). Since the memory can be accessed
* directly by the device, it can be read or written with much higher bandwidth
* than pageable memory that has not been registered.  Page-locking excessive
* amounts of memory may degrade system performance, since it reduces the amount
* of memory available to the system for paging. As a result, this function is
* best used sparingly to register staging areas for data exchange between
* host and device.
*
* The \p Flags parameter enables different options to be specified that
* affect the allocation, as follows.
*
* - ::CU_MEMHOSTREGISTER_PORTABLE: The memory returned by this call will be
*   considered as pinned memory by all CUDA contexts, not just the one that
*   performed the allocation.
*
* - ::CU_MEMHOSTREGISTER_DEVICEMAP: Maps the allocation into the CUDA address
*   space. The device pointer to the memory may be obtained by calling
*   ::cuMemHostGetDevicePointer().
*
* - ::CU_MEMHOSTREGISTER_IOMEMORY: The pointer is treated as pointing to some
*   I/O memory space, e.g. the PCI Express resource of a 3rd party device.
*
* - ::CU_MEMHOSTREGISTER_READ_ONLY: The pointer is treated as pointing to memory
*   that is considered read-only by the device.  On platforms without
*   ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES, this flag is
*   required in order to register memory mapped to the CPU as read-only.  Support
*   for the use of this flag can be queried from the device attribute
*   ::CU_DEVICE_ATTRIBUTE_READ_ONLY_HOST_REGISTER_SUPPORTED.  Using this flag with
*   a current context associated with a device that does not have this attribute
*   set will cause ::cuMemHostRegister to error with CUDA_ERROR_NOT_SUPPORTED.
*
* All of these flags are orthogonal to one another: a developer may page-lock
* memory that is portable or mapped with no restrictions.
*
* The ::CU_MEMHOSTREGISTER_DEVICEMAP flag may be specified on CUDA contexts for
* devices that do not support mapped pinned memory. The failure is deferred
* to ::cuMemHostGetDevicePointer() because the memory may be mapped into
* other CUDA contexts via the ::CU_MEMHOSTREGISTER_PORTABLE flag.
*
* For devices that have a non-zero value for the device attribute
* ::CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM, the memory
* can also be accessed from the device using the host pointer \p p.
* The device pointer returned by ::cuMemHostGetDevicePointer() may or may not
* match the original host pointer \p ptr and depends on the devices visible to the
* application. If all devices visible to the application have a non-zero value for the
* device attribute, the device pointer returned by ::cuMemHostGetDevicePointer()
* will match the original pointer \p ptr. If any device visible to the application
* has a zero value for the device attribute, the device pointer returned by
* ::cuMemHostGetDevicePointer() will not match the original host pointer \p ptr,
* but it will be suitable for use on all devices provided Unified Virtual Addressing
* is enabled. In such systems, it is valid to access the memory using either pointer
* on devices that have a non-zero value for the device attribute. Note however that
* such devices should access the memory using only of the two pointers and not both.
*
* The memory page-locked by this function must be unregistered with
* ::cuMemHostUnregister().
*
* \param p        - Host pointer to memory to page-lock
* \param bytesize - Size in bytes of the address range to page-lock
* \param Flags    - Flags for allocation request
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa
* ::cuMemHostUnregister,
* ::cuMemHostGetFlags,
* ::cuMemHostGetDevicePointer,
* ::cudaHostRegister
*/
int handle_cuMemHostRegister_v2(void *conn) {
    void* p;
    size_t bytesize;
    unsigned int Flags;

    if (rpc_read(conn, &p, sizeof(void*)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemHostRegister_v2(&p, bytesize, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &p, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Unregisters a memory range that was registered with cuMemHostRegister.
*
* Unmaps the memory range whose base address is specified by \p p, and makes
* it pageable again.
*
* The base address must be the same one specified to ::cuMemHostRegister().
*
* \param p - Host pointer to memory to unregister
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED,
* \notefnerr
*
* \sa
* ::cuMemHostRegister,
* ::cudaHostUnregister
*/
int handle_cuMemHostUnregister(void *conn) {
    void* p;

    if (rpc_read(conn, &p, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemHostUnregister(&p);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &p, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory
*
* Copies data between two pointers.
* \p dst and \p src are base pointers of the destination and source, respectively.
* \p ByteCount specifies the number of bytes to copy.
* Note that this function infers the type of the transfer (host to host, host to
*   device, device to device, or device to host) from the pointer values.  This
*   function is only allowed in contexts which support unified addressing.
*
* \param dst - Destination unified virtual address space pointer
* \param src - Source unified virtual address space pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy,
* ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol
*/
int handle_cuMemcpy(void *conn) {
    CUdeviceptr dst;
    CUdeviceptr src;
    size_t ByteCount;

    if (rpc_read(conn, &dst, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &src, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy(dst, src, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies device memory between two contexts
*
* Copies from device memory in one context to device memory in another
* context. \p dstDevice is the base device pointer of the destination memory
* and \p dstContext is the destination context.  \p srcDevice is the base
* device pointer of the source memory and \p srcContext is the source pointer.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstDevice  - Destination device pointer
* \param dstContext - Destination context
* \param srcDevice  - Source device pointer
* \param srcContext - Source context
* \param ByteCount  - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuMemcpyDtoD, ::cuMemcpy3DPeer, ::cuMemcpyDtoDAsync, ::cuMemcpyPeerAsync,
* ::cuMemcpy3DPeerAsync,
* ::cudaMemcpyPeer
*/
int handle_cuMemcpyPeer(void *conn) {
    CUdeviceptr dstDevice;
    CUcontext dstContext;
    CUdeviceptr srcDevice;
    CUcontext srcContext;
    size_t ByteCount;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyPeer(dstDevice, dstContext, srcDevice, srcContext, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Host to Device
*
* Copies from host memory to device memory. \p dstDevice and \p srcHost are
* the base addresses of the destination and source, respectively. \p ByteCount
* specifies the number of bytes to copy.
*
* \param dstDevice - Destination device pointer
* \param srcHost   - Source host pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy,
* ::cudaMemcpyToSymbol
*/
int handle_cuMemcpyHtoD_v2(void *conn) {
    CUdeviceptr dstDevice;
    void* srcHost;
    size_t ByteCount;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyHtoD_v2(dstDevice, &srcHost, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Device to Host
*
* Copies from device to host memory. \p dstHost and \p srcDevice specify the
* base pointers of the destination and source, respectively. \p ByteCount
* specifies the number of bytes to copy.
*
* \param dstHost   - Destination host pointer
* \param srcDevice - Source device pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy,
* ::cudaMemcpyFromSymbol
*/
int handle_cuMemcpyDtoH_v2(void *conn) {
    void* dstHost;
    CUdeviceptr srcDevice;
    size_t ByteCount;

    if (rpc_read(conn, &dstHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyDtoH_v2(&dstHost, srcDevice, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dstHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Device to Device
*
* Copies from device memory to device memory. \p dstDevice and \p srcDevice
* are the base pointers of the destination and source, respectively.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstDevice - Destination device pointer
* \param srcDevice - Source device pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy,
* ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol
*/
int handle_cuMemcpyDtoD_v2(void *conn) {
    CUdeviceptr dstDevice;
    CUdeviceptr srcDevice;
    size_t ByteCount;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyDtoD_v2(dstDevice, srcDevice, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Device to Array
*
* Copies from device memory to a 1D CUDA array. \p dstArray and \p dstOffset
* specify the CUDA array handle and starting index of the destination data.
* \p srcDevice specifies the base pointer of the source. \p ByteCount
* specifies the number of bytes to copy.
*
* \param dstArray  - Destination array
* \param dstOffset - Offset in bytes of destination array
* \param srcDevice - Source device pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpyToArray
*/
int handle_cuMemcpyDtoA_v2(void *conn) {
    CUarray dstArray;
    size_t dstOffset;
    CUdeviceptr srcDevice;
    size_t ByteCount;

    if (rpc_read(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyDtoA_v2(dstArray, dstOffset, srcDevice, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Array to Device
*
* Copies from one 1D CUDA array to device memory. \p dstDevice specifies the
* base pointer of the destination and must be naturally aligned with the CUDA
* array elements. \p srcArray and \p srcOffset specify the CUDA array handle
* and the offset in bytes into the array where the copy is to begin.
* \p ByteCount specifies the number of bytes to copy and must be evenly
* divisible by the array element size.
*
* \param dstDevice - Destination device pointer
* \param srcArray  - Source array
* \param srcOffset - Offset in bytes of source array
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpyFromArray
*/
int handle_cuMemcpyAtoD_v2(void *conn) {
    CUdeviceptr dstDevice;
    CUarray srcArray;
    size_t srcOffset;
    size_t ByteCount;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyAtoD_v2(dstDevice, srcArray, srcOffset, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Host to Array
*
* Copies from host memory to a 1D CUDA array. \p dstArray and \p dstOffset
* specify the CUDA array handle and starting offset in bytes of the destination
* data.  \p pSrc specifies the base address of the source. \p ByteCount specifies
* the number of bytes to copy.
*
* \param dstArray  - Destination array
* \param dstOffset - Offset in bytes of destination array
* \param srcHost   - Source host pointer
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpyToArray
*/
int handle_cuMemcpyHtoA_v2(void *conn) {
    CUarray dstArray;
    size_t dstOffset;
    void* srcHost;
    size_t ByteCount;

    if (rpc_read(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &srcHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyHtoA_v2(dstArray, dstOffset, &srcHost, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Array to Host
*
* Copies from one 1D CUDA array to host memory. \p dstHost specifies the base
* pointer of the destination. \p srcArray and \p srcOffset specify the CUDA
* array handle and starting offset in bytes of the source data.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstHost   - Destination device pointer
* \param srcArray  - Source array
* \param srcOffset - Offset in bytes of source array
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpyFromArray
*/
int handle_cuMemcpyAtoH_v2(void *conn) {
    void* dstHost;
    CUarray srcArray;
    size_t srcOffset;
    size_t ByteCount;

    if (rpc_read(conn, &dstHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyAtoH_v2(&dstHost, srcArray, srcOffset, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dstHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Array to Array
*
* Copies from one 1D CUDA array to another. \p dstArray and \p srcArray
* specify the handles of the destination and source CUDA arrays for the copy,
* respectively. \p dstOffset and \p srcOffset specify the destination and
* source offsets in bytes into the CUDA arrays. \p ByteCount is the number of
* bytes to be copied. The size of the elements in the CUDA arrays need not be
* the same format, but the elements must be the same size; and count must be
* evenly divisible by that size.
*
* \param dstArray  - Destination array
* \param dstOffset - Offset in bytes of destination array
* \param srcArray  - Source array
* \param srcOffset - Offset in bytes of source array
* \param ByteCount - Size of memory copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpyArrayToArray
*/
int handle_cuMemcpyAtoA_v2(void *conn) {
    CUarray dstArray;
    size_t dstOffset;
    CUarray srcArray;
    size_t srcOffset;
    size_t ByteCount;

    if (rpc_read(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyAtoA_v2(dstArray, dstOffset, srcArray, srcOffset, ByteCount);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory for 2D arrays
*
* Perform a 2D memory copy according to the parameters specified in \p pCopy.
* The ::CUDA_MEMCPY2D structure is defined as:
*
* \code
   typedef struct CUDA_MEMCPY2D_st {
      unsigned int srcXInBytes, srcY;
      CUmemorytype srcMemoryType;
          const void *srcHost;
          CUdeviceptr srcDevice;
          CUarray srcArray;
          unsigned int srcPitch;
      unsigned int dstXInBytes, dstY;
      CUmemorytype dstMemoryType;
          void *dstHost;
          CUdeviceptr dstDevice;
          CUarray dstArray;
          unsigned int dstPitch;
      unsigned int WidthInBytes;
      unsigned int Height;
   } CUDA_MEMCPY2D;
* \endcode
* where:
* - ::srcMemoryType and ::dstMemoryType specify the type of memory of the
*   source and destination, respectively; ::CUmemorytype_enum is defined as:
*
* \code
   typedef enum CUmemorytype_enum {
      CU_MEMORYTYPE_HOST = 0x01,
      CU_MEMORYTYPE_DEVICE = 0x02,
      CU_MEMORYTYPE_ARRAY = 0x03,
      CU_MEMORYTYPE_UNIFIED = 0x04
   } CUmemorytype;
* \endcode
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::srcDevice and ::srcPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::srcArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_HOST, ::srcHost and ::srcPitch
* specify the (host) base address of the source data and the bytes per row to
* apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_DEVICE, ::srcDevice and ::srcPitch
* specify the (device) base address of the source data and the bytes per row
* to apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_ARRAY, ::srcArray specifies the
* handle of the source data. ::srcHost, ::srcDevice and ::srcPitch are
* ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_HOST, ::dstHost and ::dstPitch
* specify the (host) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::dstDevice and ::dstPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::dstArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_DEVICE, ::dstDevice and ::dstPitch
* specify the (device) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_ARRAY, ::dstArray specifies the
* handle of the destination data. ::dstHost, ::dstDevice and ::dstPitch are
* ignored.
*
* - ::srcXInBytes and ::srcY specify the base address of the source data for
*   the copy.
*
* \par
* For host pointers, the starting address is
* \code
  void* Start = (void*)((char*)srcHost+srcY*srcPitch + srcXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr Start = srcDevice+srcY*srcPitch+srcXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::srcXInBytes must be evenly divisible by the array
* element size.
*
* - ::dstXInBytes and ::dstY specify the base address of the destination data
*   for the copy.
*
* \par
* For host pointers, the base address is
* \code
  void* dstStart = (void*)((char*)dstHost+dstY*dstPitch + dstXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr dstStart = dstDevice+dstY*dstPitch+dstXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::dstXInBytes must be evenly divisible by the array
* element size.
*
* - ::WidthInBytes and ::Height specify the width (in bytes) and height of
*   the 2D copy being performed.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
*
* \par
* ::cuMemcpy2D() returns an error if any pitch is greater than the maximum
* allowed (::CU_DEVICE_ATTRIBUTE_MAX_PITCH). ::cuMemAllocPitch() passes back
* pitches that always work with ::cuMemcpy2D(). On intra-device memory copies
* (device to device, CUDA array to device, CUDA array to CUDA array),
* ::cuMemcpy2D() may fail for pitches not computed by ::cuMemAllocPitch().
* ::cuMemcpy2DUnaligned() does not have this restriction, but may run
* significantly slower in the cases where ::cuMemcpy2D() would have returned
* an error code.
*
* \param pCopy - Parameters for the memory copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray,
* ::cudaMemcpy2DFromArray
*/
int handle_cuMemcpy2D_v2(void *conn) {
    CUDA_MEMCPY2D pCopy;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY2D)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy2D_v2(&pCopy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory for 2D arrays
*
* Perform a 2D memory copy according to the parameters specified in \p pCopy.
* The ::CUDA_MEMCPY2D structure is defined as:
*
* \code
   typedef struct CUDA_MEMCPY2D_st {
      unsigned int srcXInBytes, srcY;
      CUmemorytype srcMemoryType;
      const void *srcHost;
      CUdeviceptr srcDevice;
      CUarray srcArray;
      unsigned int srcPitch;
      unsigned int dstXInBytes, dstY;
      CUmemorytype dstMemoryType;
      void *dstHost;
      CUdeviceptr dstDevice;
      CUarray dstArray;
      unsigned int dstPitch;
      unsigned int WidthInBytes;
      unsigned int Height;
   } CUDA_MEMCPY2D;
* \endcode
* where:
* - ::srcMemoryType and ::dstMemoryType specify the type of memory of the
*   source and destination, respectively; ::CUmemorytype_enum is defined as:
*
* \code
   typedef enum CUmemorytype_enum {
      CU_MEMORYTYPE_HOST = 0x01,
      CU_MEMORYTYPE_DEVICE = 0x02,
      CU_MEMORYTYPE_ARRAY = 0x03,
      CU_MEMORYTYPE_UNIFIED = 0x04
   } CUmemorytype;
* \endcode
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::srcDevice and ::srcPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::srcArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_HOST, ::srcHost and ::srcPitch
* specify the (host) base address of the source data and the bytes per row to
* apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_DEVICE, ::srcDevice and ::srcPitch
* specify the (device) base address of the source data and the bytes per row
* to apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_ARRAY, ::srcArray specifies the
* handle of the source data. ::srcHost, ::srcDevice and ::srcPitch are
* ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::dstDevice and ::dstPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::dstArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_HOST, ::dstHost and ::dstPitch
* specify the (host) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_DEVICE, ::dstDevice and ::dstPitch
* specify the (device) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_ARRAY, ::dstArray specifies the
* handle of the destination data. ::dstHost, ::dstDevice and ::dstPitch are
* ignored.
*
* - ::srcXInBytes and ::srcY specify the base address of the source data for
*   the copy.
*
* \par
* For host pointers, the starting address is
* \code
  void* Start = (void*)((char*)srcHost+srcY*srcPitch + srcXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr Start = srcDevice+srcY*srcPitch+srcXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::srcXInBytes must be evenly divisible by the array
* element size.
*
* - ::dstXInBytes and ::dstY specify the base address of the destination data
*   for the copy.
*
* \par
* For host pointers, the base address is
* \code
  void* dstStart = (void*)((char*)dstHost+dstY*dstPitch + dstXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr dstStart = dstDevice+dstY*dstPitch+dstXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::dstXInBytes must be evenly divisible by the array
* element size.
*
* - ::WidthInBytes and ::Height specify the width (in bytes) and height of
*   the 2D copy being performed.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
*
* \par
* ::cuMemcpy2D() returns an error if any pitch is greater than the maximum
* allowed (::CU_DEVICE_ATTRIBUTE_MAX_PITCH). ::cuMemAllocPitch() passes back
* pitches that always work with ::cuMemcpy2D(). On intra-device memory copies
* (device to device, CUDA array to device, CUDA array to CUDA array),
* ::cuMemcpy2D() may fail for pitches not computed by ::cuMemAllocPitch().
* ::cuMemcpy2DUnaligned() does not have this restriction, but may run
* significantly slower in the cases where ::cuMemcpy2D() would have returned
* an error code.
*
* \param pCopy - Parameters for the memory copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray,
* ::cudaMemcpy2DFromArray
*/
int handle_cuMemcpy2DUnaligned_v2(void *conn) {
    CUDA_MEMCPY2D pCopy;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY2D)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy2DUnaligned_v2(&pCopy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory for 3D arrays
*
* Perform a 3D memory copy according to the parameters specified in
* \p pCopy. The ::CUDA_MEMCPY3D structure is defined as:
*
* \code
        typedef struct CUDA_MEMCPY3D_st {
            unsigned int srcXInBytes, srcY, srcZ;
            unsigned int srcLOD;
            CUmemorytype srcMemoryType;
                const void *srcHost;
                CUdeviceptr srcDevice;
                CUarray srcArray;
                unsigned int srcPitch;  // ignored when src is array
                unsigned int srcHeight; // ignored when src is array; may be 0 if Depth==1
            unsigned int dstXInBytes, dstY, dstZ;
            unsigned int dstLOD;
            CUmemorytype dstMemoryType;
                void *dstHost;
                CUdeviceptr dstDevice;
                CUarray dstArray;
                unsigned int dstPitch;  // ignored when dst is array
                unsigned int dstHeight; // ignored when dst is array; may be 0 if Depth==1
            unsigned int WidthInBytes;
            unsigned int Height;
            unsigned int Depth;
        } CUDA_MEMCPY3D;
* \endcode
* where:
* - ::srcMemoryType and ::dstMemoryType specify the type of memory of the
*   source and destination, respectively; ::CUmemorytype_enum is defined as:
*
* \code
   typedef enum CUmemorytype_enum {
      CU_MEMORYTYPE_HOST = 0x01,
      CU_MEMORYTYPE_DEVICE = 0x02,
      CU_MEMORYTYPE_ARRAY = 0x03,
      CU_MEMORYTYPE_UNIFIED = 0x04
   } CUmemorytype;
* \endcode
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::srcDevice and ::srcPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::srcArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_HOST, ::srcHost, ::srcPitch and
* ::srcHeight specify the (host) base address of the source data, the bytes
* per row, and the height of each 2D slice of the 3D array. ::srcArray is
* ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_DEVICE, ::srcDevice, ::srcPitch and
* ::srcHeight specify the (device) base address of the source data, the bytes
* per row, and the height of each 2D slice of the 3D array. ::srcArray is
* ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_ARRAY, ::srcArray specifies the
* handle of the source data. ::srcHost, ::srcDevice, ::srcPitch and
* ::srcHeight are ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::dstDevice and ::dstPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::dstArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_HOST, ::dstHost and ::dstPitch
* specify the (host) base address of the destination data, the bytes per row,
* and the height of each 2D slice of the 3D array. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_DEVICE, ::dstDevice and ::dstPitch
* specify the (device) base address of the destination data, the bytes per
* row, and the height of each 2D slice of the 3D array. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_ARRAY, ::dstArray specifies the
* handle of the destination data. ::dstHost, ::dstDevice, ::dstPitch and
* ::dstHeight are ignored.
*
* - ::srcXInBytes, ::srcY and ::srcZ specify the base address of the source
*   data for the copy.
*
* \par
* For host pointers, the starting address is
* \code
  void* Start = (void*)((char*)srcHost+(srcZ*srcHeight+srcY)*srcPitch + srcXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr Start = srcDevice+(srcZ*srcHeight+srcY)*srcPitch+srcXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::srcXInBytes must be evenly divisible by the array
* element size.
*
* - dstXInBytes, ::dstY and ::dstZ specify the base address of the
*   destination data for the copy.
*
* \par
* For host pointers, the base address is
* \code
  void* dstStart = (void*)((char*)dstHost+(dstZ*dstHeight+dstY)*dstPitch + dstXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr dstStart = dstDevice+(dstZ*dstHeight+dstY)*dstPitch+dstXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::dstXInBytes must be evenly divisible by the array
* element size.
*
* - ::WidthInBytes, ::Height and ::Depth specify the width (in bytes), height
*   and depth of the 3D copy being performed.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
* - If specified, ::srcHeight must be greater than or equal to ::Height +
*   ::srcY, and ::dstHeight must be greater than or equal to ::Height + ::dstY.
*
* \par
* ::cuMemcpy3D() returns an error if any pitch is greater than the maximum
* allowed (::CU_DEVICE_ATTRIBUTE_MAX_PITCH).
*
* The ::srcLOD and ::dstLOD members of the ::CUDA_MEMCPY3D structure must be
* set to 0.
*
* \param pCopy - Parameters for the memory copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMemcpy3D
*/
int handle_cuMemcpy3D_v2(void *conn) {
    CUDA_MEMCPY3D pCopy;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY3D)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy3D_v2(&pCopy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between contexts
*
* Perform a 3D memory copy according to the parameters specified in
* \p pCopy.  See the definition of the ::CUDA_MEMCPY3D_PEER structure
* for documentation of its parameters.
*
* \param pCopy - Parameters for the memory copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_sync
*
* \sa ::cuMemcpyDtoD, ::cuMemcpyPeer, ::cuMemcpyDtoDAsync, ::cuMemcpyPeerAsync,
* ::cuMemcpy3DPeerAsync,
* ::cudaMemcpy3DPeer
*/
int handle_cuMemcpy3DPeer(void *conn) {
    CUDA_MEMCPY3D_PEER pCopy;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY3D_PEER)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy3DPeer(&pCopy);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory asynchronously
*
* Copies data between two pointers.
* \p dst and \p src are base pointers of the destination and source, respectively.
* \p ByteCount specifies the number of bytes to copy.
* Note that this function infers the type of the transfer (host to host, host to
*   device, device to device, or device to host) from the pointer values.  This
*   function is only allowed in contexts which support unified addressing.
*
* \param dst       - Destination unified virtual address space pointer
* \param src       - Source unified virtual address space pointer
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyAsync,
* ::cudaMemcpyToSymbolAsync,
* ::cudaMemcpyFromSymbolAsync
*/
int handle_cuMemcpyAsync(void *conn) {
    CUdeviceptr dst;
    CUdeviceptr src;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dst, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &src, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyAsync(dst, src, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies device memory between two contexts asynchronously.
*
* Copies from device memory in one context to device memory in another
* context. \p dstDevice is the base device pointer of the destination memory
* and \p dstContext is the destination context.  \p srcDevice is the base
* device pointer of the source memory and \p srcContext is the source pointer.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstDevice  - Destination device pointer
* \param dstContext - Destination context
* \param srcDevice  - Source device pointer
* \param srcContext - Source context
* \param ByteCount  - Size of memory copy in bytes
* \param hStream    - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuMemcpyDtoD, ::cuMemcpyPeer, ::cuMemcpy3DPeer, ::cuMemcpyDtoDAsync,
* ::cuMemcpy3DPeerAsync,
* ::cudaMemcpyPeerAsync
*/
int handle_cuMemcpyPeerAsync(void *conn) {
    CUdeviceptr dstDevice;
    CUcontext dstContext;
    CUdeviceptr srcDevice;
    CUcontext srcContext;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyPeerAsync(dstDevice, dstContext, srcDevice, srcContext, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Host to Device
*
* Copies from host memory to device memory. \p dstDevice and \p srcHost are
* the base addresses of the destination and source, respectively. \p ByteCount
* specifies the number of bytes to copy.
*
* \param dstDevice - Destination device pointer
* \param srcHost   - Source host pointer
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyAsync,
* ::cudaMemcpyToSymbolAsync
*/
int handle_cuMemcpyHtoDAsync_v2(void *conn) {
    CUdeviceptr dstDevice;
    void* srcHost;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyHtoDAsync_v2(dstDevice, &srcHost, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Device to Host
*
* Copies from device to host memory. \p dstHost and \p srcDevice specify the
* base pointers of the destination and source, respectively. \p ByteCount
* specifies the number of bytes to copy.
*
* \param dstHost   - Destination host pointer
* \param srcDevice - Source device pointer
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyAsync,
* ::cudaMemcpyFromSymbolAsync
*/
int handle_cuMemcpyDtoHAsync_v2(void *conn) {
    void* dstHost;
    CUdeviceptr srcDevice;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyDtoHAsync_v2(&dstHost, srcDevice, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dstHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Device to Device
*
* Copies from device memory to device memory. \p dstDevice and \p srcDevice
* are the base pointers of the destination and source, respectively.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstDevice - Destination device pointer
* \param srcDevice - Source device pointer
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyAsync,
* ::cudaMemcpyToSymbolAsync,
* ::cudaMemcpyFromSymbolAsync
*/
int handle_cuMemcpyDtoDAsync_v2(void *conn) {
    CUdeviceptr dstDevice;
    CUdeviceptr srcDevice;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyDtoDAsync_v2(dstDevice, srcDevice, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Host to Array
*
* Copies from host memory to a 1D CUDA array. \p dstArray and \p dstOffset
* specify the CUDA array handle and starting offset in bytes of the
* destination data. \p srcHost specifies the base address of the source.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstArray  - Destination array
* \param dstOffset - Offset in bytes of destination array
* \param srcHost   - Source host pointer
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyToArrayAsync
*/
int handle_cuMemcpyHtoAAsync_v2(void *conn) {
    CUarray dstArray;
    size_t dstOffset;
    void* srcHost;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &srcHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyHtoAAsync_v2(dstArray, dstOffset, &srcHost, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory from Array to Host
*
* Copies from one 1D CUDA array to host memory. \p dstHost specifies the base
* pointer of the destination. \p srcArray and \p srcOffset specify the CUDA
* array handle and starting offset in bytes of the source data.
* \p ByteCount specifies the number of bytes to copy.
*
* \param dstHost   - Destination pointer
* \param srcArray  - Source array
* \param srcOffset - Offset in bytes of source array
* \param ByteCount - Size of memory copy in bytes
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
* \note_memcpy
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpyFromArrayAsync
*/
int handle_cuMemcpyAtoHAsync_v2(void *conn) {
    void* dstHost;
    CUarray srcArray;
    size_t srcOffset;
    size_t ByteCount;
    CUstream hStream;

    if (rpc_read(conn, &dstHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpyAtoHAsync_v2(&dstHost, srcArray, srcOffset, ByteCount, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dstHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory for 2D arrays
*
* Perform a 2D memory copy according to the parameters specified in \p pCopy.
* The ::CUDA_MEMCPY2D structure is defined as:
*
* \code
   typedef struct CUDA_MEMCPY2D_st {
      unsigned int srcXInBytes, srcY;
      CUmemorytype srcMemoryType;
      const void *srcHost;
      CUdeviceptr srcDevice;
      CUarray srcArray;
      unsigned int srcPitch;
      unsigned int dstXInBytes, dstY;
      CUmemorytype dstMemoryType;
      void *dstHost;
      CUdeviceptr dstDevice;
      CUarray dstArray;
      unsigned int dstPitch;
      unsigned int WidthInBytes;
      unsigned int Height;
   } CUDA_MEMCPY2D;
* \endcode
* where:
* - ::srcMemoryType and ::dstMemoryType specify the type of memory of the
*   source and destination, respectively; ::CUmemorytype_enum is defined as:
*
* \code
   typedef enum CUmemorytype_enum {
      CU_MEMORYTYPE_HOST = 0x01,
      CU_MEMORYTYPE_DEVICE = 0x02,
      CU_MEMORYTYPE_ARRAY = 0x03,
      CU_MEMORYTYPE_UNIFIED = 0x04
   } CUmemorytype;
* \endcode
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_HOST, ::srcHost and ::srcPitch
* specify the (host) base address of the source data and the bytes per row to
* apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::srcDevice and ::srcPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::srcArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_DEVICE, ::srcDevice and ::srcPitch
* specify the (device) base address of the source data and the bytes per row
* to apply. ::srcArray is ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_ARRAY, ::srcArray specifies the
* handle of the source data. ::srcHost, ::srcDevice and ::srcPitch are
* ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::dstDevice and ::dstPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::dstArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_HOST, ::dstHost and ::dstPitch
* specify the (host) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_DEVICE, ::dstDevice and ::dstPitch
* specify the (device) base address of the destination data and the bytes per
* row to apply. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_ARRAY, ::dstArray specifies the
* handle of the destination data. ::dstHost, ::dstDevice and ::dstPitch are
* ignored.
*
* - ::srcXInBytes and ::srcY specify the base address of the source data for
*   the copy.
*
* \par
* For host pointers, the starting address is
* \code
  void* Start = (void*)((char*)srcHost+srcY*srcPitch + srcXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr Start = srcDevice+srcY*srcPitch+srcXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::srcXInBytes must be evenly divisible by the array
* element size.
*
* - ::dstXInBytes and ::dstY specify the base address of the destination data
*   for the copy.
*
* \par
* For host pointers, the base address is
* \code
  void* dstStart = (void*)((char*)dstHost+dstY*dstPitch + dstXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr dstStart = dstDevice+dstY*dstPitch+dstXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::dstXInBytes must be evenly divisible by the array
* element size.
*
* - ::WidthInBytes and ::Height specify the width (in bytes) and height of
*   the 2D copy being performed.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
* - If specified, ::srcHeight must be greater than or equal to ::Height +
*   ::srcY, and ::dstHeight must be greater than or equal to ::Height + ::dstY.
*
* \par
* ::cuMemcpy2DAsync() returns an error if any pitch is greater than the maximum
* allowed (::CU_DEVICE_ATTRIBUTE_MAX_PITCH). ::cuMemAllocPitch() passes back
* pitches that always work with ::cuMemcpy2D(). On intra-device memory copies
* (device to device, CUDA array to device, CUDA array to CUDA array),
* ::cuMemcpy2DAsync() may fail for pitches not computed by ::cuMemAllocPitch().
*
* \param pCopy   - Parameters for the memory copy
* \param hStream - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync
*/
int handle_cuMemcpy2DAsync_v2(void *conn) {
    CUDA_MEMCPY2D pCopy;
    CUstream hStream;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY2D)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy2DAsync_v2(&pCopy, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory for 3D arrays
*
* Perform a 3D memory copy according to the parameters specified in
* \p pCopy. The ::CUDA_MEMCPY3D structure is defined as:
*
* \code
        typedef struct CUDA_MEMCPY3D_st {
            unsigned int srcXInBytes, srcY, srcZ;
            unsigned int srcLOD;
            CUmemorytype srcMemoryType;
                const void *srcHost;
                CUdeviceptr srcDevice;
                CUarray srcArray;
                unsigned int srcPitch;  // ignored when src is array
                unsigned int srcHeight; // ignored when src is array; may be 0 if Depth==1
            unsigned int dstXInBytes, dstY, dstZ;
            unsigned int dstLOD;
            CUmemorytype dstMemoryType;
                void *dstHost;
                CUdeviceptr dstDevice;
                CUarray dstArray;
                unsigned int dstPitch;  // ignored when dst is array
                unsigned int dstHeight; // ignored when dst is array; may be 0 if Depth==1
            unsigned int WidthInBytes;
            unsigned int Height;
            unsigned int Depth;
        } CUDA_MEMCPY3D;
* \endcode
* where:
* - ::srcMemoryType and ::dstMemoryType specify the type of memory of the
*   source and destination, respectively; ::CUmemorytype_enum is defined as:
*
* \code
   typedef enum CUmemorytype_enum {
      CU_MEMORYTYPE_HOST = 0x01,
      CU_MEMORYTYPE_DEVICE = 0x02,
      CU_MEMORYTYPE_ARRAY = 0x03,
      CU_MEMORYTYPE_UNIFIED = 0x04
   } CUmemorytype;
* \endcode
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::srcDevice and ::srcPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::srcArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_HOST, ::srcHost, ::srcPitch and
* ::srcHeight specify the (host) base address of the source data, the bytes
* per row, and the height of each 2D slice of the 3D array. ::srcArray is
* ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_DEVICE, ::srcDevice, ::srcPitch and
* ::srcHeight specify the (device) base address of the source data, the bytes
* per row, and the height of each 2D slice of the 3D array. ::srcArray is
* ignored.
*
* \par
* If ::srcMemoryType is ::CU_MEMORYTYPE_ARRAY, ::srcArray specifies the
* handle of the source data. ::srcHost, ::srcDevice, ::srcPitch and
* ::srcHeight are ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_UNIFIED, ::dstDevice and ::dstPitch
*   specify the (unified virtual address space) base address of the source data
*   and the bytes per row to apply.  ::dstArray is ignored.
* This value may be used only if unified addressing is supported in the calling
*   context.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_HOST, ::dstHost and ::dstPitch
* specify the (host) base address of the destination data, the bytes per row,
* and the height of each 2D slice of the 3D array. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_DEVICE, ::dstDevice and ::dstPitch
* specify the (device) base address of the destination data, the bytes per
* row, and the height of each 2D slice of the 3D array. ::dstArray is ignored.
*
* \par
* If ::dstMemoryType is ::CU_MEMORYTYPE_ARRAY, ::dstArray specifies the
* handle of the destination data. ::dstHost, ::dstDevice, ::dstPitch and
* ::dstHeight are ignored.
*
* - ::srcXInBytes, ::srcY and ::srcZ specify the base address of the source
*   data for the copy.
*
* \par
* For host pointers, the starting address is
* \code
  void* Start = (void*)((char*)srcHost+(srcZ*srcHeight+srcY)*srcPitch + srcXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr Start = srcDevice+(srcZ*srcHeight+srcY)*srcPitch+srcXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::srcXInBytes must be evenly divisible by the array
* element size.
*
* - dstXInBytes, ::dstY and ::dstZ specify the base address of the
*   destination data for the copy.
*
* \par
* For host pointers, the base address is
* \code
  void* dstStart = (void*)((char*)dstHost+(dstZ*dstHeight+dstY)*dstPitch + dstXInBytes);
* \endcode
*
* \par
* For device pointers, the starting address is
* \code
  CUdeviceptr dstStart = dstDevice+(dstZ*dstHeight+dstY)*dstPitch+dstXInBytes;
* \endcode
*
* \par
* For CUDA arrays, ::dstXInBytes must be evenly divisible by the array
* element size.
*
* - ::WidthInBytes, ::Height and ::Depth specify the width (in bytes), height
*   and depth of the 3D copy being performed.
* - If specified, ::srcPitch must be greater than or equal to ::WidthInBytes +
*   ::srcXInBytes, and ::dstPitch must be greater than or equal to
*   ::WidthInBytes + dstXInBytes.
* - If specified, ::srcHeight must be greater than or equal to ::Height +
*   ::srcY, and ::dstHeight must be greater than or equal to ::Height + ::dstY.
*
* \par
* ::cuMemcpy3DAsync() returns an error if any pitch is greater than the maximum
* allowed (::CU_DEVICE_ATTRIBUTE_MAX_PITCH).
*
* The ::srcLOD and ::dstLOD members of the ::CUDA_MEMCPY3D structure must be
* set to 0.
*
* \param pCopy - Parameters for the memory copy
* \param hStream - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemcpy3DAsync
*/
int handle_cuMemcpy3DAsync_v2(void *conn) {
    CUDA_MEMCPY3D pCopy;
    CUstream hStream;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY3D)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy3DAsync_v2(&pCopy, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between contexts asynchronously.
*
* Perform a 3D memory copy according to the parameters specified in
* \p pCopy.  See the definition of the ::CUDA_MEMCPY3D_PEER structure
* for documentation of its parameters.
*
* \param pCopy - Parameters for the memory copy
* \param hStream - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuMemcpyDtoD, ::cuMemcpyPeer, ::cuMemcpyDtoDAsync, ::cuMemcpyPeerAsync,
* ::cuMemcpy3DPeerAsync,
* ::cudaMemcpy3DPeerAsync
*/
int handle_cuMemcpy3DPeerAsync(void *conn) {
    CUDA_MEMCPY3D_PEER pCopy;
    CUstream hStream;

    if (rpc_read(conn, &pCopy, sizeof(CUDA_MEMCPY3D_PEER)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemcpy3DPeerAsync(&pCopy, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the memory range of \p N 8-bit values to the specified value
* \p uc.
*
* \param dstDevice - Destination device pointer
* \param uc        - Value to set
* \param N         - Number of elements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset
*/
int handle_cuMemsetD8_v2(void *conn) {
    CUdeviceptr dstDevice;
    unsigned char uc;
    size_t N;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD8_v2(dstDevice, uc, N);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the memory range of \p N 16-bit values to the specified value
* \p us. The \p dstDevice pointer must be two byte aligned.
*
* \param dstDevice - Destination device pointer
* \param us        - Value to set
* \param N         - Number of elements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset
*/
int handle_cuMemsetD16_v2(void *conn) {
    CUdeviceptr dstDevice;
    unsigned short us;
    size_t N;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD16_v2(dstDevice, us, N);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the memory range of \p N 32-bit values to the specified value
* \p ui. The \p dstDevice pointer must be four byte aligned.
*
* \param dstDevice - Destination device pointer
* \param ui        - Value to set
* \param N         - Number of elements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32Async,
* ::cudaMemset
*/
int handle_cuMemsetD32_v2(void *conn) {
    CUdeviceptr dstDevice;
    unsigned int ui;
    size_t N;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD32_v2(dstDevice, ui, N);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the 2D memory range of \p Width 8-bit values to the specified value
* \p uc. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param uc        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2D
*/
int handle_cuMemsetD2D8_v2(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned char uc;
    size_t Width;
    size_t Height;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D8_v2(dstDevice, dstPitch, uc, Width, Height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the 2D memory range of \p Width 16-bit values to the specified value
* \p us. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. The \p dstDevice pointer
* and \p dstPitch offset must be two byte aligned. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param us        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2D
*/
int handle_cuMemsetD2D16_v2(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned short us;
    size_t Width;
    size_t Height;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D16_v2(dstDevice, dstPitch, us, Width, Height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes device memory
*
* Sets the 2D memory range of \p Width 32-bit values to the specified value
* \p ui. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. The \p dstDevice pointer
* and \p dstPitch offset must be four byte aligned. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param ui        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2D
*/
int handle_cuMemsetD2D32_v2(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned int ui;
    size_t Width;
    size_t Height;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D32_v2(dstDevice, dstPitch, ui, Width, Height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the memory range of \p N 8-bit values to the specified value
* \p uc.
*
* \param dstDevice - Destination device pointer
* \param uc        - Value to set
* \param N         - Number of elements
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemsetAsync
*/
int handle_cuMemsetD8Async(void *conn) {
    CUdeviceptr dstDevice;
    unsigned char uc;
    size_t N;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD8Async(dstDevice, uc, N, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the memory range of \p N 16-bit values to the specified value
* \p us. The \p dstDevice pointer must be two byte aligned.
*
* \param dstDevice - Destination device pointer
* \param us        - Value to set
* \param N         - Number of elements
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemsetAsync
*/
int handle_cuMemsetD16Async(void *conn) {
    CUdeviceptr dstDevice;
    unsigned short us;
    size_t N;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD16Async(dstDevice, us, N, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the memory range of \p N 32-bit values to the specified value
* \p ui. The \p dstDevice pointer must be four byte aligned.
*
* \param dstDevice - Destination device pointer
* \param ui        - Value to set
* \param N         - Number of elements
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async, ::cuMemsetD32,
* ::cudaMemsetAsync
*/
int handle_cuMemsetD32Async(void *conn) {
    CUdeviceptr dstDevice;
    unsigned int ui;
    size_t N;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &N, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD32Async(dstDevice, ui, N, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the 2D memory range of \p Width 8-bit values to the specified value
* \p uc. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param uc        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2DAsync
*/
int handle_cuMemsetD2D8Async(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned char uc;
    size_t Width;
    size_t Height;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D8Async(dstDevice, dstPitch, uc, Width, Height, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the 2D memory range of \p Width 16-bit values to the specified value
* \p us. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. The \p dstDevice pointer
* and \p dstPitch offset must be two byte aligned. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param us        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D32, ::cuMemsetD2D32Async,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2DAsync
*/
int handle_cuMemsetD2D16Async(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned short us;
    size_t Width;
    size_t Height;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D16Async(dstDevice, dstPitch, us, Width, Height, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets device memory
*
* Sets the 2D memory range of \p Width 32-bit values to the specified value
* \p ui. \p Height specifies the number of rows to set, and \p dstPitch
* specifies the number of bytes between each row. The \p dstDevice pointer
* and \p dstPitch offset must be four byte aligned. This function performs
* fastest when the pitch is one that has been passed back by
* ::cuMemAllocPitch().
*
* \param dstDevice - Destination device pointer
* \param dstPitch  - Pitch of destination device pointer(Unused if \p Height is 1)
* \param ui        - Value to set
* \param Width     - Width of row
* \param Height    - Number of rows
* \param hStream   - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
* \note_memset
* \note_null_stream
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D8Async,
* ::cuMemsetD2D16, ::cuMemsetD2D16Async, ::cuMemsetD2D32,
* ::cuMemsetD8, ::cuMemsetD8Async, ::cuMemsetD16, ::cuMemsetD16Async,
* ::cuMemsetD32, ::cuMemsetD32Async,
* ::cudaMemset2DAsync
*/
int handle_cuMemsetD2D32Async(void *conn) {
    CUdeviceptr dstDevice;
    size_t dstPitch;
    unsigned int ui;
    size_t Width;
    size_t Height;
    CUstream hStream;

    if (rpc_read(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemsetD2D32Async(dstDevice, dstPitch, ui, Width, Height, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a 1D or 2D CUDA array
*
* Creates a CUDA array according to the ::CUDA_ARRAY_DESCRIPTOR structure
* \p pAllocateArray and returns a handle to the new CUDA array in \p *pHandle.
* The ::CUDA_ARRAY_DESCRIPTOR is defined as:
*
* \code
    typedef struct {
        unsigned int Width;
        unsigned int Height;
        CUarray_format Format;
        unsigned int NumChannels;
    } CUDA_ARRAY_DESCRIPTOR;
* \endcode
* where:
*
* - \p Width, and \p Height are the width, and height of the CUDA array (in
* elements); the CUDA array is one-dimensional if height is 0, two-dimensional
* otherwise;
* - ::Format specifies the format of the elements; ::CUarray_format is
* defined as:
* \code
    typedef enum CUarray_format_enum {
        CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
        CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
        CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
        CU_AD_FORMAT_SIGNED_INT8 = 0x08,
        CU_AD_FORMAT_SIGNED_INT16 = 0x09,
        CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
        CU_AD_FORMAT_HALF = 0x10,
        CU_AD_FORMAT_FLOAT = 0x20
    } CUarray_format;
*  \endcode
* - \p NumChannels specifies the number of packed components per CUDA array
* element; it may be 1, 2, or 4;
*
* Here are examples of CUDA array descriptions:
*
* Description for a CUDA array of 2048 floats:
* \code
    CUDA_ARRAY_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_FLOAT;
    desc.NumChannels = 1;
    desc.Width = 2048;
    desc.Height = 1;
* \endcode
*
* Description for a 64 x 64 CUDA array of floats:
* \code
    CUDA_ARRAY_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_FLOAT;
    desc.NumChannels = 1;
    desc.Width = 64;
    desc.Height = 64;
* \endcode
*
* Description for a \p width x \p height CUDA array of 64-bit, 4x16-bit
* float16's:
* \code
    CUDA_ARRAY_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_HALF;
    desc.NumChannels = 4;
    desc.Width = width;
    desc.Height = height;
* \endcode
*
* Description for a \p width x \p height CUDA array of 16-bit elements, each
* of which is two 8-bit unsigned chars:
* \code
    CUDA_ARRAY_DESCRIPTOR arrayDesc;
    desc.Format = CU_AD_FORMAT_UNSIGNED_INT8;
    desc.NumChannels = 2;
    desc.Width = width;
    desc.Height = height;
* \endcode
*
* \param pHandle        - Returned array
* \param pAllocateArray - Array descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMallocArray
*/
int handle_cuArrayCreate_v2(void *conn) {
    CUarray pHandle;
    CUDA_ARRAY_DESCRIPTOR pAllocateArray;

    if (rpc_read(conn, &pHandle, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &pAllocateArray, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayCreate_v2(&pHandle, &pAllocateArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHandle, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Get a 1D or 2D CUDA array descriptor
*
* Returns in \p *pArrayDescriptor a descriptor containing information on the
* format and dimensions of the CUDA array \p hArray. It is useful for
* subroutines that have been passed a CUDA array, but need to know the CUDA
* array parameters for validation or other purposes.
*
* \param pArrayDescriptor - Returned array descriptor
* \param hArray           - Array to get descriptor of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaArrayGetInfo
*/
int handle_cuArrayGetDescriptor_v2(void *conn) {
    CUDA_ARRAY_DESCRIPTOR pArrayDescriptor;
    CUarray hArray;

    if (rpc_read(conn, &pArrayDescriptor, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &hArray, sizeof(CUarray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayGetDescriptor_v2(&pArrayDescriptor, hArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pArrayDescriptor, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the layout properties of a sparse CUDA array
*
* Returns the layout properties of a sparse CUDA array in \p sparseProperties
* If the CUDA array is not allocated with flag ::CUDA_ARRAY3D_SPARSE 
* ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* If the returned value in ::CUDA_ARRAY_SPARSE_PROPERTIES::flags contains ::CU_ARRAY_SPARSE_PROPERTIES_SINGLE_MIPTAIL,
* then ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailSize represents the total size of the array. Otherwise, it will be zero.
* Also, the returned value in ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailFirstLevel is always zero.
* Note that the \p array must have been allocated using ::cuArrayCreate or ::cuArray3DCreate. For CUDA arrays obtained
* using ::cuMipmappedArrayGetLevel, ::CUDA_ERROR_INVALID_VALUE will be returned. Instead, ::cuMipmappedArrayGetSparseProperties 
* must be used to obtain the sparse properties of the entire CUDA mipmapped array to which \p array belongs to.
*
* \return
* ::CUDA_SUCCESS
* ::CUDA_ERROR_INVALID_VALUE
*
* \param[out] sparseProperties - Pointer to ::CUDA_ARRAY_SPARSE_PROPERTIES
* \param[in] array - CUDA array to get the sparse properties of
* \sa ::cuMipmappedArrayGetSparseProperties, ::cuMemMapArrayAsync
*/
int handle_cuArrayGetSparseProperties(void *conn) {
    CUDA_ARRAY_SPARSE_PROPERTIES sparseProperties;
    CUarray array;

    if (rpc_read(conn, &sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_read(conn, &array, sizeof(CUarray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayGetSparseProperties(&sparseProperties, array);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the layout properties of a sparse CUDA mipmapped array
*
* Returns the sparse array layout properties in \p sparseProperties
* If the CUDA mipmapped array is not allocated with flag ::CUDA_ARRAY3D_SPARSE 
* ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* For non-layered CUDA mipmapped arrays, ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailSize returns the
* size of the mip tail region. The mip tail region includes all mip levels whose width, height or depth
* is less than that of the tile.
* For layered CUDA mipmapped arrays, if ::CUDA_ARRAY_SPARSE_PROPERTIES::flags contains ::CU_ARRAY_SPARSE_PROPERTIES_SINGLE_MIPTAIL,
* then ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailSize specifies the size of the mip tail of all layers combined. 
* Otherwise, ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailSize specifies mip tail size per layer.
* The returned value of ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailFirstLevel is valid only if ::CUDA_ARRAY_SPARSE_PROPERTIES::miptailSize is non-zero.
*
* \return
* ::CUDA_SUCCESS
* ::CUDA_ERROR_INVALID_VALUE
*
* \param[out] sparseProperties - Pointer to ::CUDA_ARRAY_SPARSE_PROPERTIES
* \param[in] mipmap - CUDA mipmapped array to get the sparse properties of
* \sa ::cuArrayGetSparseProperties, ::cuMemMapArrayAsync
*/
int handle_cuMipmappedArrayGetSparseProperties(void *conn) {
    CUDA_ARRAY_SPARSE_PROPERTIES sparseProperties;
    CUmipmappedArray mipmap;

    if (rpc_read(conn, &sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_read(conn, &mipmap, sizeof(CUmipmappedArray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMipmappedArrayGetSparseProperties(&sparseProperties, mipmap);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the memory requirements of a CUDA array
*
* Returns the memory requirements of a CUDA array in \p memoryRequirements
* If the CUDA array is not allocated with flag ::CUDA_ARRAY3D_DEFERRED_MAPPING
* ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* The returned value in ::CUDA_ARRAY_MEMORY_REQUIREMENTS::size 
* represents the total size of the CUDA array.
* The returned value in ::CUDA_ARRAY_MEMORY_REQUIREMENTS::alignment 
* represents the alignment necessary for mapping the CUDA array.
*
* \return
* ::CUDA_SUCCESS
* ::CUDA_ERROR_INVALID_VALUE
*
* \param[out] memoryRequirements - Pointer to ::CUDA_ARRAY_MEMORY_REQUIREMENTS
* \param[in] array - CUDA array to get the memory requirements of
* \param[in] device - Device to get the memory requirements for
* \sa ::cuMipmappedArrayGetMemoryRequirements, ::cuMemMapArrayAsync
*/
int handle_cuArrayGetMemoryRequirements(void *conn) {
    CUDA_ARRAY_MEMORY_REQUIREMENTS memoryRequirements;
    CUarray array;
    CUdevice device;

    if (rpc_read(conn, &memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_read(conn, &array, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayGetMemoryRequirements(&memoryRequirements, array, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the memory requirements of a CUDA mipmapped array
*
* Returns the memory requirements of a CUDA mipmapped array in \p memoryRequirements
* If the CUDA mipmapped array is not allocated with flag ::CUDA_ARRAY3D_DEFERRED_MAPPING
* ::CUDA_ERROR_INVALID_VALUE will be returned.
*
* The returned value in ::CUDA_ARRAY_MEMORY_REQUIREMENTS::size 
* represents the total size of the CUDA mipmapped array.
* The returned value in ::CUDA_ARRAY_MEMORY_REQUIREMENTS::alignment 
* represents the alignment necessary for mapping the CUDA mipmapped  
* array.
*
* \return
* ::CUDA_SUCCESS
* ::CUDA_ERROR_INVALID_VALUE
*
* \param[out] memoryRequirements - Pointer to ::CUDA_ARRAY_MEMORY_REQUIREMENTS
* \param[in] mipmap - CUDA mipmapped array to get the memory requirements of
* \param[in] device - Device to get the memory requirements for
* \sa ::cuArrayGetMemoryRequirements, ::cuMemMapArrayAsync
*/
int handle_cuMipmappedArrayGetMemoryRequirements(void *conn) {
    CUDA_ARRAY_MEMORY_REQUIREMENTS memoryRequirements;
    CUmipmappedArray mipmap;
    CUdevice device;

    if (rpc_read(conn, &memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_read(conn, &mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMipmappedArrayGetMemoryRequirements(&memoryRequirements, mipmap, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a CUDA array plane from a CUDA array
*
* Returns in \p pPlaneArray a CUDA array that represents a single format plane
* of the CUDA array \p hArray.
*
* If \p planeIdx is greater than the maximum number of planes in this array or if the array does
* not have a multi-planar format e.g: ::CU_AD_FORMAT_NV12, then ::CUDA_ERROR_INVALID_VALUE is returned.
*
* Note that if the \p hArray has format ::CU_AD_FORMAT_NV12, then passing in 0 for \p planeIdx returns
* a CUDA array of the same size as \p hArray but with one channel and ::CU_AD_FORMAT_UNSIGNED_INT8 as its format.
* If 1 is passed for \p planeIdx, then the returned CUDA array has half the height and width
* of \p hArray with two channels and ::CU_AD_FORMAT_UNSIGNED_INT8 as its format.
*
* \param pPlaneArray   - Returned CUDA array referenced by the \p planeIdx
* \param hArray        - Multiplanar CUDA array
* \param planeIdx      - Plane index
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa
* ::cuArrayCreate,
* ::cudaArrayGetPlane
*/
int handle_cuArrayGetPlane(void *conn) {
    CUarray pPlaneArray;
    CUarray hArray;
    unsigned int planeIdx;

    if (rpc_read(conn, &pPlaneArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &planeIdx, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayGetPlane(&pPlaneArray, hArray, planeIdx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pPlaneArray, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a CUDA array
*
* Destroys the CUDA array \p hArray.
*
* \param hArray - Array to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_ARRAY_IS_MAPPED,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaFreeArray
*/
int handle_cuArrayDestroy(void *conn) {
    CUarray hArray;

    if (rpc_read(conn, &hArray, sizeof(CUarray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArrayDestroy(hArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a 3D CUDA array
*
* Creates a CUDA array according to the ::CUDA_ARRAY3D_DESCRIPTOR structure
* \p pAllocateArray and returns a handle to the new CUDA array in \p *pHandle.
* The ::CUDA_ARRAY3D_DESCRIPTOR is defined as:
*
* \code
    typedef struct {
        unsigned int Width;
        unsigned int Height;
        unsigned int Depth;
        CUarray_format Format;
        unsigned int NumChannels;
        unsigned int Flags;
    } CUDA_ARRAY3D_DESCRIPTOR;
* \endcode
* where:
*
* - \p Width, \p Height, and \p Depth are the width, height, and depth of the
* CUDA array (in elements); the following types of CUDA arrays can be allocated:
*     - A 1D array is allocated if \p Height and \p Depth extents are both zero.
*     - A 2D array is allocated if only \p Depth extent is zero.
*     - A 3D array is allocated if all three extents are non-zero.
*     - A 1D layered CUDA array is allocated if only \p Height is zero and the
*       ::CUDA_ARRAY3D_LAYERED flag is set. Each layer is a 1D array. The number
*       of layers is determined by the depth extent.
*     - A 2D layered CUDA array is allocated if all three extents are non-zero and
*       the ::CUDA_ARRAY3D_LAYERED flag is set. Each layer is a 2D array. The number
*       of layers is determined by the depth extent.
*     - A cubemap CUDA array is allocated if all three extents are non-zero and the
*       ::CUDA_ARRAY3D_CUBEMAP flag is set. \p Width must be equal to \p Height, and
*       \p Depth must be six. A cubemap is a special type of 2D layered CUDA array,
*       where the six layers represent the six faces of a cube. The order of the six
*       layers in memory is the same as that listed in ::CUarray_cubemap_face.
*     - A cubemap layered CUDA array is allocated if all three extents are non-zero,
*       and both, ::CUDA_ARRAY3D_CUBEMAP and ::CUDA_ARRAY3D_LAYERED flags are set.
*       \p Width must be equal to \p Height, and \p Depth must be a multiple of six.
*       A cubemap layered CUDA array is a special type of 2D layered CUDA array that
*       consists of a collection of cubemaps. The first six layers represent the first
*       cubemap, the next six layers form the second cubemap, and so on.
*
* - ::Format specifies the format of the elements; ::CUarray_format is
* defined as:
* \code
    typedef enum CUarray_format_enum {
        CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
        CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
        CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
        CU_AD_FORMAT_SIGNED_INT8 = 0x08,
        CU_AD_FORMAT_SIGNED_INT16 = 0x09,
        CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
        CU_AD_FORMAT_HALF = 0x10,
        CU_AD_FORMAT_FLOAT = 0x20
    } CUarray_format;
*  \endcode
*
* - \p NumChannels specifies the number of packed components per CUDA array
* element; it may be 1, 2, or 4;
*
* - ::Flags may be set to
*   - ::CUDA_ARRAY3D_LAYERED to enable creation of layered CUDA arrays. If this flag is set,
*     \p Depth specifies the number of layers, not the depth of a 3D array.
*   - ::CUDA_ARRAY3D_SURFACE_LDST to enable surface references to be bound to the CUDA array.
*     If this flag is not set, ::cuSurfRefSetArray will fail when attempting to bind the CUDA array
*     to a surface reference.
*   - ::CUDA_ARRAY3D_CUBEMAP to enable creation of cubemaps. If this flag is set, \p Width must be
*     equal to \p Height, and \p Depth must be six. If the ::CUDA_ARRAY3D_LAYERED flag is also set,
*     then \p Depth must be a multiple of six.
*   - ::CUDA_ARRAY3D_TEXTURE_GATHER to indicate that the CUDA array will be used for texture gather.
*     Texture gather can only be performed on 2D CUDA arrays.
*
* \p Width, \p Height and \p Depth must meet certain size requirements as listed in the following table.
* All values are specified in elements. Note that for brevity's sake, the full name of the device attribute
* is not specified. For ex., TEXTURE1D_WIDTH refers to the device attribute
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH.
*
* Note that 2D CUDA arrays have different size requirements if the ::CUDA_ARRAY3D_TEXTURE_GATHER flag
* is set. \p Width and \p Height must not be greater than ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_WIDTH
* and ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT respectively, in that case.
*
* <table>
* <tr><td><b>CUDA array type</b></td>
* <td><b>Valid extents that must always be met<br>{(width range in elements), (height range),
* (depth range)}</b></td>
* <td><b>Valid extents with CUDA_ARRAY3D_SURFACE_LDST set<br>
* {(width range in elements), (height range), (depth range)}</b></td></tr>
* <tr><td>1D</td>
* <td><small>{ (1,TEXTURE1D_WIDTH), 0, 0 }</small></td>
* <td><small>{ (1,SURFACE1D_WIDTH), 0, 0 }</small></td></tr>
* <tr><td>2D</td>
* <td><small>{ (1,TEXTURE2D_WIDTH), (1,TEXTURE2D_HEIGHT), 0 }</small></td>
* <td><small>{ (1,SURFACE2D_WIDTH), (1,SURFACE2D_HEIGHT), 0 }</small></td></tr>
* <tr><td>3D</td>
* <td><small>{ (1,TEXTURE3D_WIDTH), (1,TEXTURE3D_HEIGHT), (1,TEXTURE3D_DEPTH) }
* <br>OR<br>{ (1,TEXTURE3D_WIDTH_ALTERNATE), (1,TEXTURE3D_HEIGHT_ALTERNATE),
* (1,TEXTURE3D_DEPTH_ALTERNATE) }</small></td>
* <td><small>{ (1,SURFACE3D_WIDTH), (1,SURFACE3D_HEIGHT),
* (1,SURFACE3D_DEPTH) }</small></td></tr>
* <tr><td>1D Layered</td>
* <td><small>{ (1,TEXTURE1D_LAYERED_WIDTH), 0,
* (1,TEXTURE1D_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACE1D_LAYERED_WIDTH), 0,
* (1,SURFACE1D_LAYERED_LAYERS) }</small></td></tr>
* <tr><td>2D Layered</td>
* <td><small>{ (1,TEXTURE2D_LAYERED_WIDTH), (1,TEXTURE2D_LAYERED_HEIGHT),
* (1,TEXTURE2D_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACE2D_LAYERED_WIDTH), (1,SURFACE2D_LAYERED_HEIGHT),
* (1,SURFACE2D_LAYERED_LAYERS) }</small></td></tr>
* <tr><td>Cubemap</td>
* <td><small>{ (1,TEXTURECUBEMAP_WIDTH), (1,TEXTURECUBEMAP_WIDTH), 6 }</small></td>
* <td><small>{ (1,SURFACECUBEMAP_WIDTH),
* (1,SURFACECUBEMAP_WIDTH), 6 }</small></td></tr>
* <tr><td>Cubemap Layered</td>
* <td><small>{ (1,TEXTURECUBEMAP_LAYERED_WIDTH), (1,TEXTURECUBEMAP_LAYERED_WIDTH),
* (1,TEXTURECUBEMAP_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACECUBEMAP_LAYERED_WIDTH), (1,SURFACECUBEMAP_LAYERED_WIDTH),
* (1,SURFACECUBEMAP_LAYERED_LAYERS) }</small></td></tr>
* </table>
*
* Here are examples of CUDA array descriptions:
*
* Description for a CUDA array of 2048 floats:
* \code
    CUDA_ARRAY3D_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_FLOAT;
    desc.NumChannels = 1;
    desc.Width = 2048;
    desc.Height = 0;
    desc.Depth = 0;
* \endcode
*
* Description for a 64 x 64 CUDA array of floats:
* \code
    CUDA_ARRAY3D_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_FLOAT;
    desc.NumChannels = 1;
    desc.Width = 64;
    desc.Height = 64;
    desc.Depth = 0;
* \endcode
*
* Description for a \p width x \p height x \p depth CUDA array of 64-bit,
* 4x16-bit float16's:
* \code
    CUDA_ARRAY3D_DESCRIPTOR desc;
    desc.Format = CU_AD_FORMAT_HALF;
    desc.NumChannels = 4;
    desc.Width = width;
    desc.Height = height;
    desc.Depth = depth;
* \endcode
*
* \param pHandle        - Returned array
* \param pAllocateArray - 3D array descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuArray3DGetDescriptor, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaMalloc3DArray
*/
int handle_cuArray3DCreate_v2(void *conn) {
    CUarray pHandle;
    CUDA_ARRAY3D_DESCRIPTOR pAllocateArray;

    if (rpc_read(conn, &pHandle, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &pAllocateArray, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArray3DCreate_v2(&pHandle, &pAllocateArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHandle, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Get a 3D CUDA array descriptor
*
* Returns in \p *pArrayDescriptor a descriptor containing information on the
* format and dimensions of the CUDA array \p hArray. It is useful for
* subroutines that have been passed a CUDA array, but need to know the CUDA
* array parameters for validation or other purposes.
*
* This function may be called on 1D and 2D arrays, in which case the \p Height
* and/or \p Depth members of the descriptor struct will be set to 0.
*
* \param pArrayDescriptor - Returned 3D array descriptor
* \param hArray           - 3D array to get descriptor of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
* \notefnerr
*
* \sa ::cuArray3DCreate, ::cuArrayCreate,
* ::cuArrayDestroy, ::cuArrayGetDescriptor, ::cuMemAlloc, ::cuMemAllocHost,
* ::cuMemAllocPitch, ::cuMemcpy2D, ::cuMemcpy2DAsync, ::cuMemcpy2DUnaligned,
* ::cuMemcpy3D, ::cuMemcpy3DAsync, ::cuMemcpyAtoA, ::cuMemcpyAtoD,
* ::cuMemcpyAtoH, ::cuMemcpyAtoHAsync, ::cuMemcpyDtoA, ::cuMemcpyDtoD, ::cuMemcpyDtoDAsync,
* ::cuMemcpyDtoH, ::cuMemcpyDtoHAsync, ::cuMemcpyHtoA, ::cuMemcpyHtoAAsync,
* ::cuMemcpyHtoD, ::cuMemcpyHtoDAsync, ::cuMemFree, ::cuMemFreeHost,
* ::cuMemGetAddressRange, ::cuMemGetInfo, ::cuMemHostAlloc,
* ::cuMemHostGetDevicePointer, ::cuMemsetD2D8, ::cuMemsetD2D16,
* ::cuMemsetD2D32, ::cuMemsetD8, ::cuMemsetD16, ::cuMemsetD32,
* ::cudaArrayGetInfo
*/
int handle_cuArray3DGetDescriptor_v2(void *conn) {
    CUDA_ARRAY3D_DESCRIPTOR pArrayDescriptor;
    CUarray hArray;

    if (rpc_read(conn, &pArrayDescriptor, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &hArray, sizeof(CUarray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuArray3DGetDescriptor_v2(&pArrayDescriptor, hArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pArrayDescriptor, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a CUDA mipmapped array
*
* Creates a CUDA mipmapped array according to the ::CUDA_ARRAY3D_DESCRIPTOR structure
* \p pMipmappedArrayDesc and returns a handle to the new CUDA mipmapped array in \p *pHandle.
* \p numMipmapLevels specifies the number of mipmap levels to be allocated. This value is
* clamped to the range [1, 1 + floor(log2(max(width, height, depth)))].
*
* The ::CUDA_ARRAY3D_DESCRIPTOR is defined as:
*
* \code
    typedef struct {
        unsigned int Width;
        unsigned int Height;
        unsigned int Depth;
        CUarray_format Format;
        unsigned int NumChannels;
        unsigned int Flags;
    } CUDA_ARRAY3D_DESCRIPTOR;
* \endcode
* where:
*
* - \p Width, \p Height, and \p Depth are the width, height, and depth of the
* CUDA array (in elements); the following types of CUDA arrays can be allocated:
*     - A 1D mipmapped array is allocated if \p Height and \p Depth extents are both zero.
*     - A 2D mipmapped array is allocated if only \p Depth extent is zero.
*     - A 3D mipmapped array is allocated if all three extents are non-zero.
*     - A 1D layered CUDA mipmapped array is allocated if only \p Height is zero and the
*       ::CUDA_ARRAY3D_LAYERED flag is set. Each layer is a 1D array. The number
*       of layers is determined by the depth extent.
*     - A 2D layered CUDA mipmapped array is allocated if all three extents are non-zero and
*       the ::CUDA_ARRAY3D_LAYERED flag is set. Each layer is a 2D array. The number
*       of layers is determined by the depth extent.
*     - A cubemap CUDA mipmapped array is allocated if all three extents are non-zero and the
*       ::CUDA_ARRAY3D_CUBEMAP flag is set. \p Width must be equal to \p Height, and
*       \p Depth must be six. A cubemap is a special type of 2D layered CUDA array,
*       where the six layers represent the six faces of a cube. The order of the six
*       layers in memory is the same as that listed in ::CUarray_cubemap_face.
*     - A cubemap layered CUDA mipmapped array is allocated if all three extents are non-zero,
*       and both, ::CUDA_ARRAY3D_CUBEMAP and ::CUDA_ARRAY3D_LAYERED flags are set.
*       \p Width must be equal to \p Height, and \p Depth must be a multiple of six.
*       A cubemap layered CUDA array is a special type of 2D layered CUDA array that
*       consists of a collection of cubemaps. The first six layers represent the first
*       cubemap, the next six layers form the second cubemap, and so on.
*
* - ::Format specifies the format of the elements; ::CUarray_format is
* defined as:
* \code
    typedef enum CUarray_format_enum {
        CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
        CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
        CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
        CU_AD_FORMAT_SIGNED_INT8 = 0x08,
        CU_AD_FORMAT_SIGNED_INT16 = 0x09,
        CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
        CU_AD_FORMAT_HALF = 0x10,
        CU_AD_FORMAT_FLOAT = 0x20
    } CUarray_format;
*  \endcode
*
* - \p NumChannels specifies the number of packed components per CUDA array
* element; it may be 1, 2, or 4;
*
* - ::Flags may be set to
*   - ::CUDA_ARRAY3D_LAYERED to enable creation of layered CUDA mipmapped arrays. If this flag is set,
*     \p Depth specifies the number of layers, not the depth of a 3D array.
*   - ::CUDA_ARRAY3D_SURFACE_LDST to enable surface references to be bound to individual mipmap levels of
*     the CUDA mipmapped array. If this flag is not set, ::cuSurfRefSetArray will fail when attempting to
*     bind a mipmap level of the CUDA mipmapped array to a surface reference.
*   - ::CUDA_ARRAY3D_CUBEMAP to enable creation of mipmapped cubemaps. If this flag is set, \p Width must be
*     equal to \p Height, and \p Depth must be six. If the ::CUDA_ARRAY3D_LAYERED flag is also set,
*     then \p Depth must be a multiple of six.
*   - ::CUDA_ARRAY3D_TEXTURE_GATHER to indicate that the CUDA mipmapped array will be used for texture gather.
*     Texture gather can only be performed on 2D CUDA mipmapped arrays.
*
* \p Width, \p Height and \p Depth must meet certain size requirements as listed in the following table.
* All values are specified in elements. Note that for brevity's sake, the full name of the device attribute
* is not specified. For ex., TEXTURE1D_MIPMAPPED_WIDTH refers to the device attribute
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH.
*
* <table>
* <tr><td><b>CUDA array type</b></td>
* <td><b>Valid extents that must always be met<br>{(width range in elements), (height range),
* (depth range)}</b></td>
* <td><b>Valid extents with CUDA_ARRAY3D_SURFACE_LDST set<br>
* {(width range in elements), (height range), (depth range)}</b></td></tr>
* <tr><td>1D</td>
* <td><small>{ (1,TEXTURE1D_MIPMAPPED_WIDTH), 0, 0 }</small></td>
* <td><small>{ (1,SURFACE1D_WIDTH), 0, 0 }</small></td></tr>
* <tr><td>2D</td>
* <td><small>{ (1,TEXTURE2D_MIPMAPPED_WIDTH), (1,TEXTURE2D_MIPMAPPED_HEIGHT), 0 }</small></td>
* <td><small>{ (1,SURFACE2D_WIDTH), (1,SURFACE2D_HEIGHT), 0 }</small></td></tr>
* <tr><td>3D</td>
* <td><small>{ (1,TEXTURE3D_WIDTH), (1,TEXTURE3D_HEIGHT), (1,TEXTURE3D_DEPTH) }
* <br>OR<br>{ (1,TEXTURE3D_WIDTH_ALTERNATE), (1,TEXTURE3D_HEIGHT_ALTERNATE),
* (1,TEXTURE3D_DEPTH_ALTERNATE) }</small></td>
* <td><small>{ (1,SURFACE3D_WIDTH), (1,SURFACE3D_HEIGHT),
* (1,SURFACE3D_DEPTH) }</small></td></tr>
* <tr><td>1D Layered</td>
* <td><small>{ (1,TEXTURE1D_LAYERED_WIDTH), 0,
* (1,TEXTURE1D_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACE1D_LAYERED_WIDTH), 0,
* (1,SURFACE1D_LAYERED_LAYERS) }</small></td></tr>
* <tr><td>2D Layered</td>
* <td><small>{ (1,TEXTURE2D_LAYERED_WIDTH), (1,TEXTURE2D_LAYERED_HEIGHT),
* (1,TEXTURE2D_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACE2D_LAYERED_WIDTH), (1,SURFACE2D_LAYERED_HEIGHT),
* (1,SURFACE2D_LAYERED_LAYERS) }</small></td></tr>
* <tr><td>Cubemap</td>
* <td><small>{ (1,TEXTURECUBEMAP_WIDTH), (1,TEXTURECUBEMAP_WIDTH), 6 }</small></td>
* <td><small>{ (1,SURFACECUBEMAP_WIDTH),
* (1,SURFACECUBEMAP_WIDTH), 6 }</small></td></tr>
* <tr><td>Cubemap Layered</td>
* <td><small>{ (1,TEXTURECUBEMAP_LAYERED_WIDTH), (1,TEXTURECUBEMAP_LAYERED_WIDTH),
* (1,TEXTURECUBEMAP_LAYERED_LAYERS) }</small></td>
* <td><small>{ (1,SURFACECUBEMAP_LAYERED_WIDTH), (1,SURFACECUBEMAP_LAYERED_WIDTH),
* (1,SURFACECUBEMAP_LAYERED_LAYERS) }</small></td></tr>
* </table>
*
*
* \param pHandle             - Returned mipmapped array
* \param pMipmappedArrayDesc - mipmapped array descriptor
* \param numMipmapLevels     - Number of mipmap levels
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cuMipmappedArrayDestroy,
* ::cuMipmappedArrayGetLevel,
* ::cuArrayCreate,
* ::cudaMallocMipmappedArray
*/
int handle_cuMipmappedArrayCreate(void *conn) {
    CUmipmappedArray pHandle;
    CUDA_ARRAY3D_DESCRIPTOR pMipmappedArrayDesc;
    unsigned int numMipmapLevels;

    if (rpc_read(conn, &pHandle, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &pMipmappedArrayDesc, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &numMipmapLevels, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMipmappedArrayCreate(&pHandle, &pMipmappedArrayDesc, numMipmapLevels);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHandle, sizeof(CUmipmappedArray)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a mipmap level of a CUDA mipmapped array
*
* Returns in \p *pLevelArray a CUDA array that represents a single mipmap level
* of the CUDA mipmapped array \p hMipmappedArray.
*
* If \p level is greater than the maximum number of levels in this mipmapped array,
* ::CUDA_ERROR_INVALID_VALUE is returned.
*
* \param pLevelArray     - Returned mipmap level CUDA array
* \param hMipmappedArray - CUDA mipmapped array
* \param level           - Mipmap level
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa
* ::cuMipmappedArrayCreate,
* ::cuMipmappedArrayDestroy,
* ::cuArrayCreate,
* ::cudaGetMipmappedArrayLevel
*/
int handle_cuMipmappedArrayGetLevel(void *conn) {
    CUarray pLevelArray;
    CUmipmappedArray hMipmappedArray;
    unsigned int level;

    if (rpc_read(conn, &pLevelArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &level, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMipmappedArrayGetLevel(&pLevelArray, hMipmappedArray, level);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pLevelArray, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a CUDA mipmapped array
*
* Destroys the CUDA mipmapped array \p hMipmappedArray.
*
* \param hMipmappedArray - Mipmapped array to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_ARRAY_IS_MAPPED,
* ::CUDA_ERROR_CONTEXT_IS_DESTROYED
* \notefnerr
*
* \sa
* ::cuMipmappedArrayCreate,
* ::cuMipmappedArrayGetLevel,
* ::cuArrayCreate,
* ::cudaFreeMipmappedArray
*/
int handle_cuMipmappedArrayDestroy(void *conn) {
    CUmipmappedArray hMipmappedArray;

    if (rpc_read(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMipmappedArrayDestroy(hMipmappedArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/** 
* \brief Retrieve handle for an address range 
* 
* Get a handle of the specified type to an address range. The address range
* must have been obtained by a prior call to either ::cuMemAlloc or ::cuMemAddressReserve.
* If the address range was obtained via ::cuMemAddressReserve, it must also be fully mapped via ::cuMemMap.
* 
* Users must ensure the \p dptr and \p size are aligned to the host page size.
* 
* When requesting CUmemRangeHandleType::CU_MEM_RANGE_HANDLE_TYPE_DMA_BUF_FD,
* users are expected to query for dma_buf support for the platform
* by using ::CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED device attribute before calling
* this API. The \p handle will be interpreted as a pointer to an integer to store the dma_buf file descriptor.
* Users must ensure the entire address range is backed and mapped when
* the address range is allocated by ::cuMemAddressReserve. All the physical
* allocations backing the address range must be resident on the same device and
* have identical allocation properties. Users are also expected to retrieve a
* new handle every time the underlying physical allocation(s) corresponding
* to a previously queried VA range are changed.
* 
* \param[out] handle     - Pointer to the location where the returned handle will be stored. 
* \param[in] dptr        - Pointer to a valid CUDA device allocation. Must be aligned to host page size.
* \param[in] size        - Length of the address range. Must be aligned to host page size.
* \param[in] handleType  - Type of handle requested (defines type and size of the \p handle output parameter)
* \param[in] flags       - Reserved, must be zero 
* 
* \return
* CUDA_SUCCESS 
* CUDA_ERROR_INVALID_VALUE 
* CUDA_ERROR_NOT_SUPPORTED 
*/
int handle_cuMemGetHandleForAddressRange(void *conn) {
    void* handle;
    CUdeviceptr dptr;
    size_t size;
    CUmemRangeHandleType handleType;
    unsigned long long flags;

    if (rpc_read(conn, &handle, sizeof(void*)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &handleType, sizeof(CUmemRangeHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetHandleForAddressRange(&handle, dptr, size, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocate an address range reservation. 
* 
* Reserves a virtual address range based on the given parameters, giving
* the starting address of the range in \p ptr.  This API requires a system that
* supports UVA.  The size and address parameters must be a multiple of the
* host page size and the alignment must be a power of two or zero for default
* alignment.
*
* \param[out] ptr       - Resulting pointer to start of virtual address range allocated
* \param[in]  size      - Size of the reserved virtual address range requested
* \param[in]  alignment - Alignment of the reserved virtual address range requested
* \param[in]  addr      - Fixed starting address range requested
* \param[in]  flags     - Currently unused, must be zero
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemAddressFree
*/
int handle_cuMemAddressReserve(void *conn) {
    CUdeviceptr ptr;
    size_t size;
    size_t alignment;
    CUdeviceptr addr;
    unsigned long long flags;

    if (rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &alignment, sizeof(size_t)) < 0 ||
        rpc_read(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAddressReserve(&ptr, size, alignment, addr, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Free an address range reservation.
* 
* Frees a virtual address range reserved by cuMemAddressReserve.  The size
* must match what was given to memAddressReserve and the ptr given must
* match what was returned from memAddressReserve.
*
* \param[in] ptr  - Starting address of the virtual address range to free
* \param[in] size - Size of the virtual address region to free
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemAddressReserve
*/
int handle_cuMemAddressFree(void *conn) {
    CUdeviceptr ptr;
    size_t size;

    if (rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAddressFree(ptr, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Create a CUDA memory handle representing a memory allocation of a given size described by the given properties
*
* This creates a memory allocation on the target device specified through the
* \p prop structure. The created allocation will not have any device or host
* mappings. The generic memory \p handle for the allocation can be
* mapped to the address space of calling process via ::cuMemMap. This handle
* cannot be transmitted directly to other processes (see
* ::cuMemExportToShareableHandle).  On Windows, the caller must also pass
* an LPSECURITYATTRIBUTE in \p prop to be associated with this handle which
* limits or allows access to this handle for a recipient process (see
* ::CUmemAllocationProp::win32HandleMetaData for more).  The \p size of this
* allocation must be a multiple of the the value given via
* ::cuMemGetAllocationGranularity with the ::CU_MEM_ALLOC_GRANULARITY_MINIMUM
* flag.
* If ::CUmemAllocationProp::allocFlags::usage contains ::CU_MEM_CREATE_USAGE_TILE_POOL flag then
* the memory allocation is intended only to be used as backing tile pool for sparse CUDA arrays
* and sparse CUDA mipmapped arrays.
* (see ::cuMemMapArrayAsync).
*
* \param[out] handle - Value of handle returned. All operations on this allocation are to be performed using this handle.
* \param[in]  size   - Size of the allocation requested
* \param[in]  prop   - Properties of the allocation to create.
* \param[in]  flags  - flags for future use, must be zero now.
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuMemRelease, ::cuMemExportToShareableHandle, ::cuMemImportFromShareableHandle
*/
int handle_cuMemCreate(void *conn) {
    CUmemGenericAllocationHandle handle;
    size_t size;
    CUmemAllocationProp prop;
    unsigned long long flags;

    if (rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &prop, sizeof(CUmemAllocationProp)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemCreate(&handle, size, &prop, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0)
        return -1;

    return result;
}

/**
* \brief Release a memory handle representing a memory allocation which was previously allocated through cuMemCreate.
* 
* Frees the memory that was allocated on a device through cuMemCreate.
*
* The memory allocation will be freed when all outstanding mappings to the memory
* are unmapped and when all outstanding references to the handle (including it's
* shareable counterparts) are also released. The generic memory handle can be
* freed when there are still outstanding mappings made with this handle. Each
* time a recipient process imports a shareable handle, it needs to pair it with
* ::cuMemRelease for the handle to be freed.  If \p handle is not a valid handle
* the behavior is undefined. 
*
* \param[in] handle Value of handle which was returned previously by cuMemCreate.
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuMemCreate
*/
int handle_cuMemRelease(void *conn) {
    CUmemGenericAllocationHandle handle;

    if (rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemRelease(handle);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Maps an allocation handle to a reserved virtual address range.
*
* Maps bytes of memory represented by \p handle starting from byte \p offset to
* \p size to address range [\p addr, \p addr + \p size]. This range must be an
* address reservation previously reserved with ::cuMemAddressReserve, and
* \p offset + \p size must be less than the size of the memory allocation.
* Both \p ptr, \p size, and \p offset must be a multiple of the value given via
* ::cuMemGetAllocationGranularity with the ::CU_MEM_ALLOC_GRANULARITY_MINIMUM flag.
* 
* Please note calling ::cuMemMap does not make the address accessible,
* the caller needs to update accessibility of a contiguous mapped VA
* range by calling ::cuMemSetAccess.
* 
* Once a recipient process obtains a shareable memory handle
* from ::cuMemImportFromShareableHandle, the process must
* use ::cuMemMap to map the memory into its address ranges before
* setting accessibility with ::cuMemSetAccess.
*  
* ::cuMemMap can only create mappings on VA range reservations 
* that are not currently mapped.
* 
* \param[in] ptr    - Address where memory will be mapped. 
* \param[in] size   - Size of the memory mapping. 
* \param[in] offset - Offset into the memory represented by 
*                   - \p handle from which to start mapping
*                   - Note: currently must be zero.
* \param[in] handle - Handle to a shareable memory 
* \param[in] flags  - flags for future use, must be zero now. 
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuMemUnmap, ::cuMemSetAccess, ::cuMemCreate, ::cuMemAddressReserve, ::cuMemImportFromShareableHandle
*/
int handle_cuMemMap(void *conn) {
    CUdeviceptr ptr;
    size_t size;
    size_t offset;
    CUmemGenericAllocationHandle handle;
    unsigned long long flags;

    if (rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemMap(ptr, size, offset, handle, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Maps or unmaps subregions of sparse CUDA arrays and sparse CUDA mipmapped arrays
*
* Performs map or unmap operations on subregions of sparse CUDA arrays and sparse CUDA mipmapped arrays.
* Each operation is specified by a ::CUarrayMapInfo entry in the \p mapInfoList array of size \p count.
* The structure ::CUarrayMapInfo is defined as follow:
 \code
     typedef struct CUarrayMapInfo_st {
        CUresourcetype resourceType;                   
        union {
            CUmipmappedArray mipmap;
            CUarray array;
        } resource;
        CUarraySparseSubresourceType subresourceType;   
        union {
            struct {
                unsigned int level;                     
                unsigned int layer;                     
                unsigned int offsetX;                   
                unsigned int offsetY;                   
                unsigned int offsetZ;                   
                unsigned int extentWidth;               
                unsigned int extentHeight;              
                unsigned int extentDepth;               
            } sparseLevel;
            struct {
                unsigned int layer;
                unsigned long long offset;              
                unsigned long long size;                
            } miptail;
        } subresource;
        CUmemOperationType memOperationType;
        
        CUmemHandleType memHandleType;                  
        union {
            CUmemGenericAllocationHandle memHandle;
        } memHandle;
        unsigned long long offset;                      
        unsigned int deviceBitMask;                     
        unsigned int flags;                             
        unsigned int reserved[2];                       
    } CUarrayMapInfo;
 \endcode
*
* where ::CUarrayMapInfo::resourceType specifies the type of resource to be operated on.
* If ::CUarrayMapInfo::resourceType is set to ::CUresourcetype::CU_RESOURCE_TYPE_ARRAY then 
* ::CUarrayMapInfo::resource::array must be set to a valid sparse CUDA array handle.
* The CUDA array must be either a 2D, 2D layered or 3D CUDA array and must have been allocated using
* ::cuArrayCreate or ::cuArray3DCreate with the flag ::CUDA_ARRAY3D_SPARSE
* or ::CUDA_ARRAY3D_DEFERRED_MAPPING.
* For CUDA arrays obtained using ::cuMipmappedArrayGetLevel, ::CUDA_ERROR_INVALID_VALUE will be returned.
* If ::CUarrayMapInfo::resourceType is set to ::CUresourcetype::CU_RESOURCE_TYPE_MIPMAPPED_ARRAY 
* then ::CUarrayMapInfo::resource::mipmap must be set to a valid sparse CUDA mipmapped array handle.
* The CUDA mipmapped array must be either a 2D, 2D layered or 3D CUDA mipmapped array and must have been
* allocated using ::cuMipmappedArrayCreate with the flag ::CUDA_ARRAY3D_SPARSE
* or ::CUDA_ARRAY3D_DEFERRED_MAPPING.
*
* ::CUarrayMapInfo::subresourceType specifies the type of subresource within the resource. 
* ::CUarraySparseSubresourceType_enum is defined as:
 \code
    typedef enum CUarraySparseSubresourceType_enum {
        CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_SPARSE_LEVEL = 0,
        CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_MIPTAIL = 1
    } CUarraySparseSubresourceType;
 \endcode
*
* where ::CUarraySparseSubresourceType::CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_SPARSE_LEVEL indicates a
* sparse-miplevel which spans at least one tile in every dimension. The remaining miplevels which
* are too small to span at least one tile in any dimension constitute the mip tail region as indicated by 
* ::CUarraySparseSubresourceType::CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_MIPTAIL subresource type.
*
* If ::CUarrayMapInfo::subresourceType is set to ::CUarraySparseSubresourceType::CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_SPARSE_LEVEL
* then ::CUarrayMapInfo::subresource::sparseLevel struct must contain valid array subregion offsets and extents.
* The ::CUarrayMapInfo::subresource::sparseLevel::offsetX, ::CUarrayMapInfo::subresource::sparseLevel::offsetY
* and ::CUarrayMapInfo::subresource::sparseLevel::offsetZ must specify valid X, Y and Z offsets respectively.
* The ::CUarrayMapInfo::subresource::sparseLevel::extentWidth, ::CUarrayMapInfo::subresource::sparseLevel::extentHeight
* and ::CUarrayMapInfo::subresource::sparseLevel::extentDepth must specify valid width, height and depth extents respectively.
* These offsets and extents must be aligned to the corresponding tile dimension.
* For CUDA mipmapped arrays ::CUarrayMapInfo::subresource::sparseLevel::level must specify a valid mip level index. Otherwise,
* must be zero.
* For layered CUDA arrays and layered CUDA mipmapped arrays ::CUarrayMapInfo::subresource::sparseLevel::layer must specify a valid layer index. Otherwise,
* must be zero.
* ::CUarrayMapInfo::subresource::sparseLevel::offsetZ must be zero and ::CUarrayMapInfo::subresource::sparseLevel::extentDepth
* must be set to 1 for 2D and 2D layered CUDA arrays and CUDA mipmapped arrays.
* Tile extents can be obtained by calling ::cuArrayGetSparseProperties and ::cuMipmappedArrayGetSparseProperties
*
* If ::CUarrayMapInfo::subresourceType is set to ::CUarraySparseSubresourceType::CU_ARRAY_SPARSE_SUBRESOURCE_TYPE_MIPTAIL
* then ::CUarrayMapInfo::subresource::miptail struct must contain valid mip tail offset in 
* ::CUarrayMapInfo::subresource::miptail::offset and size in ::CUarrayMapInfo::subresource::miptail::size.
* Both, mip tail offset and mip tail size must be aligned to the tile size. 
* For layered CUDA mipmapped arrays which don't have the flag ::CU_ARRAY_SPARSE_PROPERTIES_SINGLE_MIPTAIL set in ::CUDA_ARRAY_SPARSE_PROPERTIES::flags
* as returned by ::cuMipmappedArrayGetSparseProperties, ::CUarrayMapInfo::subresource::miptail::layer must specify a valid layer index.
* Otherwise, must be zero.
*
* If ::CUarrayMapInfo::resource::array or ::CUarrayMapInfo::resource::mipmap was created with ::CUDA_ARRAY3D_DEFERRED_MAPPING
* flag set the ::CUarrayMapInfo::subresourceType and the contents of ::CUarrayMapInfo::subresource will be ignored.
*
* ::CUarrayMapInfo::memOperationType specifies the type of operation. ::CUmemOperationType is defined as:
 \code
    typedef enum CUmemOperationType_enum {
        CU_MEM_OPERATION_TYPE_MAP = 1,
        CU_MEM_OPERATION_TYPE_UNMAP = 2
    } CUmemOperationType;
 \endcode
* If ::CUarrayMapInfo::memOperationType is set to ::CUmemOperationType::CU_MEM_OPERATION_TYPE_MAP then the subresource 
* will be mapped onto the tile pool memory specified by ::CUarrayMapInfo::memHandle at offset ::CUarrayMapInfo::offset. 
* The tile pool allocation has to be created by specifying the ::CU_MEM_CREATE_USAGE_TILE_POOL flag when calling ::cuMemCreate. Also, 
* ::CUarrayMapInfo::memHandleType must be set to ::CUmemHandleType::CU_MEM_HANDLE_TYPE_GENERIC.
* 
* If ::CUarrayMapInfo::memOperationType is set to ::CUmemOperationType::CU_MEM_OPERATION_TYPE_UNMAP then an unmapping operation
* is performed. ::CUarrayMapInfo::memHandle must be NULL.
*
* ::CUarrayMapInfo::deviceBitMask specifies the list of devices that must map or unmap physical memory. 
* Currently, this mask must have exactly one bit set, and the corresponding device must match the device associated with the stream. 
* If ::CUarrayMapInfo::memOperationType is set to ::CUmemOperationType::CU_MEM_OPERATION_TYPE_MAP, the device must also match 
* the device associated with the tile pool memory allocation as specified by ::CUarrayMapInfo::memHandle.
*
* ::CUarrayMapInfo::flags and ::CUarrayMapInfo::reserved[] are unused and must be set to zero.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
*
* \param[in] mapInfoList - List of ::CUarrayMapInfo
* \param[in] count       - Count of ::CUarrayMapInfo  in \p mapInfoList
* \param[in] hStream     - Stream identifier for the stream to use for map or unmap operations
*
* \sa ::cuMipmappedArrayCreate, ::cuArrayCreate, ::cuArray3DCreate, ::cuMemCreate, ::cuArrayGetSparseProperties, ::cuMipmappedArrayGetSparseProperties
*/
int handle_cuMemMapArrayAsync(void *conn) {
    CUarrayMapInfo mapInfoList;
    unsigned int count;
    CUstream hStream;

    if (rpc_read(conn, &mapInfoList, sizeof(CUarrayMapInfo)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemMapArrayAsync(&mapInfoList, count, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mapInfoList, sizeof(CUarrayMapInfo)) < 0)
        return -1;

    return result;
}

/**
* \brief Unmap the backing memory of a given address range.
*
* The range must be the entire contiguous address range that was mapped to.  In
* other words, ::cuMemUnmap cannot unmap a sub-range of an address range mapped
* by ::cuMemCreate / ::cuMemMap.  Any backing memory allocations will be freed
* if there are no existing mappings and there are no unreleased memory handles.
*
* When ::cuMemUnmap returns successfully the address range is converted to an
* address reservation and can be used for a future calls to ::cuMemMap.  Any new
* mapping to this virtual address will need to have access granted through
* ::cuMemSetAccess, as all mappings start with no accessibility setup.
*
* \param[in] ptr  - Starting address for the virtual address range to unmap
* \param[in] size - Size of the virtual address range to unmap
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
* \note_sync
*
* \sa ::cuMemCreate, ::cuMemAddressReserve
*/
int handle_cuMemUnmap(void *conn) {
    CUdeviceptr ptr;
    size_t size;

    if (rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemUnmap(ptr, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set the access flags for each location specified in \p desc for the given virtual address range
* 
* Given the virtual address range via \p ptr and \p size, and the locations
* in the array given by \p desc and \p count, set the access flags for the
* target locations.  The range must be a fully mapped address range
* containing all allocations created by ::cuMemMap / ::cuMemCreate.
*
* \param[in] ptr   - Starting address for the virtual address range
* \param[in] size  - Length of the virtual address range
* \param[in] desc  - Array of ::CUmemAccessDesc that describe how to change the
*                  - mapping for each location specified
* \param[in] count - Number of ::CUmemAccessDesc in \p desc
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
* \note_sync
*
* \sa ::cuMemSetAccess, ::cuMemCreate, :cuMemMap
*/
int handle_cuMemSetAccess(void *conn) {
    CUdeviceptr ptr;
    size_t size;
    CUmemAccessDesc desc;
    size_t count;

    if (rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &desc, sizeof(CUmemAccessDesc)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemSetAccess(ptr, size, &desc, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Get the access \p flags set for the given \p location and \p ptr
*
* \param[out] flags   - Flags set for this location
* \param[in] location - Location in which to check the flags for
* \param[in] ptr      - Address in which to check the access flags for
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemSetAccess
*/
int handle_cuMemGetAccess(void *conn) {
    unsigned long long flags;
    CUmemLocation location;
    CUdeviceptr ptr;

    if (rpc_read(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &location, sizeof(CUmemLocation)) < 0 ||
        rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetAccess(&flags, &location, ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* \brief Exports an allocation to a requested shareable handle type
*
* Given a CUDA memory handle, create a shareable memory
* allocation handle that can be used to share the memory with other
* processes. The recipient process can convert the shareable handle back into a
* CUDA memory handle using ::cuMemImportFromShareableHandle and map
* it with ::cuMemMap. The implementation of what this handle is and how it
* can be transferred is defined by the requested handle type in \p handleType
*
* Once all shareable handles are closed and the allocation is released, the allocated
* memory referenced will be released back to the OS and uses of the CUDA handle afterward
* will lead to undefined behavior.
*
* This API can also be used in conjunction with other APIs (e.g. Vulkan, OpenGL)
* that support importing memory from the shareable type
*
* \param[out] shareableHandle - Pointer to the location in which to store the requested handle type
* \param[in] handle           - CUDA handle for the memory allocation
* \param[in] handleType       - Type of shareable handle requested (defines type and size of the \p shareableHandle output parameter)
* \param[in] flags            - Reserved, must be zero
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemImportFromShareableHandle
*/
int handle_cuMemExportToShareableHandle(void *conn) {
    void* shareableHandle;
    CUmemGenericAllocationHandle handle;
    CUmemAllocationHandleType handleType;
    unsigned long long flags;

    if (rpc_read(conn, &shareableHandle, sizeof(void*)) < 0 ||
        rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &handleType, sizeof(CUmemAllocationHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemExportToShareableHandle(&shareableHandle, handle, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &shareableHandle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Imports an allocation from a requested shareable handle type.
*
* If the current process cannot support the memory described by this shareable
* handle, this API will error as CUDA_ERROR_NOT_SUPPORTED.
*
* \note Importing shareable handles exported from some graphics APIs(VUlkan, OpenGL, etc)
* created on devices under an SLI group may not be supported, and thus this API will
* return CUDA_ERROR_NOT_SUPPORTED.
* There is no guarantee that the contents of \p handle will be the same CUDA memory handle
* for the same given OS shareable handle, or the same underlying allocation.
*
* \param[out] handle       - CUDA Memory handle for the memory allocation.
* \param[in]  osHandle     - Shareable Handle representing the memory allocation that is to be imported. 
* \param[in]  shHandleType - handle type of the exported handle ::CUmemAllocationHandleType.
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemExportToShareableHandle, ::cuMemMap, ::cuMemRelease
*/
int handle_cuMemImportFromShareableHandle(void *conn) {
    CUmemGenericAllocationHandle handle;
    void* osHandle;
    CUmemAllocationHandleType shHandleType;

    if (rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &osHandle, sizeof(void*)) < 0 ||
        rpc_read(conn, &shHandleType, sizeof(CUmemAllocationHandleType)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemImportFromShareableHandle(&handle, &osHandle, shHandleType);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &osHandle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Calculates either the minimal or recommended granularity 
*
* Calculates either the minimal or recommended granularity
* for a given allocation specification and returns it in granularity.  This
* granularity can be used as a multiple for alignment, size, or address mapping.
*
* \param[out] granularity Returned granularity.
* \param[in]  prop Property for which to determine the granularity for
* \param[in]  option Determines which granularity to return
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemCreate, ::cuMemMap
*/
int handle_cuMemGetAllocationGranularity(void *conn) {
    size_t granularity;
    CUmemAllocationProp prop;
    CUmemAllocationGranularity_flags option;

    if (rpc_read(conn, &granularity, sizeof(size_t)) < 0 ||
        rpc_read(conn, &prop, sizeof(CUmemAllocationProp)) < 0 ||
        rpc_read(conn, &option, sizeof(CUmemAllocationGranularity_flags)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetAllocationGranularity(&granularity, &prop, option);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &granularity, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Retrieve the contents of the property structure defining properties for this handle
*
* \param[out] prop  - Pointer to a properties structure which will hold the information about this handle
* \param[in] handle - Handle which to perform the query on
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemCreate, ::cuMemImportFromShareableHandle
*/
int handle_cuMemGetAllocationPropertiesFromHandle(void *conn) {
    CUmemAllocationProp prop;
    CUmemGenericAllocationHandle handle;

    if (rpc_read(conn, &prop, sizeof(CUmemAllocationProp)) < 0 ||
        rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemGetAllocationPropertiesFromHandle(&prop, handle);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &prop, sizeof(CUmemAllocationProp)) < 0)
        return -1;

    return result;
}

/**
* \brief Given an address \p addr, returns the allocation handle of the backing memory allocation.
*
* The handle is guaranteed to be the same handle value used to map the memory. If the address
* requested is not mapped, the function will fail. The returned handle must be released with
* corresponding number of calls to ::cuMemRelease.
*
* \note The address \p addr, can be any address in a range previously mapped
* by ::cuMemMap, and not necessarily the start address.
*
* \param[out] handle CUDA Memory handle for the backing memory allocation.
* \param[in] addr Memory address to query, that has been mapped previously.
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_PERMITTED,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuMemCreate, ::cuMemRelease, ::cuMemMap
*/
int handle_cuMemRetainAllocationHandle(void *conn) {
    CUmemGenericAllocationHandle handle;
    void* addr;

    if (rpc_read(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &addr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemRetainAllocationHandle(&handle, &addr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &addr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees memory with stream ordered semantics
*
* Inserts a free operation into \p hStream.
* The allocation must not be accessed after stream execution reaches the free.
* After this API returns, accessing the memory from any subsequent work launched on the GPU
* or querying its pointer attributes results in undefined behavior.
*
* \note During stream capture, this function results in the creation of a free node and
*       must therefore be passed the address of a graph allocation.
* 
* \param dptr - memory to free
* \param hStream - The stream establishing the stream ordering contract. 
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT (default stream specified with no current context),
* ::CUDA_ERROR_NOT_SUPPORTED
*/
int handle_cuMemFreeAsync(void *conn) {
    CUdeviceptr dptr;
    CUstream hStream;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemFreeAsync(dptr, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates memory with stream ordered semantics
*
* Inserts an allocation operation into \p hStream.
* A pointer to the allocated memory is returned immediately in *dptr.
* The allocation must not be accessed until the the allocation operation completes.
* The allocation comes from the memory pool current to the stream's device.
*
* \note The default memory pool of a device contains device memory from that device.
* \note Basic stream ordering allows future work submitted into the same stream to use the allocation.
*       Stream query, stream synchronize, and CUDA events can be used to guarantee that the allocation
*       operation completes before work submitted in a separate stream runs. 
* \note During stream capture, this function results in the creation of an allocation node.  In this case,
*       the allocation is owned by the graph instead of the memory pool. The memory pool's properties
*       are used to set the node's creation parameters.
*
* \param[out] dptr    - Returned device pointer
* \param[in] bytesize - Number of bytes to allocate
* \param[in] hStream  - The stream establishing the stream ordering contract and the memory pool to allocate from
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT (default stream specified with no current context),
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemAllocFromPoolAsync, ::cuMemFreeAsync, ::cuDeviceSetMemPool,
*     ::cuDeviceGetDefaultMemPool, ::cuDeviceGetMemPool, ::cuMemPoolCreate,
*     ::cuMemPoolSetAccess, ::cuMemPoolSetAttribute
*/
int handle_cuMemAllocAsync(void *conn) {
    CUdeviceptr dptr;
    size_t bytesize;
    CUstream hStream;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAllocAsync(&dptr, bytesize, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Tries to release memory back to the OS
*
* Releases memory back to the OS until the pool contains fewer than minBytesToKeep
* reserved bytes, or there is no more memory that the allocator can safely release.
* The allocator cannot release OS allocations that back outstanding asynchronous allocations.
* The OS allocations may happen at different granularity from the user allocations.
*
* \note: Allocations that have not been freed count as outstanding. 
* \note: Allocations that have been asynchronously freed but whose completion has
*        not been observed on the host (eg. by a synchronize) can count as outstanding.
*
* \param[in] pool           - The memory pool to trim
* \param[in] minBytesToKeep - If the pool has less than minBytesToKeep reserved,
* the TrimTo operation is a no-op.  Otherwise the pool will be guaranteed to have
* at least minBytesToKeep bytes reserved after the operation.
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolTrimTo(void *conn) {
    CUmemoryPool pool;
    size_t minBytesToKeep;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &minBytesToKeep, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolTrimTo(pool, minBytesToKeep);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets attributes of a memory pool
*
* Supported attributes are:
* - ::CU_MEMPOOL_ATTR_RELEASE_THRESHOLD: (value type = cuuint64_t)
*                    Amount of reserved memory in bytes to hold onto before trying
*                    to release memory back to the OS. When more than the release
*                    threshold bytes of memory are held by the memory pool, the
*                    allocator will try to release memory back to the OS on the
*                    next call to stream, event or context synchronize. (default 0)
* - ::CU_MEMPOOL_ATTR_REUSE_FOLLOW_EVENT_DEPENDENCIES: (value type = int)
*                    Allow ::cuMemAllocAsync to use memory asynchronously freed
*                    in another stream as long as a stream ordering dependency
*                    of the allocating stream on the free action exists.
*                    Cuda events and null stream interactions can create the required
*                    stream ordered dependencies. (default enabled)
* - ::CU_MEMPOOL_ATTR_REUSE_ALLOW_OPPORTUNISTIC: (value type = int)
*                    Allow reuse of already completed frees when there is no dependency
*                    between the free and allocation. (default enabled)
* - ::CU_MEMPOOL_ATTR_REUSE_ALLOW_INTERNAL_DEPENDENCIES: (value type = int)
*                    Allow ::cuMemAllocAsync to insert new stream dependencies
*                    in order to establish the stream ordering required to reuse
*                    a piece of memory released by ::cuMemFreeAsync (default enabled).
* - ::CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: (value type = cuuint64_t)
*                    Reset the high watermark that tracks the amount of backing memory that was
*                    allocated for the memory pool. It is illegal to set this attribute to a non-zero value.
* - ::CU_MEMPOOL_ATTR_USED_MEM_HIGH: (value type = cuuint64_t)
*                    Reset the high watermark that tracks the amount of used memory that was
*                    allocated for the memory pool.
*
* \param[in] pool  - The memory pool to modify
* \param[in] attr  - The attribute to modify
* \param[in] value - Pointer to the value to assign
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolSetAttribute(void *conn) {
    CUmemoryPool pool;
    CUmemPool_attribute attr;
    void* value;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUmemPool_attribute)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolSetAttribute(pool, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets attributes of a memory pool
*
* Supported attributes are:
* - ::CU_MEMPOOL_ATTR_RELEASE_THRESHOLD: (value type = cuuint64_t)
*                    Amount of reserved memory in bytes to hold onto before trying
*                    to release memory back to the OS. When more than the release
*                    threshold bytes of memory are held by the memory pool, the
*                    allocator will try to release memory back to the OS on the
*                    next call to stream, event or context synchronize. (default 0)
* - ::CU_MEMPOOL_ATTR_REUSE_FOLLOW_EVENT_DEPENDENCIES: (value type = int)
*                    Allow ::cuMemAllocAsync to use memory asynchronously freed
*                    in another stream as long as a stream ordering dependency
*                    of the allocating stream on the free action exists.
*                    Cuda events and null stream interactions can create the required
*                    stream ordered dependencies. (default enabled)
* - ::CU_MEMPOOL_ATTR_REUSE_ALLOW_OPPORTUNISTIC: (value type = int)
*                    Allow reuse of already completed frees when there is no dependency
*                    between the free and allocation. (default enabled)
* - ::CU_MEMPOOL_ATTR_REUSE_ALLOW_INTERNAL_DEPENDENCIES: (value type = int)
*                    Allow ::cuMemAllocAsync to insert new stream dependencies
*                    in order to establish the stream ordering required to reuse
*                    a piece of memory released by ::cuMemFreeAsync (default enabled).
* - ::CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: (value type = cuuint64_t)
*                    Amount of backing memory currently allocated for the mempool
* - ::CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: (value type = cuuint64_t)
*                    High watermark of backing memory allocated for the mempool since the
*                    last time it was reset.
* - ::CU_MEMPOOL_ATTR_USED_MEM_CURRENT: (value type = cuuint64_t)
*                    Amount of memory from the pool that is currently in use by the application.
* - ::CU_MEMPOOL_ATTR_USED_MEM_HIGH: (value type = cuuint64_t)
*                    High watermark of the amount of memory from the pool that was in use by the application.
*
* \param[in] pool   - The memory pool to get attributes of
* \param[in] attr   - The attribute to get 
* \param[out] value - Retrieved value
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolGetAttribute(void *conn) {
    CUmemoryPool pool;
    CUmemPool_attribute attr;
    void* value;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUmemPool_attribute)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolGetAttribute(pool, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Controls visibility of pools between devices
*
* \param[in] pool  - The pool being modified
* \param[in] map   - Array of access descriptors. Each descriptor instructs the access to enable for a single gpu.
* \param[in] count - Number of descriptors in the map array.
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolSetAccess(void *conn) {
    CUmemoryPool pool;
    CUmemAccessDesc map;
    size_t count;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &map, sizeof(CUmemAccessDesc)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolSetAccess(pool, &map, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the accessibility of a pool from a device
*
* Returns the accessibility of the pool's memory from the specified location. 
*
* \param[out] flags   - the accessibility of the pool from the specified location
* \param[in] memPool  - the pool being queried
* \param[in] location - the location accessing the pool
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolGetAccess(void *conn) {
    CUmemAccess_flags flags;
    CUmemoryPool memPool;
    CUmemLocation location;

    if (rpc_read(conn, &flags, sizeof(CUmemAccess_flags)) < 0 ||
        rpc_read(conn, &memPool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &location, sizeof(CUmemLocation)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolGetAccess(&flags, memPool, &location);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(CUmemAccess_flags)) < 0 ||
        rpc_write(conn, &location, sizeof(CUmemLocation)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memory pool
*
* Creates a CUDA memory pool and returns the handle in \p pool.  The \p poolProps determines
* the properties of the pool such as the backing device and IPC capabilities. 
*
* By default, the pool's memory will be accessible from the device it is allocated on.
*
* \note Specifying CU_MEM_HANDLE_TYPE_NONE creates a memory pool that will not support IPC.
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY,
* ::CUDA_ERROR_NOT_SUPPORTED
*
* \sa ::cuDeviceSetMemPool, ::cuDeviceGetMemPool, ::cuDeviceGetDefaultMemPool,
*     ::cuMemAllocFromPoolAsync, ::cuMemPoolExportToShareableHandle
*/
int handle_cuMemPoolCreate(void *conn) {
    CUmemoryPool pool;
    CUmemPoolProps poolProps;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &poolProps, sizeof(CUmemPoolProps)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolCreate(&pool, &poolProps);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys the specified memory pool
*
* If any pointers obtained from this pool haven't been freed or
* the pool has free operations that haven't completed
* when ::cuMemPoolDestroy is invoked, the function will return immediately and the
* resources associated with the pool will be released automatically
* once there are no more outstanding allocations. 
*
* Destroying the current mempool of a device sets the default mempool of
* that device as the current mempool for that device.
*
* \note A device's default memory pool cannot be destroyed.
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuMemFreeAsync, ::cuDeviceSetMemPool, ::cuDeviceGetMemPool,
*     ::cuDeviceGetDefaultMemPool, ::cuMemPoolCreate
*/
int handle_cuMemPoolDestroy(void *conn) {
    CUmemoryPool pool;

    if (rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolDestroy(pool);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates memory from a specified pool with stream ordered semantics.
*
* Inserts an allocation operation into \p hStream.
* A pointer to the allocated memory is returned immediately in *dptr.
* The allocation must not be accessed until the the allocation operation completes.
* The allocation comes from the specified memory pool.
*
* \note
*    -  The specified memory pool may be from a device different than that of the specified \p hStream. 
* 
*    -  Basic stream ordering allows future work submitted into the same stream to use the allocation.
*       Stream query, stream synchronize, and CUDA events can be used to guarantee that the allocation
*       operation completes before work submitted in a separate stream runs. 
*
* \note During stream capture, this function results in the creation of an allocation node.  In this case,
*       the allocation is owned by the graph instead of the memory pool. The memory pool's properties
*       are used to set the node's creation parameters.
*
* \param[out] dptr    - Returned device pointer
* \param[in] bytesize - Number of bytes to allocate
* \param[in] pool     - The pool to allocate from 
* \param[in] hStream  - The stream establishing the stream ordering semantic
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT (default stream specified with no current context),
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemAllocAsync, ::cuMemFreeAsync, ::cuDeviceGetDefaultMemPool,
*     ::cuDeviceGetMemPool, ::cuMemPoolCreate, ::cuMemPoolSetAccess,
*     ::cuMemPoolSetAttribute
*/
int handle_cuMemAllocFromPoolAsync(void *conn) {
    CUdeviceptr dptr;
    size_t bytesize;
    CUmemoryPool pool;
    CUstream hStream;

    if (rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAllocFromPoolAsync(&dptr, bytesize, pool, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Exports a memory pool to the requested handle type.
*
* Given an IPC capable mempool, create an OS handle to share the pool with another process.
* A recipient process can convert the shareable handle into a mempool with ::cuMemPoolImportFromShareableHandle.
* Individual pointers can then be shared with the ::cuMemPoolExportPointer and ::cuMemPoolImportPointer APIs.
* The implementation of what the shareable handle is and how it can be transferred is defined by the requested
* handle type.
*
* \note: To create an IPC capable mempool, create a mempool with a CUmemAllocationHandleType other than CU_MEM_HANDLE_TYPE_NONE.
*
* \param[out] handle_out  - Returned OS handle 
* \param[in] pool         - pool to export 
* \param[in] handleType   - the type of handle to create 
* \param[in] flags        - must be 0 
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemPoolImportFromShareableHandle, ::cuMemPoolExportPointer,
*     ::cuMemPoolImportPointer, ::cuMemAllocAsync, ::cuMemFreeAsync,
*     ::cuDeviceGetDefaultMemPool, ::cuDeviceGetMemPool, ::cuMemPoolCreate,
*     ::cuMemPoolSetAccess, ::cuMemPoolSetAttribute
*/
int handle_cuMemPoolExportToShareableHandle(void *conn) {
    void* handle_out;
    CUmemoryPool pool;
    CUmemAllocationHandleType handleType;
    unsigned long long flags;

    if (rpc_read(conn, &handle_out, sizeof(void*)) < 0 ||
        rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &handleType, sizeof(CUmemAllocationHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolExportToShareableHandle(&handle_out, pool, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle_out, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief imports a memory pool from a shared handle.
*
* Specific allocations can be imported from the imported pool with cuMemPoolImportPointer.
*
* \note Imported memory pools do not support creating new allocations.
*       As such imported memory pools may not be used in cuDeviceSetMemPool
*       or ::cuMemAllocFromPoolAsync calls.
*
* \param[out] pool_out    - Returned memory pool
* \param[in] handle       - OS handle of the pool to open 
* \param[in] handleType   - The type of handle being imported 
* \param[in] flags        - must be 0 
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemPoolExportToShareableHandle, ::cuMemPoolExportPointer, ::cuMemPoolImportPointer
*/
int handle_cuMemPoolImportFromShareableHandle(void *conn) {
    CUmemoryPool pool_out;
    void* handle;
    CUmemAllocationHandleType handleType;
    unsigned long long flags;

    if (rpc_read(conn, &pool_out, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &handle, sizeof(void*)) < 0 ||
        rpc_read(conn, &handleType, sizeof(CUmemAllocationHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolImportFromShareableHandle(&pool_out, &handle, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pool_out, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, &handle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Export data to share a memory pool allocation between processes.
*
* Constructs \p shareData_out for sharing a specific allocation from an already shared memory pool.
* The recipient process can import the allocation with the ::cuMemPoolImportPointer api.
* The data is not a handle and may be shared through any IPC mechanism.
*
* \param[out] shareData_out - Returned export data  
* \param[in] ptr            - pointer to memory being exported
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemPoolExportToShareableHandle, ::cuMemPoolImportFromShareableHandle, ::cuMemPoolImportPointer
*/
int handle_cuMemPoolExportPointer(void *conn) {
    CUmemPoolPtrExportData shareData_out;
    CUdeviceptr ptr;

    if (rpc_read(conn, &shareData_out, sizeof(CUmemPoolPtrExportData)) < 0 ||
        rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolExportPointer(&shareData_out, ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &shareData_out, sizeof(CUmemPoolPtrExportData)) < 0)
        return -1;

    return result;
}

/**
* \brief Import a memory pool allocation from another process.
*
* Returns in \p ptr_out a pointer to the imported memory.
* The imported memory must not be accessed before the allocation operation completes
* in the exporting process. The imported memory must be freed from all importing processes before
* being freed in the exporting process. The pointer may be freed with cuMemFree
* or cuMemFreeAsync.  If cuMemFreeAsync is used, the free must be completed
* on the importing process before the free operation on the exporting process.
*
* \note The cuMemFreeAsync api may be used in the exporting process before
*       the cuMemFreeAsync operation completes in its stream as long as the
*       cuMemFreeAsync in the exporting process specifies a stream with
*       a stream dependency on the importing process's cuMemFreeAsync.
*
* \param[out] ptr_out  - pointer to imported memory
* \param[in] pool      - pool from which to import
* \param[in] shareData - data specifying the memory to import
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemPoolExportToShareableHandle, ::cuMemPoolImportFromShareableHandle, ::cuMemPoolExportPointer
*/
int handle_cuMemPoolImportPointer(void *conn) {
    CUdeviceptr ptr_out;
    CUmemoryPool pool;
    CUmemPoolPtrExportData shareData;

    if (rpc_read(conn, &ptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &shareData, sizeof(CUmemPoolPtrExportData)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPoolImportPointer(&ptr_out, pool, &shareData);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &shareData, sizeof(CUmemPoolPtrExportData)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about a pointer
*
* The supported attributes are:
*
* - ::CU_POINTER_ATTRIBUTE_CONTEXT:
*
*      Returns in \p *data the ::CUcontext in which \p ptr was allocated or
*      registered.
*      The type of \p data must be ::CUcontext *.
*
*      If \p ptr was not allocated by, mapped by, or registered with
*      a ::CUcontext which uses unified virtual addressing then
*      ::CUDA_ERROR_INVALID_VALUE is returned.
*
* - ::CU_POINTER_ATTRIBUTE_MEMORY_TYPE:
*
*      Returns in \p *data the physical memory type of the memory that
*      \p ptr addresses as a ::CUmemorytype enumerated value.
*      The type of \p data must be unsigned int.
*
*      If \p ptr addresses device memory then \p *data is set to
*      ::CU_MEMORYTYPE_DEVICE.  The particular ::CUdevice on which the
*      memory resides is the ::CUdevice of the ::CUcontext returned by the
*      ::CU_POINTER_ATTRIBUTE_CONTEXT attribute of \p ptr.
*
*      If \p ptr addresses host memory then \p *data is set to
*      ::CU_MEMORYTYPE_HOST.
*
*      If \p ptr was not allocated by, mapped by, or registered with
*      a ::CUcontext which uses unified virtual addressing then
*      ::CUDA_ERROR_INVALID_VALUE is returned.
*
*      If the current ::CUcontext does not support unified virtual
*      addressing then ::CUDA_ERROR_INVALID_CONTEXT is returned.
*
* - ::CU_POINTER_ATTRIBUTE_DEVICE_POINTER:
*
*      Returns in \p *data the device pointer value through which
*      \p ptr may be accessed by kernels running in the current
*      ::CUcontext.
*      The type of \p data must be CUdeviceptr *.
*
*      If there exists no device pointer value through which
*      kernels running in the current ::CUcontext may access
*      \p ptr then ::CUDA_ERROR_INVALID_VALUE is returned.
*
*      If there is no current ::CUcontext then
*      ::CUDA_ERROR_INVALID_CONTEXT is returned.
*
*      Except in the exceptional disjoint addressing cases discussed
*      below, the value returned in \p *data will equal the input
*      value \p ptr.
*
* - ::CU_POINTER_ATTRIBUTE_HOST_POINTER:
*
*      Returns in \p *data the host pointer value through which
*      \p ptr may be accessed by by the host program.
*      The type of \p data must be void **.
*      If there exists no host pointer value through which
*      the host program may directly access \p ptr then
*      ::CUDA_ERROR_INVALID_VALUE is returned.
*
*      Except in the exceptional disjoint addressing cases discussed
*      below, the value returned in \p *data will equal the input
*      value \p ptr.
*
* - ::CU_POINTER_ATTRIBUTE_P2P_TOKENS:
*
*      Returns in \p *data two tokens for use with the nv-p2p.h Linux
*      kernel interface. \p data must be a struct of type
*      CUDA_POINTER_ATTRIBUTE_P2P_TOKENS.
*
*      \p ptr must be a pointer to memory obtained from :cuMemAlloc().
*      Note that p2pToken and vaSpaceToken are only valid for the
*      lifetime of the source allocation. A subsequent allocation at
*      the same address may return completely different tokens.
*      Querying this attribute has a side effect of setting the attribute
*      ::CU_POINTER_ATTRIBUTE_SYNC_MEMOPS for the region of memory that
*      \p ptr points to.
*
* - ::CU_POINTER_ATTRIBUTE_SYNC_MEMOPS:
*
*      A boolean attribute which when set, ensures that synchronous memory operations
*      initiated on the region of memory that \p ptr points to will always synchronize.
*      See further documentation in the section titled "API synchronization behavior"
*      to learn more about cases when synchronous memory operations can
*      exhibit asynchronous behavior.
*
* - ::CU_POINTER_ATTRIBUTE_BUFFER_ID:
*
*      Returns in \p *data a buffer ID which is guaranteed to be unique within the process.
*      \p data must point to an unsigned long long.
*
*      \p ptr must be a pointer to memory obtained from a CUDA memory allocation API.
*      Every memory allocation from any of the CUDA memory allocation APIs will
*      have a unique ID over a process lifetime. Subsequent allocations do not reuse IDs
*      from previous freed allocations. IDs are only unique within a single process.
*
*
* - ::CU_POINTER_ATTRIBUTE_IS_MANAGED:
*
*      Returns in \p *data a boolean that indicates whether the pointer points to
*      managed memory or not.
*
*      If \p ptr is not a valid CUDA pointer then ::CUDA_ERROR_INVALID_VALUE is returned.
*
* - ::CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL:
*
*      Returns in \p *data an integer representing a device ordinal of a device against
*      which the memory was allocated or registered.
*
* - ::CU_POINTER_ATTRIBUTE_IS_LEGACY_CUDA_IPC_CAPABLE:
*
*      Returns in \p *data a boolean that indicates if this pointer maps to
*      an allocation that is suitable for ::cudaIpcGetMemHandle.
*
* - ::CU_POINTER_ATTRIBUTE_RANGE_START_ADDR:
*
*      Returns in \p *data the starting address for the allocation referenced
*      by the device pointer \p ptr.  Note that this is not necessarily the
*      address of the mapped region, but the address of the mappable address
*      range \p ptr references (e.g. from ::cuMemAddressReserve).
*
* - ::CU_POINTER_ATTRIBUTE_RANGE_SIZE:
*
*      Returns in \p *data the size for the allocation referenced by the device
*      pointer \p ptr.  Note that this is not necessarily the size of the mapped
*      region, but the size of the mappable address range \p ptr references
*      (e.g. from ::cuMemAddressReserve).  To retrieve the size of the mapped
*      region, see ::cuMemGetAddressRange
*
* - ::CU_POINTER_ATTRIBUTE_MAPPED:
*
*      Returns in \p *data a boolean that indicates if this pointer is in a
*      valid address range that is mapped to a backing allocation.
*
* - ::CU_POINTER_ATTRIBUTE_ALLOWED_HANDLE_TYPES:
*
*      Returns a bitmask of the allowed handle types for an allocation that may
*      be passed to ::cuMemExportToShareableHandle.
* 
* - ::CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE:
* 
*      Returns in \p *data the handle to the mempool that the allocation was obtained from.
*
* \par
*
* Note that for most allocations in the unified virtual address space
* the host and device pointer for accessing the allocation will be the
* same.  The exceptions to this are
*  - user memory registered using ::cuMemHostRegister
*  - host memory allocated using ::cuMemHostAlloc with the
*    ::CU_MEMHOSTALLOC_WRITECOMBINED flag
* For these types of allocation there will exist separate, disjoint host
* and device addresses for accessing the allocation.  In particular
*  - The host address will correspond to an invalid unmapped device address
*    (which will result in an exception if accessed from the device)
*  - The device address will correspond to an invalid unmapped host address
*    (which will result in an exception if accessed from the host).
* For these types of allocations, querying ::CU_POINTER_ATTRIBUTE_HOST_POINTER
* and ::CU_POINTER_ATTRIBUTE_DEVICE_POINTER may be used to retrieve the host
* and device addresses from either address.
*
* \param data      - Returned pointer attribute value
* \param attribute - Pointer attribute to query
* \param ptr       - Pointer
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuPointerSetAttribute,
* ::cuMemAlloc,
* ::cuMemFree,
* ::cuMemAllocHost,
* ::cuMemFreeHost,
* ::cuMemHostAlloc,
* ::cuMemHostRegister,
* ::cuMemHostUnregister,
* ::cudaPointerGetAttributes
*/
int handle_cuPointerGetAttribute(void *conn) {
    void* data;
    CUpointer_attribute attribute;
    CUdeviceptr ptr;

    if (rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &attribute, sizeof(CUpointer_attribute)) < 0 ||
        rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuPointerGetAttribute(&data, attribute, ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Prefetches memory to the specified destination device
*
* Prefetches memory to the specified destination device.  \p devPtr is the
* base device pointer of the memory to be prefetched and \p dstDevice is the
* destination device. \p count specifies the number of bytes to copy. \p hStream
* is the stream in which the operation is enqueued. The memory range must refer
* to managed memory allocated via ::cuMemAllocManaged or declared via __managed__ variables.
*
* Passing in CU_DEVICE_CPU for \p dstDevice will prefetch the data to host memory. If
* \p dstDevice is a GPU, then the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS
* must be non-zero. Additionally, \p hStream must be associated with a device that has a
* non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS.
*
* The start address and end address of the memory range will be rounded down and rounded up
* respectively to be aligned to CPU page size before the prefetch operation is enqueued
* in the stream.
*
* If no physical memory has been allocated for this region, then this memory region
* will be populated and mapped on the destination device. If there's insufficient
* memory to prefetch the desired region, the Unified Memory driver may evict pages from other
* ::cuMemAllocManaged allocations to host memory in order to make room. Device memory
* allocated using ::cuMemAlloc or ::cuArrayCreate will not be evicted.
*
* By default, any mappings to the previous location of the migrated pages are removed and
* mappings for the new location are only setup on \p dstDevice. The exact behavior however
* also depends on the settings applied to this memory range via ::cuMemAdvise as described
* below:
*
* If ::CU_MEM_ADVISE_SET_READ_MOSTLY was set on any subset of this memory range,
* then that subset will create a read-only copy of the pages on \p dstDevice.
*
* If ::CU_MEM_ADVISE_SET_PREFERRED_LOCATION was called on any subset of this memory
* range, then the pages will be migrated to \p dstDevice even if \p dstDevice is not the
* preferred location of any pages in the memory range.
*
* If ::CU_MEM_ADVISE_SET_ACCESSED_BY was called on any subset of this memory range,
* then mappings to those pages from all the appropriate processors are updated to
* refer to the new location if establishing such a mapping is possible. Otherwise,
* those mappings are cleared.
*
* Note that this API is not required for functionality and only serves to improve performance
* by allowing the application to migrate data to a suitable location before it is accessed.
* Memory accesses to this range are always coherent and are allowed even when the data is
* actively being migrated.
*
* Note that this function is asynchronous with respect to the host and all work
* on other devices.
*
* \param devPtr    - Pointer to be prefetched
* \param count     - Size in bytes
* \param dstDevice - Destination device to prefetch to
* \param hStream    - Stream to enqueue prefetch operation
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuMemcpy, ::cuMemcpyPeer, ::cuMemcpyAsync,
* ::cuMemcpy3DPeerAsync, ::cuMemAdvise,
* ::cudaMemPrefetchAsync
*/
int handle_cuMemPrefetchAsync(void *conn) {
    CUdeviceptr devPtr;
    size_t count;
    CUdevice dstDevice;
    CUstream hStream;

    if (rpc_read(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemPrefetchAsync(devPtr, count, dstDevice, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Advise about the usage of a given memory range
*
* Advise the Unified Memory subsystem about the usage pattern for the memory range
* starting at \p devPtr with a size of \p count bytes. The start address and end address of the memory
* range will be rounded down and rounded up respectively to be aligned to CPU page size before the
* advice is applied. The memory range must refer to managed memory allocated via ::cuMemAllocManaged
* or declared via __managed__ variables. The memory range could also refer to system-allocated pageable
* memory provided it represents a valid, host-accessible region of memory and all additional constraints
* imposed by \p advice as outlined below are also satisfied. Specifying an invalid system-allocated pageable
* memory range results in an error being returned.
*
* The \p advice parameter can take the following values:
* - ::CU_MEM_ADVISE_SET_READ_MOSTLY: This implies that the data is mostly going to be read
* from and only occasionally written to. Any read accesses from any processor to this region will create a
* read-only copy of at least the accessed pages in that processor's memory. Additionally, if ::cuMemPrefetchAsync
* is called on this region, it will create a read-only copy of the data on the destination processor.
* If any processor writes to this region, all copies of the corresponding page will be invalidated
* except for the one where the write occurred. The \p device argument is ignored for this advice.
* Note that for a page to be read-duplicated, the accessing processor must either be the CPU or a GPU
* that has a non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS.
* Also, if a context is created on a device that does not have the device attribute
* ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS set, then read-duplication will not occur until
* all such contexts are destroyed.
* If the memory region refers to valid system-allocated pageable memory, then the accessing device must
* have a non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS for a read-only
* copy to be created on that device. Note however that if the accessing device also has a non-zero value for the
* device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES, then setting this advice
* will not create a read-only copy when that device accesses this memory region.
*
* - ::CU_MEM_ADVISE_UNSET_READ_MOSTLY:  Undoes the effect of ::CU_MEM_ADVISE_SET_READ_MOSTLY and also prevents the
* Unified Memory driver from attempting heuristic read-duplication on the memory range. Any read-duplicated
* copies of the data will be collapsed into a single copy. The location for the collapsed
* copy will be the preferred location if the page has a preferred location and one of the read-duplicated
* copies was resident at that location. Otherwise, the location chosen is arbitrary.
*
* - ::CU_MEM_ADVISE_SET_PREFERRED_LOCATION: This advice sets the preferred location for the
* data to be the memory belonging to \p device. Passing in CU_DEVICE_CPU for \p device sets the
* preferred location as host memory. If \p device is a GPU, then it must have a non-zero value for the
* device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS. Setting the preferred location
* does not cause data to migrate to that location immediately. Instead, it guides the migration policy
* when a fault occurs on that memory region. If the data is already in its preferred location and the
* faulting processor can establish a mapping without requiring the data to be migrated, then
* data migration will be avoided. On the other hand, if the data is not in its preferred location
* or if a direct mapping cannot be established, then it will be migrated to the processor accessing
* it. It is important to note that setting the preferred location does not prevent data prefetching
* done using ::cuMemPrefetchAsync.
* Having a preferred location can override the page thrash detection and resolution logic in the Unified
* Memory driver. Normally, if a page is detected to be constantly thrashing between for example host and device
* memory, the page may eventually be pinned to host memory by the Unified Memory driver. But
* if the preferred location is set as device memory, then the page will continue to thrash indefinitely.
* If ::CU_MEM_ADVISE_SET_READ_MOSTLY is also set on this memory region or any subset of it, then the
* policies associated with that advice will override the policies of this advice, unless read accesses from
* \p device will not result in a read-only copy being created on that device as outlined in description for
* the advice ::CU_MEM_ADVISE_SET_READ_MOSTLY.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS. Additionally, if \p device has
* a non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES,
* then this call has no effect. Note however that this behavior may change in the future.
*
* - ::CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION: Undoes the effect of ::CU_MEM_ADVISE_SET_PREFERRED_LOCATION
* and changes the preferred location to none.
*
* - ::CU_MEM_ADVISE_SET_ACCESSED_BY: This advice implies that the data will be accessed by \p device.
* Passing in ::CU_DEVICE_CPU for \p device will set the advice for the CPU. If \p device is a GPU, then
* the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS must be non-zero.
* This advice does not cause data migration and has no impact on the location of the data per se. Instead,
* it causes the data to always be mapped in the specified processor's page tables, as long as the
* location of the data permits a mapping to be established. If the data gets migrated for any reason,
* the mappings are updated accordingly.
* This advice is recommended in scenarios where data locality is not important, but avoiding faults is.
* Consider for example a system containing multiple GPUs with peer-to-peer access enabled, where the
* data located on one GPU is occasionally accessed by peer GPUs. In such scenarios, migrating data
* over to the other GPUs is not as important because the accesses are infrequent and the overhead of
* migration may be too high. But preventing faults can still help improve performance, and so having
* a mapping set up in advance is useful. Note that on CPU access of this data, the data may be migrated
* to host memory because the CPU typically cannot access device memory directly. Any GPU that had the
* ::CU_MEM_ADVISE_SET_ACCESSED_BY flag set for this data will now have its mapping updated to point to the
* page in host memory.
* If ::CU_MEM_ADVISE_SET_READ_MOSTLY is also set on this memory region or any subset of it, then the
* policies associated with that advice will override the policies of this advice. Additionally, if the
* preferred location of this memory region or any subset of it is also \p device, then the policies
* associated with ::CU_MEM_ADVISE_SET_PREFERRED_LOCATION will override the policies of this advice.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS. Additionally, if \p device has
* a non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES,
* then this call has no effect.
*
* - ::CU_MEM_ADVISE_UNSET_ACCESSED_BY: Undoes the effect of ::CU_MEM_ADVISE_SET_ACCESSED_BY. Any mappings to
* the data from \p device may be removed at any time causing accesses to result in non-fatal page faults.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS. Additionally, if \p device has
* a non-zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES,
* then this call has no effect.
*
* \param devPtr - Pointer to memory to set the advice for
* \param count  - Size in bytes of the memory range
* \param advice - Advice to be applied for the specified memory range
* \param device - Device to apply the advice for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuMemcpy, ::cuMemcpyPeer, ::cuMemcpyAsync,
* ::cuMemcpy3DPeerAsync, ::cuMemPrefetchAsync,
* ::cudaMemAdvise
*/
int handle_cuMemAdvise(void *conn) {
    CUdeviceptr devPtr;
    size_t count;
    CUmem_advise advice;
    CUdevice device;

    if (rpc_read(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &advice, sizeof(CUmem_advise)) < 0 ||
        rpc_read(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemAdvise(devPtr, count, advice, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Query an attribute of a given memory range
*
* Query an attribute about the memory range starting at \p devPtr with a size of \p count bytes. The
* memory range must refer to managed memory allocated via ::cuMemAllocManaged or declared via
* __managed__ variables.
*
* The \p attribute parameter can take the following values:
* - ::CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY: If this attribute is specified, \p data will be interpreted
* as a 32-bit integer, and \p dataSize must be 4. The result returned will be 1 if all pages in the given
* memory range have read-duplication enabled, or 0 otherwise.
* - ::CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be a GPU device
* id if all pages in the memory range have that GPU as their preferred location, or it will be CU_DEVICE_CPU
* if all pages in the memory range have the CPU as their preferred location, or it will be CU_DEVICE_INVALID
* if either all the pages don't have the same preferred location or some of the pages don't have a
* preferred location at all. Note that the actual location of the pages in the memory range at the time of
* the query may be different from the preferred location.
* - ::CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY: If this attribute is specified, \p data will be interpreted
* as an array of 32-bit integers, and \p dataSize must be a non-zero multiple of 4. The result returned
* will be a list of device ids that had ::CU_MEM_ADVISE_SET_ACCESSED_BY set for that entire memory range.
* If any device does not have that advice set for the entire memory range, that device will not be included.
* If \p data is larger than the number of devices that have that advice set for that memory range,
* CU_DEVICE_INVALID will be returned in all the extra space provided. For ex., if \p dataSize is 12
* (i.e. \p data has 3 elements) and only device 0 has the advice set, then the result returned will be
* { 0, CU_DEVICE_INVALID, CU_DEVICE_INVALID }. If \p data is smaller than the number of devices that have
* that advice set, then only as many devices will be returned as can fit in the array. There is no
* guarantee on which specific devices will be returned, however.
* - ::CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be the last location
* to which all pages in the memory range were prefetched explicitly via ::cuMemPrefetchAsync. This will either be
* a GPU id or CU_DEVICE_CPU depending on whether the last location for prefetch was a GPU or the CPU
* respectively. If any page in the memory range was never explicitly prefetched or if all pages were not
* prefetched to the same location, CU_DEVICE_INVALID will be returned. Note that this simply returns the
* last location that the applicaton requested to prefetch the memory range to. It gives no indication as to
* whether the prefetch operation to that location has completed or even begun.
*
* \param data      - A pointers to a memory location where the result
*                    of each attribute query will be written to.
* \param dataSize  - Array containing the size of data
* \param attribute - The attribute to query
* \param devPtr    - Start of the range to query
* \param count     - Size of the range to query
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
* \note_async
* \note_null_stream
*
* \sa ::cuMemRangeGetAttributes, ::cuMemPrefetchAsync,
* ::cuMemAdvise,
* ::cudaMemRangeGetAttribute
*/
int handle_cuMemRangeGetAttribute(void *conn) {
    void* data;
    size_t dataSize;
    CUmem_range_attribute attribute;
    CUdeviceptr devPtr;
    size_t count;

    if (rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &dataSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &attribute, sizeof(CUmem_range_attribute)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemRangeGetAttribute(&data, dataSize, attribute, devPtr, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Query attributes of a given memory range.
*
* Query attributes of the memory range starting at \p devPtr with a size of \p count bytes. The
* memory range must refer to managed memory allocated via ::cuMemAllocManaged or declared via
* __managed__ variables. The \p attributes array will be interpreted to have \p numAttributes
* entries. The \p dataSizes array will also be interpreted to have \p numAttributes entries.
* The results of the query will be stored in \p data.
*
* The list of supported attributes are given below. Please refer to ::cuMemRangeGetAttribute for
* attribute descriptions and restrictions.
*
* - ::CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY
* - ::CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION
* - ::CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY
* - ::CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION
*
* \param data          - A two-dimensional array containing pointers to memory
*                        locations where the result of each attribute query will be written to.
* \param dataSizes     - Array containing the sizes of each result
* \param attributes    - An array of attributes to query
*                        (numAttributes and the number of attributes in this array should match)
* \param numAttributes - Number of attributes to query
* \param devPtr        - Start of the range to query
* \param count         - Size of the range to query
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa ::cuMemRangeGetAttribute, ::cuMemAdvise,
* ::cuMemPrefetchAsync,
* ::cudaMemRangeGetAttributes
*/
int handle_cuMemRangeGetAttributes(void *conn) {
    void* data;
    size_t dataSizes;
    CUmem_range_attribute attributes;
    size_t numAttributes;
    CUdeviceptr devPtr;
    size_t count;

    if (rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &dataSizes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &attributes, sizeof(CUmem_range_attribute)) < 0 ||
        rpc_read(conn, &numAttributes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuMemRangeGetAttributes(&data, &dataSizes, &attributes, numAttributes, devPtr, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0 ||
        rpc_write(conn, &dataSizes, sizeof(size_t)) < 0 ||
        rpc_write(conn, &attributes, sizeof(CUmem_range_attribute)) < 0)
        return -1;

    return result;
}

/**
* \brief Set attributes on a previously allocated memory region
*
* The supported attributes are:
*
* - ::CU_POINTER_ATTRIBUTE_SYNC_MEMOPS:
*
*      A boolean attribute that can either be set (1) or unset (0). When set,
*      the region of memory that \p ptr points to is guaranteed to always synchronize
*      memory operations that are synchronous. If there are some previously initiated
*      synchronous memory operations that are pending when this attribute is set, the
*      function does not return until those memory operations are complete.
*      See further documentation in the section titled "API synchronization behavior"
*      to learn more about cases when synchronous memory operations can
*      exhibit asynchronous behavior.
*      \p value will be considered as a pointer to an unsigned integer to which this attribute is to be set.
*
* \param value     - Pointer to memory containing the value to be set
* \param attribute - Pointer attribute to set
* \param ptr       - Pointer to a memory region allocated using CUDA memory allocation APIs
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa ::cuPointerGetAttribute,
* ::cuPointerGetAttributes,
* ::cuMemAlloc,
* ::cuMemFree,
* ::cuMemAllocHost,
* ::cuMemFreeHost,
* ::cuMemHostAlloc,
* ::cuMemHostRegister,
* ::cuMemHostUnregister
*/
int handle_cuPointerSetAttribute(void *conn) {
    void* value;
    CUpointer_attribute attribute;
    CUdeviceptr ptr;

    if (rpc_read(conn, &value, sizeof(void*)) < 0 ||
        rpc_read(conn, &attribute, sizeof(CUpointer_attribute)) < 0 ||
        rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuPointerSetAttribute(&value, attribute, ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about a pointer.
*
* The supported attributes are (refer to ::cuPointerGetAttribute for attribute descriptions and restrictions):
*
* - ::CU_POINTER_ATTRIBUTE_CONTEXT
* - ::CU_POINTER_ATTRIBUTE_MEMORY_TYPE
* - ::CU_POINTER_ATTRIBUTE_DEVICE_POINTER
* - ::CU_POINTER_ATTRIBUTE_HOST_POINTER
* - ::CU_POINTER_ATTRIBUTE_SYNC_MEMOPS
* - ::CU_POINTER_ATTRIBUTE_BUFFER_ID
* - ::CU_POINTER_ATTRIBUTE_IS_MANAGED
* - ::CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL
* - ::CU_POINTER_ATTRIBUTE_RANGE_START_ADDR
* - ::CU_POINTER_ATTRIBUTE_RANGE_SIZE
* - ::CU_POINTER_ATTRIBUTE_MAPPED
* - ::CU_POINTER_ATTRIBUTE_IS_LEGACY_CUDA_IPC_CAPABLE
* - ::CU_POINTER_ATTRIBUTE_ALLOWED_HANDLE_TYPES
* - ::CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE
*
* \param numAttributes - Number of attributes to query
* \param attributes    - An array of attributes to query
*                      (numAttributes and the number of attributes in this array should match)
* \param data          - A two-dimensional array containing pointers to memory
*                      locations where the result of each attribute query will be written to.
* \param ptr           - Pointer to query
*
* Unlike ::cuPointerGetAttribute, this function will not return an error when the \p ptr
* encountered is not a valid CUDA pointer. Instead, the attributes are assigned default NULL values
* and CUDA_SUCCESS is returned.
*
* If \p ptr was not allocated by, mapped by, or registered with a ::CUcontext which uses UVA
* (Unified Virtual Addressing), ::CUDA_ERROR_INVALID_CONTEXT is returned.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuPointerGetAttribute,
* ::cuPointerSetAttribute,
* ::cudaPointerGetAttributes
*/
int handle_cuPointerGetAttributes(void *conn) {
    unsigned int numAttributes;
    CUpointer_attribute attributes;
    void* data;
    CUdeviceptr ptr;

    if (rpc_read(conn, &numAttributes, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &attributes, sizeof(CUpointer_attribute)) < 0 ||
        rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &ptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuPointerGetAttributes(numAttributes, &attributes, &data, ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &attributes, sizeof(CUpointer_attribute)) < 0 ||
        rpc_write(conn, &data, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Create a stream
*
* Creates a stream and returns a handle in \p phStream.  The \p Flags argument
* determines behaviors of the stream.
*
* Valid values for \p Flags are:
* - ::CU_STREAM_DEFAULT: Default stream creation flag.
* - ::CU_STREAM_NON_BLOCKING: Specifies that work running in the created
*   stream may run concurrently with work in stream 0 (the NULL stream), and that
*   the created stream should perform no implicit synchronization with stream 0.
*
* \param phStream - Returned newly created stream
* \param Flags    - Parameters for stream creation
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreateWithPriority,
* ::cuStreamGetPriority,
* ::cuStreamGetFlags,
* ::cuStreamWaitEvent,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cudaStreamCreate,
* ::cudaStreamCreateWithFlags
*/
int handle_cuStreamCreate(void *conn) {
    CUstream phStream;
    unsigned int Flags;

    if (rpc_read(conn, &phStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamCreate(&phStream, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phStream, sizeof(CUstream)) < 0)
        return -1;

    return result;
}

/**
* \brief Create a stream with the given priority
*
* Creates a stream with the specified priority and returns a handle in \p phStream.
* This API alters the scheduler priority of work in the stream. Work in a higher
* priority stream may preempt work already executing in a low priority stream.
*
* \p priority follows a convention where lower numbers represent higher priorities.
* '0' represents default priority. The range of meaningful numerical priorities can
* be queried using ::cuCtxGetStreamPriorityRange. If the specified priority is
* outside the numerical range returned by ::cuCtxGetStreamPriorityRange,
* it will automatically be clamped to the lowest or the highest number in the range.
*
* \param phStream    - Returned newly created stream
* \param flags       - Flags for stream creation. See ::cuStreamCreate for a list of
*                      valid flags
* \param priority    - Stream priority. Lower numbers represent higher priorities.
*                      See ::cuCtxGetStreamPriorityRange for more information about
*                      meaningful stream priorities that can be passed.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \note Stream priorities are supported only on GPUs
* with compute capability 3.5 or higher.
*
* \note In the current implementation, only compute kernels launched in
* priority streams are affected by the stream's priority. Stream priorities have
* no effect on host-to-device and device-to-host memory operations.
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreate,
* ::cuStreamGetPriority,
* ::cuCtxGetStreamPriorityRange,
* ::cuStreamGetFlags,
* ::cuStreamWaitEvent,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cudaStreamCreateWithPriority
*/
int handle_cuStreamCreateWithPriority(void *conn) {
    CUstream phStream;
    unsigned int flags;
    int priority;

    if (rpc_read(conn, &phStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &priority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamCreateWithPriority(&phStream, flags, priority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phStream, sizeof(CUstream)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the priority of a given stream
*
* Query the priority of a stream created using ::cuStreamCreate or ::cuStreamCreateWithPriority
* and return the priority in \p priority. Note that if the stream was created with a
* priority outside the numerical range returned by ::cuCtxGetStreamPriorityRange,
* this function returns the clamped priority.
* See ::cuStreamCreateWithPriority for details about priority clamping.
*
* \param hStream    - Handle to the stream to be queried
* \param priority   - Pointer to a signed integer in which the stream's priority is returned
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreate,
* ::cuStreamCreateWithPriority,
* ::cuCtxGetStreamPriorityRange,
* ::cuStreamGetFlags,
* ::cudaStreamGetPriority
*/
int handle_cuStreamGetPriority(void *conn) {
    CUstream hStream;
    int priority;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &priority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetPriority(hStream, &priority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &priority, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the flags of a given stream
*
* Query the flags of a stream created using ::cuStreamCreate or ::cuStreamCreateWithPriority
* and return the flags in \p flags.
*
* \param hStream    - Handle to the stream to be queried
* \param flags      - Pointer to an unsigned integer in which the stream's flags are returned
*                     The value returned in \p flags is a logical 'OR' of all flags that
*                     were used while creating this stream. See ::cuStreamCreate for the list
*                     of valid flags
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreate,
* ::cuStreamGetPriority,
* ::cudaStreamGetFlags
*/
int handle_cuStreamGetFlags(void *conn) {
    CUstream hStream;
    unsigned int flags;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetFlags(hStream, &flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the unique Id associated with the stream handle supplied
*
* Returns in \p streamId the unique Id which is associated with the given stream handle.
* The Id is unique for the life of the program for this instance of CUDA.
* 
* The stream handle \p hStream can refer to any of the following:
* <ul>
*   <li>a stream created via any of the CUDA driver APIs such as ::cuStreamCreate
*   and ::cuStreamCreateWithPriority, or their runtime API equivalents such as
*   ::cudaStreamCreate, ::cudaStreamCreateWithFlags and ::cudaStreamCreateWithPriority.
*   Passing an invalid handle will result in undefined behavior.</li>
*   <li>any of the special streams such as the NULL stream, ::CU_STREAM_LEGACY and
*   ::CU_STREAM_PER_THREAD. The runtime API equivalents of these are also accepted,
*   which are NULL, ::cudaStreamLegacy and ::cudaStreamPerThread respectively.</li>
* </ul>
*
* \param hStream    - Handle to the stream to be queried
* \param streamId   - Pointer to store the Id of the stream
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreate,
* ::cuStreamGetPriority,
* ::cudaStreamGetId
*/
int handle_cuStreamGetId(void *conn) {
    CUstream hStream;
    unsigned long long streamId;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &streamId, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetId(hStream, &streamId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &streamId, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the context associated with a stream
*
* Returns the CUDA context that the stream is associated with.
*
* The stream handle \p hStream can refer to any of the following:
* <ul>
*   <li>a stream created via any of the CUDA driver APIs such as ::cuStreamCreate
*   and ::cuStreamCreateWithPriority, or their runtime API equivalents such as
*   ::cudaStreamCreate, ::cudaStreamCreateWithFlags and ::cudaStreamCreateWithPriority.
*   The returned context is the context that was active in the calling thread when the
*   stream was created. Passing an invalid handle will result in undefined behavior.</li>
*   <li>any of the special streams such as the NULL stream, ::CU_STREAM_LEGACY and
*   ::CU_STREAM_PER_THREAD. The runtime API equivalents of these are also accepted,
*   which are NULL, ::cudaStreamLegacy and ::cudaStreamPerThread respectively.
*   Specifying any of the special handles will return the context current to the
*   calling thread. If no context is current to the calling thread,
*   ::CUDA_ERROR_INVALID_CONTEXT is returned.</li>
* </ul>
*
* \param hStream - Handle to the stream to be queried
* \param pctx    - Returned context associated with the stream
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* \notefnerr
*
* \sa ::cuStreamDestroy,
* ::cuStreamCreateWithPriority,
* ::cuStreamGetPriority,
* ::cuStreamGetFlags,
* ::cuStreamWaitEvent,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cudaStreamCreate,
* ::cudaStreamCreateWithFlags
*/
int handle_cuStreamGetCtx(void *conn) {
    CUstream hStream;
    CUcontext pctx;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetCtx(hStream, &pctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pctx, sizeof(CUcontext)) < 0)
        return -1;

    return result;
}

/**
* \brief Make a compute stream wait on an event
*
* Makes all future work submitted to \p hStream wait for all work captured in
* \p hEvent.  See ::cuEventRecord() for details on what is captured by an event.
* The synchronization will be performed efficiently on the device when applicable.
* \p hEvent may be from a different context or device than \p hStream.
*
* flags include:
* - ::CU_EVENT_WAIT_DEFAULT: Default event creation flag.
* - ::CU_EVENT_WAIT_EXTERNAL: Event is captured in the graph as an external
*   event node when performing stream capture. This flag is invalid outside
*   of stream capture.
*
* \param hStream - Stream to wait
* \param hEvent  - Event to wait on (may not be NULL)
* \param Flags   - See ::CUevent_capture_flags
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuEventRecord,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cuStreamDestroy,
* ::cudaStreamWaitEvent
*/
int handle_cuStreamWaitEvent(void *conn) {
    CUstream hStream;
    CUevent hEvent;
    unsigned int Flags;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamWaitEvent(hStream, hEvent, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Add a callback to a compute stream
*
* \note This function is slated for eventual deprecation and removal. If
* you do not require the callback to execute in case of a device error,
* consider using ::cuLaunchHostFunc. Additionally, this function is not
* supported with ::cuStreamBeginCapture and ::cuStreamEndCapture, unlike
* ::cuLaunchHostFunc.
*
* Adds a callback to be called on the host after all currently enqueued
* items in the stream have completed.  For each
* cuStreamAddCallback call, the callback will be executed exactly once.
* The callback will block later work in the stream until it is finished.
*
* The callback may be passed ::CUDA_SUCCESS or an error code.  In the event
* of a device error, all subsequently executed callbacks will receive an
* appropriate ::CUresult.
*
* Callbacks must not make any CUDA API calls.  Attempting to use a CUDA API
* will result in ::CUDA_ERROR_NOT_PERMITTED.  Callbacks must not perform any
* synchronization that may depend on outstanding device work or other callbacks
* that are not mandated to run earlier.  Callbacks without a mandated order
* (in independent streams) execute in undefined order and may be serialized.
*
* For the purposes of Unified Memory, callback execution makes a number of
* guarantees:
* <ul>
*   <li>The callback stream is considered idle for the duration of the
*   callback.  Thus, for example, a callback may always use memory attached
*   to the callback stream.</li>
*   <li>The start of execution of a callback has the same effect as
*   synchronizing an event recorded in the same stream immediately prior to
*   the callback.  It thus synchronizes streams which have been "joined"
*   prior to the callback.</li>
*   <li>Adding device work to any stream does not have the effect of making
*   the stream active until all preceding host functions and stream callbacks
*   have executed.  Thus, for
*   example, a callback might use global attached memory even if work has
*   been added to another stream, if the work has been ordered behind the
*   callback with an event.</li>
*   <li>Completion of a callback does not cause a stream to become
*   active except as described above.  The callback stream will remain idle
*   if no device work follows the callback, and will remain idle across
*   consecutive callbacks without device work in between.  Thus, for example,
*   stream synchronization can be done by signaling from a callback at the
*   end of the stream.</li>
* </ul>
*
* \param hStream  - Stream to add callback to
* \param callback - The function to call once preceding stream operations are complete
* \param userData - User specified data to be passed to the callback function
* \param flags    - Reserved for future use, must be 0
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamWaitEvent,
* ::cuStreamDestroy,
* ::cuMemAllocManaged,
* ::cuStreamAttachMemAsync,
* ::cuLaunchHostFunc,
* ::cudaStreamAddCallback
*/
int handle_cuStreamAddCallback(void *conn) {
    CUstream hStream;
    CUstreamCallback callback;
    void* userData;
    unsigned int flags;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &callback, sizeof(CUstreamCallback)) < 0 ||
        rpc_read(conn, &userData, sizeof(void*)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamAddCallback(hStream, callback, &userData, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &userData, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Begins graph capture on a stream
*
* Begin graph capture on \p hStream. When a stream is in capture mode, all operations
* pushed into the stream will not be executed, but will instead be captured into
* a graph, which will be returned via ::cuStreamEndCapture. Capture may not be initiated
* if \p stream is CU_STREAM_LEGACY. Capture must be ended on the same stream in which
* it was initiated, and it may only be initiated if the stream is not already in capture
* mode. The capture mode may be queried via ::cuStreamIsCapturing. A unique id
* representing the capture sequence may be queried via ::cuStreamGetCaptureInfo.
*
* If \p mode is not ::CU_STREAM_CAPTURE_MODE_RELAXED, ::cuStreamEndCapture must be
* called on this stream from the same thread.
*
* \param hStream - Stream in which to initiate capture
* \param mode    - Controls the interaction of this capture sequence with other API
*                  calls that are potentially unsafe. For more details see
*                  ::cuThreadExchangeStreamCaptureMode.
*
* \note Kernels captured using this API must not use texture and surface references.
*       Reading or writing through any texture or surface reference is undefined
*       behavior. This restriction does not apply to texture and surface objects.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuStreamCreate,
* ::cuStreamIsCapturing,
* ::cuStreamEndCapture,
* ::cuThreadExchangeStreamCaptureMode
*/
int handle_cuStreamBeginCapture_v2(void *conn) {
    CUstream hStream;
    CUstreamCaptureMode mode;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &mode, sizeof(CUstreamCaptureMode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamBeginCapture_v2(hStream, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Swaps the stream capture interaction mode for a thread
*
* Sets the calling thread's stream capture interaction mode to the value contained
* in \p *mode, and overwrites \p *mode with the previous mode for the thread. To
* facilitate deterministic behavior across function or module boundaries, callers
* are encouraged to use this API in a push-pop fashion: \code
     CUstreamCaptureMode mode = desiredMode;
     cuThreadExchangeStreamCaptureMode(&mode);
     ...
     cuThreadExchangeStreamCaptureMode(&mode); // restore previous mode
* \endcode
*
* During stream capture (see ::cuStreamBeginCapture), some actions, such as a call
* to ::cudaMalloc, may be unsafe. In the case of ::cudaMalloc, the operation is
* not enqueued asynchronously to a stream, and is not observed by stream capture.
* Therefore, if the sequence of operations captured via ::cuStreamBeginCapture
* depended on the allocation being replayed whenever the graph is launched, the
* captured graph would be invalid.
*
* Therefore, stream capture places restrictions on API calls that can be made within
* or concurrently to a ::cuStreamBeginCapture-::cuStreamEndCapture sequence. This
* behavior can be controlled via this API and flags to ::cuStreamBeginCapture.
*
* A thread's mode is one of the following:
* - \p CU_STREAM_CAPTURE_MODE_GLOBAL: This is the default mode. If the local thread has
*   an ongoing capture sequence that was not initiated with
*   \p CU_STREAM_CAPTURE_MODE_RELAXED at \p cuStreamBeginCapture, or if any other thread
*   has a concurrent capture sequence initiated with \p CU_STREAM_CAPTURE_MODE_GLOBAL,
*   this thread is prohibited from potentially unsafe API calls.
* - \p CU_STREAM_CAPTURE_MODE_THREAD_LOCAL: If the local thread has an ongoing capture
*   sequence not initiated with \p CU_STREAM_CAPTURE_MODE_RELAXED, it is prohibited
*   from potentially unsafe API calls. Concurrent capture sequences in other threads
*   are ignored.
* - \p CU_STREAM_CAPTURE_MODE_RELAXED: The local thread is not prohibited from potentially
*   unsafe API calls. Note that the thread is still prohibited from API calls which
*   necessarily conflict with stream capture, for example, attempting ::cuEventQuery
*   on an event that was last recorded inside a capture sequence.
*
* \param mode - Pointer to mode value to swap with the current mode
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuStreamBeginCapture
*/
int handle_cuThreadExchangeStreamCaptureMode(void *conn) {
    CUstreamCaptureMode mode;

    if (rpc_read(conn, &mode, sizeof(CUstreamCaptureMode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuThreadExchangeStreamCaptureMode(&mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(CUstreamCaptureMode)) < 0)
        return -1;

    return result;
}

/**
* \brief Ends capture on a stream, returning the captured graph
*
* End capture on \p hStream, returning the captured graph via \p phGraph.
* Capture must have been initiated on \p hStream via a call to ::cuStreamBeginCapture.
* If capture was invalidated, due to a violation of the rules of stream capture, then
* a NULL graph will be returned.
*
* If the \p mode argument to ::cuStreamBeginCapture was not
* ::CU_STREAM_CAPTURE_MODE_RELAXED, this call must be from the same thread as
* ::cuStreamBeginCapture.
*
* \param hStream - Stream to query
* \param phGraph - The captured graph
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_STREAM_CAPTURE_WRONG_THREAD
* \notefnerr
*
* \sa
* ::cuStreamCreate,
* ::cuStreamBeginCapture,
* ::cuStreamIsCapturing
*/
int handle_cuStreamEndCapture(void *conn) {
    CUstream hStream;
    CUgraph phGraph;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &phGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamEndCapture(hStream, &phGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraph, sizeof(CUgraph)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a stream's capture status
*
* Return the capture status of \p hStream via \p captureStatus. After a successful
* call, \p *captureStatus will contain one of the following:
* - ::CU_STREAM_CAPTURE_STATUS_NONE: The stream is not capturing.
* - ::CU_STREAM_CAPTURE_STATUS_ACTIVE: The stream is capturing.
* - ::CU_STREAM_CAPTURE_STATUS_INVALIDATED: The stream was capturing but an error
*   has invalidated the capture sequence. The capture sequence must be terminated
*   with ::cuStreamEndCapture on the stream where it was initiated in order to
*   continue using \p hStream.
*
* Note that, if this is called on ::CU_STREAM_LEGACY (the "null stream") while
* a blocking stream in the same context is capturing, it will return
* ::CUDA_ERROR_STREAM_CAPTURE_IMPLICIT and \p *captureStatus is unspecified
* after the call. The blocking stream capture is not invalidated.
*
* When a blocking stream is capturing, the legacy stream is in an
* unusable state until the blocking stream capture is terminated. The legacy
* stream is not supported for stream capture, but attempted use would have an
* implicit dependency on the capturing stream(s).
*
* \param hStream       - Stream to query
* \param captureStatus - Returns the stream's capture status
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_STREAM_CAPTURE_IMPLICIT
* \notefnerr
*
* \sa
* ::cuStreamCreate,
* ::cuStreamBeginCapture,
* ::cuStreamEndCapture
*/
int handle_cuStreamIsCapturing(void *conn) {
    CUstream hStream;
    CUstreamCaptureStatus captureStatus;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &captureStatus, sizeof(CUstreamCaptureStatus)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamIsCapturing(hStream, &captureStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &captureStatus, sizeof(CUstreamCaptureStatus)) < 0)
        return -1;

    return result;
}

/**
* \brief Query a stream's capture state
*
* Query stream state related to stream capture.
*
* If called on ::CU_STREAM_LEGACY (the "null stream") while a stream not created 
* with ::CU_STREAM_NON_BLOCKING is capturing, returns ::CUDA_ERROR_STREAM_CAPTURE_IMPLICIT.
*
* Valid data (other than capture status) is returned only if both of the following are true:
* - the call returns CUDA_SUCCESS
* - the returned capture status is ::CU_STREAM_CAPTURE_STATUS_ACTIVE
*
* \param hStream - The stream to query
* \param captureStatus_out - Location to return the capture status of the stream; required
* \param id_out - Optional location to return an id for the capture sequence, which is
*           unique over the lifetime of the process
* \param graph_out - Optional location to return the graph being captured into. All
*           operations other than destroy and node removal are permitted on the graph
*           while the capture sequence is in progress. This API does not transfer
*           ownership of the graph, which is transferred or destroyed at
*           ::cuStreamEndCapture. Note that the graph handle may be invalidated before
*           end of capture for certain errors. Nodes that are or become
*           unreachable from the original stream at ::cuStreamEndCapture due to direct
*           actions on the graph do not trigger ::CUDA_ERROR_STREAM_CAPTURE_UNJOINED.
* \param dependencies_out - Optional location to store a pointer to an array of nodes.
*           The next node to be captured in the stream will depend on this set of nodes,
*           absent operations such as event wait which modify this set. The array pointer
*           is valid until the next API call which operates on the stream or until end of
*           capture. The node handles may be copied out and are valid until they or the
*           graph is destroyed. The driver-owned array may also be passed directly to
*           APIs that operate on the graph (not the stream) without copying.
* \param numDependencies_out - Optional location to store the size of the array
*           returned in dependencies_out.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_STREAM_CAPTURE_IMPLICIT
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuStreamBeginCapture,
* ::cuStreamIsCapturing,
* ::cuStreamUpdateCaptureDependencies
*/
int handle_cuStreamGetCaptureInfo_v2(void *conn) {
    CUstream hStream;
    CUstreamCaptureStatus captureStatus_out;
    cuuint64_t id_out;
    CUgraph graph_out;
    const CUgraphNode* dependencies_out;
    size_t numDependencies_out;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &captureStatus_out, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_read(conn, &id_out, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &graph_out, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies_out, sizeof(const CUgraphNode*)) < 0 ||
        rpc_read(conn, &numDependencies_out, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetCaptureInfo_v2(hStream, &captureStatus_out, &id_out, &graph_out, &dependencies_out, &numDependencies_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &captureStatus_out, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_write(conn, &id_out, sizeof(cuuint64_t)) < 0 ||
        rpc_write(conn, &graph_out, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies_out, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies_out, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Update the set of dependencies in a capturing stream (11.3+)
*
* Modifies the dependency set of a capturing stream. The dependency set is the set
* of nodes that the next captured node in the stream will depend on.
*
* Valid flags are ::CU_STREAM_ADD_CAPTURE_DEPENDENCIES and
* ::CU_STREAM_SET_CAPTURE_DEPENDENCIES. These control whether the set passed to
* the API is added to the existing set or replaces it. A flags value of 0 defaults
* to ::CU_STREAM_ADD_CAPTURE_DEPENDENCIES.
*
* Nodes that are removed from the dependency set via this API do not result in
* ::CUDA_ERROR_STREAM_CAPTURE_UNJOINED if they are unreachable from the stream at
* ::cuStreamEndCapture.
*
* Returns ::CUDA_ERROR_ILLEGAL_STATE if the stream is not capturing.
*
* This API is new in CUDA 11.3. Developers requiring compatibility across minor
* versions to CUDA 11.0 should not use this API or provide a fallback.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_ILLEGAL_STATE
*
* \sa
* ::cuStreamBeginCapture,
* ::cuStreamGetCaptureInfo,
*/
int handle_cuStreamUpdateCaptureDependencies(void *conn) {
    CUstream hStream;
    CUgraphNode dependencies;
    size_t numDependencies;
    unsigned int flags;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamUpdateCaptureDependencies(hStream, &dependencies, numDependencies, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dependencies, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Attach memory to a stream asynchronously
*
* Enqueues an operation in \p hStream to specify stream association of
* \p length bytes of memory starting from \p dptr. This function is a
* stream-ordered operation, meaning that it is dependent on, and will
* only take effect when, previous work in stream has completed. Any
* previous association is automatically replaced.
*
* \p dptr must point to one of the following types of memories:
* - managed memory declared using the __managed__ keyword or allocated with
*   ::cuMemAllocManaged.
* - a valid host-accessible region of system-allocated pageable memory. This
*   type of memory may only be specified if the device associated with the
*   stream reports a non-zero value for the device attribute
*   ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS.
*
* For managed allocations, \p length must be either zero or the entire
* allocation's size. Both indicate that the entire allocation's stream
* association is being changed. Currently, it is not possible to change stream
* association for a portion of a managed allocation.
*
* For pageable host allocations, \p length must be non-zero.
*
* The stream association is specified using \p flags which must be
* one of ::CUmemAttach_flags.
* If the ::CU_MEM_ATTACH_GLOBAL flag is specified, the memory can be accessed
* by any stream on any device.
* If the ::CU_MEM_ATTACH_HOST flag is specified, the program makes a guarantee
* that it won't access the memory on the device from any stream on a device that
* has a zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS.
* If the ::CU_MEM_ATTACH_SINGLE flag is specified and \p hStream is associated with
* a device that has a zero value for the device attribute ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS,
* the program makes a guarantee that it will only access the memory on the device
* from \p hStream. It is illegal to attach singly to the NULL stream, because the
* NULL stream is a virtual global stream and not a specific stream. An error will
* be returned in this case.
*
* When memory is associated with a single stream, the Unified Memory system will
* allow CPU access to this memory region so long as all operations in \p hStream
* have completed, regardless of whether other streams are active. In effect,
* this constrains exclusive ownership of the managed memory region by
* an active GPU to per-stream activity instead of whole-GPU activity.
*
* Accessing memory on the device from streams that are not associated with
* it will produce undefined results. No error checking is performed by the
* Unified Memory system to ensure that kernels launched into other streams
* do not access this region.
*
* It is a program's responsibility to order calls to ::cuStreamAttachMemAsync
* via events, synchronization or other means to ensure legal access to memory
* at all times. Data visibility and coherency will be changed appropriately
* for all kernels which follow a stream-association change.
*
* If \p hStream is destroyed while data is associated with it, the association is
* removed and the association reverts to the default visibility of the allocation
* as specified at ::cuMemAllocManaged. For __managed__ variables, the default
* association is always ::CU_MEM_ATTACH_GLOBAL. Note that destroying a stream is an
* asynchronous operation, and as a result, the change to default association won't
* happen until all work in the stream has completed.
*
* \param hStream - Stream in which to enqueue the attach operation
* \param dptr    - Pointer to memory (must be a pointer to managed memory or
*                  to a valid host-accessible region of system-allocated
*                  pageable memory)
* \param length  - Length of memory
* \param flags   - Must be one of ::CUmemAttach_flags
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamWaitEvent,
* ::cuStreamDestroy,
* ::cuMemAllocManaged,
* ::cudaStreamAttachMemAsync
*/
int handle_cuStreamAttachMemAsync(void *conn) {
    CUstream hStream;
    CUdeviceptr dptr;
    size_t length;
    unsigned int flags;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &length, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamAttachMemAsync(hStream, dptr, length, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Determine status of a compute stream
*
* Returns ::CUDA_SUCCESS if all operations in the stream specified by
* \p hStream have completed, or ::CUDA_ERROR_NOT_READY if not.
*
* For the purposes of Unified Memory, a return value of ::CUDA_SUCCESS
* is equivalent to having called ::cuStreamSynchronize().
*
* \param hStream - Stream to query status of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_READY
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamWaitEvent,
* ::cuStreamDestroy,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cudaStreamQuery
*/
int handle_cuStreamQuery(void *conn) {
    CUstream hStream;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamQuery(hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Wait until a stream's tasks are completed
*
* Waits until the device has completed all operations in the stream specified
* by \p hStream. If the context was created with the
* ::CU_CTX_SCHED_BLOCKING_SYNC flag, the CPU thread will block until the
* stream is finished with all of its tasks.
*
* \param hStream - Stream to wait for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamDestroy,
* ::cuStreamWaitEvent,
* ::cuStreamQuery,
* ::cuStreamAddCallback,
* ::cudaStreamSynchronize
*/
int handle_cuStreamSynchronize(void *conn) {
    CUstream hStream;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamSynchronize(hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a stream
*
* Destroys the stream specified by \p hStream.
*
* In case the device is still doing work in the stream \p hStream
* when ::cuStreamDestroy() is called, the function will return immediately
* and the resources associated with \p hStream will be released automatically
* once the device has completed all work in \p hStream.
*
* \param hStream - Stream to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamWaitEvent,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamAddCallback,
* ::cudaStreamDestroy
*/
int handle_cuStreamDestroy_v2(void *conn) {
    CUstream hStream;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamDestroy_v2(hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies attributes from source stream to destination stream.
*
* Copies attributes from source stream \p src to destination stream \p dst.
* Both streams must have the same context.
*
* \param[out] dst Destination stream
* \param[in] src Source stream
* For list of attributes see ::CUstreamAttrID
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuStreamCopyAttributes(void *conn) {
    CUstream dst;
    CUstream src;

    if (rpc_read(conn, &dst, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &src, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamCopyAttributes(dst, src);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries stream attribute.
*
* Queries attribute \p attr from \p hStream and stores it in corresponding
* member of \p value_out.
*
* \param[in] hStream
* \param[in] attr
* \param[out] value_out
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuStreamGetAttribute(void *conn) {
    CUstream hStream;
    CUstreamAttrID attr;
    CUstreamAttrValue value_out;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUstreamAttrID)) < 0 ||
        rpc_read(conn, &value_out, sizeof(CUstreamAttrValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamGetAttribute(hStream, attr, &value_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value_out, sizeof(CUstreamAttrValue)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets stream attribute.
*
* Sets attribute \p attr on \p hStream from corresponding attribute of
* \p value. The updated attribute will be applied to subsequent work
* submitted to the stream. It will not affect previously submitted work.
*
* \param[out] hStream
* \param[in] attr
* \param[in] value
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuStreamSetAttribute(void *conn) {
    CUstream hStream;
    CUstreamAttrID attr;
    CUstreamAttrValue value;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUstreamAttrID)) < 0 ||
        rpc_read(conn, &value, sizeof(CUstreamAttrValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamSetAttribute(hStream, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an event
*
* Creates an event *phEvent for the current context with the flags specified via
* \p Flags. Valid flags include:
* - ::CU_EVENT_DEFAULT: Default event creation flag.
* - ::CU_EVENT_BLOCKING_SYNC: Specifies that the created event should use blocking
*   synchronization.  A CPU thread that uses ::cuEventSynchronize() to wait on
*   an event created with this flag will block until the event has actually
*   been recorded.
* - ::CU_EVENT_DISABLE_TIMING: Specifies that the created event does not need
*   to record timing data.  Events created with this flag specified and
*   the ::CU_EVENT_BLOCKING_SYNC flag not specified will provide the best
*   performance when used with ::cuStreamWaitEvent() and ::cuEventQuery().
* - ::CU_EVENT_INTERPROCESS: Specifies that the created event may be used as an
*   interprocess event by ::cuIpcGetEventHandle(). ::CU_EVENT_INTERPROCESS must
*   be specified along with ::CU_EVENT_DISABLE_TIMING.
*
* \param phEvent - Returns newly created event
* \param Flags   - Event creation flags
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \notefnerr
*
* \sa
* ::cuEventRecord,
* ::cuEventQuery,
* ::cuEventSynchronize,
* ::cuEventDestroy,
* ::cuEventElapsedTime,
* ::cudaEventCreate,
* ::cudaEventCreateWithFlags
*/
int handle_cuEventCreate(void *conn) {
    CUevent phEvent;
    unsigned int Flags;

    if (rpc_read(conn, &phEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventCreate(&phEvent, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phEvent, sizeof(CUevent)) < 0)
        return -1;

    return result;
}

/**
* \brief Records an event
*
* Captures in \p hEvent the contents of \p hStream at the time of this call.
* \p hEvent and \p hStream must be from the same context.
* Calls such as ::cuEventQuery() or ::cuStreamWaitEvent() will then
* examine or wait for completion of the work that was captured. Uses of
* \p hStream after this call do not modify \p hEvent. See note on default
* stream behavior for what is captured in the default case.
*
* ::cuEventRecord() can be called multiple times on the same event and
* will overwrite the previously captured state. Other APIs such as
* ::cuStreamWaitEvent() use the most recently captured state at the time
* of the API call, and are not affected by later calls to
* ::cuEventRecord(). Before the first call to ::cuEventRecord(), an
* event represents an empty set of work, so for example ::cuEventQuery()
* would return ::CUDA_SUCCESS.
*
* \param hEvent  - Event to record
* \param hStream - Stream to record event for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \note_null_stream
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventQuery,
* ::cuEventSynchronize,
* ::cuStreamWaitEvent,
* ::cuEventDestroy,
* ::cuEventElapsedTime,
* ::cudaEventRecord,
* ::cuEventRecordWithFlags
*/
int handle_cuEventRecord(void *conn) {
    CUevent hEvent;
    CUstream hStream;

    if (rpc_read(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventRecord(hEvent, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Records an event
*
* Captures in \p hEvent the contents of \p hStream at the time of this call.
* \p hEvent and \p hStream must be from the same context.
* Calls such as ::cuEventQuery() or ::cuStreamWaitEvent() will then
* examine or wait for completion of the work that was captured. Uses of
* \p hStream after this call do not modify \p hEvent. See note on default
* stream behavior for what is captured in the default case.
*
* ::cuEventRecordWithFlags() can be called multiple times on the same event and
* will overwrite the previously captured state. Other APIs such as
* ::cuStreamWaitEvent() use the most recently captured state at the time
* of the API call, and are not affected by later calls to
* ::cuEventRecordWithFlags(). Before the first call to ::cuEventRecordWithFlags(), an
* event represents an empty set of work, so for example ::cuEventQuery()
* would return ::CUDA_SUCCESS.
*
* flags include:
* - ::CU_EVENT_RECORD_DEFAULT: Default event creation flag.
* - ::CU_EVENT_RECORD_EXTERNAL: Event is captured in the graph as an external
*   event node when performing stream capture. This flag is invalid outside
*   of stream capture.
*
* \param hEvent  - Event to record
* \param hStream - Stream to record event for
* \param flags   - See ::CUevent_capture_flags
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \note_null_stream
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventQuery,
* ::cuEventSynchronize,
* ::cuStreamWaitEvent,
* ::cuEventDestroy,
* ::cuEventElapsedTime,
* ::cuEventRecord,
* ::cudaEventRecord
*/
int handle_cuEventRecordWithFlags(void *conn) {
    CUevent hEvent;
    CUstream hStream;
    unsigned int flags;

    if (rpc_read(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventRecordWithFlags(hEvent, hStream, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries an event's status
*
* Queries the status of all work currently captured by \p hEvent. See
* ::cuEventRecord() for details on what is captured by an event.
*
* Returns ::CUDA_SUCCESS if all captured work has been completed, or
* ::CUDA_ERROR_NOT_READY if any captured work is incomplete.
*
* For the purposes of Unified Memory, a return value of ::CUDA_SUCCESS
* is equivalent to having called ::cuEventSynchronize().
*
* \param hEvent - Event to query
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_READY
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventRecord,
* ::cuEventSynchronize,
* ::cuEventDestroy,
* ::cuEventElapsedTime,
* ::cudaEventQuery
*/
int handle_cuEventQuery(void *conn) {
    CUevent hEvent;

    if (rpc_read(conn, &hEvent, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventQuery(hEvent);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Waits for an event to complete
*
* Waits until the completion of all work currently captured in \p hEvent.
* See ::cuEventRecord() for details on what is captured by an event.
*
* Waiting for an event that was created with the ::CU_EVENT_BLOCKING_SYNC
* flag will cause the calling CPU thread to block until the event has
* been completed by the device.  If the ::CU_EVENT_BLOCKING_SYNC flag has
* not been set, then the CPU thread will busy-wait until the event has
* been completed by the device.
*
* \param hEvent - Event to wait for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventRecord,
* ::cuEventQuery,
* ::cuEventDestroy,
* ::cuEventElapsedTime,
* ::cudaEventSynchronize
*/
int handle_cuEventSynchronize(void *conn) {
    CUevent hEvent;

    if (rpc_read(conn, &hEvent, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventSynchronize(hEvent);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an event
*
* Destroys the event specified by \p hEvent.
*
* An event may be destroyed before it is complete (i.e., while
* ::cuEventQuery() would return ::CUDA_ERROR_NOT_READY). In this case, the
* call does not block on completion of the event, and any associated
* resources will automatically be released asynchronously at completion.
*
* \param hEvent - Event to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventRecord,
* ::cuEventQuery,
* ::cuEventSynchronize,
* ::cuEventElapsedTime,
* ::cudaEventDestroy
*/
int handle_cuEventDestroy_v2(void *conn) {
    CUevent hEvent;

    if (rpc_read(conn, &hEvent, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventDestroy_v2(hEvent);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Computes the elapsed time between two events
*
* Computes the elapsed time between two events (in milliseconds with a
* resolution of around 0.5 microseconds).
*
* If either event was last recorded in a non-NULL stream, the resulting time
* may be greater than expected (even if both used the same stream handle). This
* happens because the ::cuEventRecord() operation takes place asynchronously
* and there is no guarantee that the measured latency is actually just between
* the two events. Any number of other different stream operations could execute
* in between the two measured events, thus altering the timing in a significant
* way.
*
* If ::cuEventRecord() has not been called on either event then
* ::CUDA_ERROR_INVALID_HANDLE is returned. If ::cuEventRecord() has been called
* on both events but one or both of them has not yet been completed (that is,
* ::cuEventQuery() would return ::CUDA_ERROR_NOT_READY on at least one of the
* events), ::CUDA_ERROR_NOT_READY is returned. If either event was created with
* the ::CU_EVENT_DISABLE_TIMING flag, then this function will return
* ::CUDA_ERROR_INVALID_HANDLE.
*
* \param pMilliseconds - Time between \p hStart and \p hEnd in ms
* \param hStart        - Starting event
* \param hEnd          - Ending event
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_READY,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa ::cuEventCreate,
* ::cuEventRecord,
* ::cuEventQuery,
* ::cuEventSynchronize,
* ::cuEventDestroy,
* ::cudaEventElapsedTime
*/
int handle_cuEventElapsedTime(void *conn) {
    float pMilliseconds;
    CUevent hStart;
    CUevent hEnd;

    if (rpc_read(conn, &pMilliseconds, sizeof(float)) < 0 ||
        rpc_read(conn, &hStart, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &hEnd, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuEventElapsedTime(&pMilliseconds, hStart, hEnd);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pMilliseconds, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Imports an external memory object
*
* Imports an externally allocated memory object and returns
* a handle to that in \p extMem_out.
*
* The properties of the handle being imported must be described in
* \p memHandleDesc. The ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC structure
* is defined as follows:
*
* \code
        typedef struct CUDA_EXTERNAL_MEMORY_HANDLE_DESC_st {
            CUexternalMemoryHandleType type;
            union {
                int fd;
                struct {
                    void *handle;
                    const void *name;
                } win32;
                const void *nvSciBufObject;
            } handle;
            unsigned long long size;
            unsigned int flags;
        } CUDA_EXTERNAL_MEMORY_HANDLE_DESC;
* \endcode
*
* where ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type specifies the type
* of handle being imported. ::CUexternalMemoryHandleType is
* defined as:
*
* \code
        typedef enum CUexternalMemoryHandleType_enum {
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD          = 1,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32       = 2,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT   = 3,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D12_HEAP         = 4,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D12_RESOURCE     = 5,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE     = 6,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE_KMT = 7,
            CU_EXTERNAL_MEMORY_HANDLE_TYPE_NVSCIBUF           = 8
        } CUexternalMemoryHandleType;
* \endcode
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD, then
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::fd must be a valid
* file descriptor referencing a memory object. Ownership of
* the file descriptor is transferred to the CUDA driver when the
* handle is imported successfully. Performing any operations on the
* file descriptor after it is imported results in undefined behavior.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32, then exactly one
* of ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name must not be
* NULL. If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a memory object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a memory object.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT, then
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle must
* be non-NULL and
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* must be NULL. The handle specified must be a globally shared KMT
* handle. This handle does not hold a reference to the underlying
* object, and thus will be invalid when all references to the
* memory object are destroyed.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D12_HEAP, then exactly one
* of ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name must not be
* NULL. If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Heap object. This handle holds a reference to the underlying
* object. If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D12Heap object.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D12_RESOURCE, then exactly one
* of ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name must not be
* NULL. If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Resource object. This handle holds a reference to the
* underlying object. If
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D12Resource object.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE, then
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle must
* represent a valid shared NT handle that is returned by
* IDXGIResource1::CreateSharedHandle when referring to a
* ID3D11Resource object. If
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D11Resource object.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE_KMT, then
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::handle must
* represent a valid shared KMT handle that is returned by
* IDXGIResource::GetSharedHandle when referring to a
* ID3D11Resource object and
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::win32::name
* must be NULL.
*
* If ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type is
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_NVSCIBUF, then
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::handle::nvSciBufObject must be non-NULL
* and reference a valid NvSciBuf object.
* If the NvSciBuf object imported into CUDA is also mapped by other drivers, then the
* application must use ::cuWaitExternalSemaphoresAsync or ::cuSignalExternalSemaphoresAsync
* as appropriate barriers to maintain coherence between CUDA and the other drivers.
* See ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_SKIP_NVSCIBUF_MEMSYNC and ::CUDA_EXTERNAL_SEMAPHORE_WAIT_SKIP_NVSCIBUF_MEMSYNC
* for memory synchronization.
*
*
* The size of the memory object must be specified in
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::size.
*
* Specifying the flag ::CUDA_EXTERNAL_MEMORY_DEDICATED in
* ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::flags indicates that the
* resource is a dedicated resource. The definition of what a
* dedicated resource is outside the scope of this extension.
* This flag must be set if ::CUDA_EXTERNAL_MEMORY_HANDLE_DESC::type
* is one of the following:
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D12_RESOURCE
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_D3D11_RESOURCE_KMT
*
* \param extMem_out    - Returned handle to an external memory object
* \param memHandleDesc - Memory import handle descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OPERATING_SYSTEM
* \notefnerr
*
* \note If the Vulkan memory imported into CUDA is mapped on the CPU then the
* application must use vkInvalidateMappedMemoryRanges/vkFlushMappedMemoryRanges
* as well as appropriate Vulkan pipeline barriers to maintain coherence between
* CPU and GPU. For more information on these APIs, please refer to "Synchronization
* and Cache Control" chapter from Vulkan specification.
*
* \sa ::cuDestroyExternalMemory,
* ::cuExternalMemoryGetMappedBuffer,
* ::cuExternalMemoryGetMappedMipmappedArray
*/
int handle_cuImportExternalMemory(void *conn) {
    CUexternalMemory extMem_out;
    CUDA_EXTERNAL_MEMORY_HANDLE_DESC memHandleDesc;

    if (rpc_read(conn, &extMem_out, sizeof(CUexternalMemory)) < 0 ||
        rpc_read(conn, &memHandleDesc, sizeof(CUDA_EXTERNAL_MEMORY_HANDLE_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuImportExternalMemory(&extMem_out, &memHandleDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &extMem_out, sizeof(CUexternalMemory)) < 0)
        return -1;

    return result;
}

/**
* \brief Maps a buffer onto an imported memory object
*
* Maps a buffer onto an imported memory object and returns a device
* pointer in \p devPtr.
*
* The properties of the buffer being mapped must be described in
* \p bufferDesc. The ::CUDA_EXTERNAL_MEMORY_BUFFER_DESC structure is
* defined as follows:
*
* \code
        typedef struct CUDA_EXTERNAL_MEMORY_BUFFER_DESC_st {
            unsigned long long offset;
            unsigned long long size;
            unsigned int flags;
        } CUDA_EXTERNAL_MEMORY_BUFFER_DESC;
* \endcode
*
* where ::CUDA_EXTERNAL_MEMORY_BUFFER_DESC::offset is the offset in
* the memory object where the buffer's base address is.
* ::CUDA_EXTERNAL_MEMORY_BUFFER_DESC::size is the size of the buffer.
* ::CUDA_EXTERNAL_MEMORY_BUFFER_DESC::flags must be zero.
*
* The offset and size have to be suitably aligned to match the
* requirements of the external API. Mapping two buffers whose ranges
* overlap may or may not result in the same virtual address being
* returned for the overlapped portion. In such cases, the application
* must ensure that all accesses to that region from the GPU are
* volatile. Otherwise writes made via one address are not guaranteed
* to be visible via the other address, even if they're issued by the
* same thread. It is recommended that applications map the combined
* range instead of mapping separate buffers and then apply the
* appropriate offsets to the returned pointer to derive the
* individual buffers.
*
* The returned pointer \p devPtr must be freed using ::cuMemFree.
*
* \param devPtr     - Returned device pointer to buffer
* \param extMem     - Handle to external memory object
* \param bufferDesc - Buffer descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuImportExternalMemory,
* ::cuDestroyExternalMemory,
* ::cuExternalMemoryGetMappedMipmappedArray
*/
int handle_cuExternalMemoryGetMappedBuffer(void *conn) {
    CUdeviceptr devPtr;
    CUexternalMemory extMem;
    CUDA_EXTERNAL_MEMORY_BUFFER_DESC bufferDesc;

    if (rpc_read(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &extMem, sizeof(CUexternalMemory)) < 0 ||
        rpc_read(conn, &bufferDesc, sizeof(CUDA_EXTERNAL_MEMORY_BUFFER_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuExternalMemoryGetMappedBuffer(&devPtr, extMem, &bufferDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Maps a CUDA mipmapped array onto an external memory object
*
* Maps a CUDA mipmapped array onto an external object and returns a
* handle to it in \p mipmap.
*
* The properties of the CUDA mipmapped array being mapped must be
* described in \p mipmapDesc. The structure
* ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC is defined as follows:
*
* \code
        typedef struct CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_st {
            unsigned long long offset;
            CUDA_ARRAY3D_DESCRIPTOR arrayDesc;
            unsigned int numLevels;
        } CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC;
* \endcode
*
* where ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC::offset is the
* offset in the memory object where the base level of the mipmap
* chain is.
* ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC::arrayDesc describes
* the format, dimensions and type of the base level of the mipmap
* chain. For further details on these parameters, please refer to the
* documentation for ::cuMipmappedArrayCreate. Note that if the mipmapped
* array is bound as a color target in the graphics API, then the flag
* ::CUDA_ARRAY3D_COLOR_ATTACHMENT must be specified in
* ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC::arrayDesc::Flags.
* ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC::numLevels specifies
* the total number of levels in the mipmap chain.
*
* If \p extMem was imported from a handle of type ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_NVSCIBUF, then
* ::CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC::numLevels must be equal to 1.
*
* The returned CUDA mipmapped array must be freed using ::cuMipmappedArrayDestroy.
*
* \param mipmap     - Returned CUDA mipmapped array
* \param extMem     - Handle to external memory object
* \param mipmapDesc - CUDA array descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuImportExternalMemory,
* ::cuDestroyExternalMemory,
* ::cuExternalMemoryGetMappedBuffer
*/
int handle_cuExternalMemoryGetMappedMipmappedArray(void *conn) {
    CUmipmappedArray mipmap;
    CUexternalMemory extMem;
    CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC mipmapDesc;

    if (rpc_read(conn, &mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &extMem, sizeof(CUexternalMemory)) < 0 ||
        rpc_read(conn, &mipmapDesc, sizeof(CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &mipmapDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mipmap, sizeof(CUmipmappedArray)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an external memory object.
*
* Destroys the specified external memory object. Any existing buffers
* and CUDA mipmapped arrays mapped onto this object must no longer be
* used and must be explicitly freed using ::cuMemFree and
* ::cuMipmappedArrayDestroy respectively.
*
* \param extMem - External memory object to be destroyed
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuImportExternalMemory,
* ::cuExternalMemoryGetMappedBuffer,
* ::cuExternalMemoryGetMappedMipmappedArray
*/
int handle_cuDestroyExternalMemory(void *conn) {
    CUexternalMemory extMem;

    if (rpc_read(conn, &extMem, sizeof(CUexternalMemory)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDestroyExternalMemory(extMem);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Imports an external semaphore
*
* Imports an externally allocated synchronization object and returns
* a handle to that in \p extSem_out.
*
* The properties of the handle being imported must be described in
* \p semHandleDesc. The ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC is
* defined as follows:
*
* \code
        typedef struct CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_st {
            CUexternalSemaphoreHandleType type;
            union {
                int fd;
                struct {
                    void *handle;
                    const void *name;
                } win32;
                const void* NvSciSyncObj;
            } handle;
            unsigned int flags;
        } CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC;
* \endcode
*
* where ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type specifies the type of
* handle being imported. ::CUexternalSemaphoreHandleType is defined
* as:
*
* \code
        typedef enum CUexternalSemaphoreHandleType_enum {
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD                = 1,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32             = 2,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT         = 3,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D12_FENCE              = 4,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_FENCE              = 5,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC                = 6,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX        = 7,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX_KMT    = 8,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_FD    = 9,
            CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_WIN32 = 10
        } CUexternalSemaphoreHandleType;
* \endcode
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::fd must be a valid
* file descriptor referencing a synchronization object. Ownership of
* the file descriptor is transferred to the CUDA driver when the
* handle is imported successfully. Performing any operations on the
* file descriptor after it is imported results in undefined behavior.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32, then exactly one
* of ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name must not be
* NULL. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a synchronization object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* is not NULL, then it must name a valid synchronization object.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle must
* be non-NULL and
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* must be NULL. The handle specified must be a globally shared KMT
* handle. This handle does not hold a reference to the underlying
* object, and thus will be invalid when all references to the
* synchronization object are destroyed.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D12_FENCE, then exactly one
* of ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name must not be
* NULL. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Fence object. This handle holds a reference to the underlying
* object. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* is not NULL, then it must name a valid synchronization object that
* refers to a valid ID3D12Fence object.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_FENCE, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* represents a valid shared NT handle that is returned by
* ID3D11Fence::CreateSharedHandle. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* is not NULL, then it must name a valid synchronization object that
* refers to a valid ID3D11Fence object.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::nvSciSyncObj
* represents a valid NvSciSyncObj.
*
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* represents a valid shared NT handle that
* is returned by IDXGIResource1::CreateSharedHandle when referring to
* a IDXGIKeyedMutex object. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* is not NULL, then it must name a valid synchronization object that
* refers to a valid IDXGIKeyedMutex object.
*
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX_KMT, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* represents a valid shared KMT handle that
* is returned by IDXGIResource::GetSharedHandle when referring to
* a IDXGIKeyedMutex object and
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name must be NULL.
* 
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_FD, then
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::fd must be a valid
* file descriptor referencing a synchronization object. Ownership of
* the file descriptor is transferred to the CUDA driver when the
* handle is imported successfully. Performing any operations on the
* file descriptor after it is imported results in undefined behavior.
* 
* If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::type is
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_WIN32, then exactly one
* of ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle and
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name must not be
* NULL. If
* ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a synchronization object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC::handle::win32::name
* is not NULL, then it must name a valid synchronization object.
*
* \param extSem_out    - Returned handle to an external semaphore
* \param semHandleDesc - Semaphore import handle descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OPERATING_SYSTEM
* \notefnerr
*
* \sa ::cuDestroyExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuImportExternalSemaphore(void *conn) {
    CUexternalSemaphore extSem_out;
    CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC semHandleDesc;

    if (rpc_read(conn, &extSem_out, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_read(conn, &semHandleDesc, sizeof(CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuImportExternalSemaphore(&extSem_out, &semHandleDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &extSem_out, sizeof(CUexternalSemaphore)) < 0)
        return -1;

    return result;
}

/**
* \brief Signals a set of external semaphore objects
*
* Enqueues a signal operation on a set of externally allocated
* semaphore object in the specified stream. The operations will be
* executed when all prior operations in the stream complete.
*
* The exact semantics of signaling a semaphore depends on the type of
* the object.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT
* then signaling the semaphore will set it to the signaled state.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D12_FENCE,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_FENCE,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_FD,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_WIN32
* then the semaphore will be set to the value specified in
* ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS::params::fence::value.
*
* If the semaphore object is of the type ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC
* this API sets ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS::params::nvSciSync::fence
* to a value that can be used by subsequent waiters of the same NvSciSync object
* to order operations with those currently submitted in \p stream. Such an update
* will overwrite previous contents of
* ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS::params::nvSciSync::fence. By default,
* signaling such an external semaphore object causes appropriate memory synchronization
* operations to be performed over all external memory objects that are imported as
* ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_NVSCIBUF. This ensures that any subsequent accesses
* made by other importers of the same set of NvSciBuf memory object(s) are coherent.
* These operations can be skipped by specifying the flag
* ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_SKIP_NVSCIBUF_MEMSYNC, which can be used as a
* performance optimization when data coherency is not required. But specifying this
* flag in scenarios where data coherency is required results in undefined behavior.
* Also, for semaphore object of the type ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC,
* if the NvSciSyncAttrList used to create the NvSciSyncObj had not set the flags in
* ::cuDeviceGetNvSciSyncAttributes to CUDA_NVSCISYNC_ATTR_SIGNAL, this API will return
* CUDA_ERROR_NOT_SUPPORTED.
* NvSciSyncFence associated with semaphore object of the type 
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC can be deterministic. For this the 
* NvSciSyncAttrList used to create the semaphore object must have value of 
* NvSciSyncAttrKey_RequireDeterministicFences key set to true. Deterministic fences 
* allow users to enqueue a wait over the semaphore object even before corresponding
* signal is enqueued. For such a semaphore object, CUDA guarantees that each signal 
* operation will increment the fence value by '1'. Users are expected to track count 
* of signals enqueued on the semaphore object and insert waits accordingly. When such 
* a semaphore object is signaled from multiple streams, due to concurrent stream 
* execution, it is possible that the order in which the semaphore gets signaled is 
* indeterministic. This could lead to waiters of the semaphore getting unblocked 
* incorrectly. Users are expected to handle such situations, either by not using the 
* same semaphore object with deterministic fence support enabled in different streams 
* or by adding explicit dependency amongst such streams so that the semaphore is 
* signaled in order.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX_KMT
* then the keyed mutex will be released with the key specified in
* ::CUDA_EXTERNAL_SEMAPHORE_PARAMS::params::keyedmutex::key.
*
* \param extSemArray - Set of external semaphores to be signaled
* \param paramsArray - Array of semaphore parameters
* \param numExtSems  - Number of semaphores to signal
* \param stream      - Stream to enqueue the signal operations in
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuImportExternalSemaphore,
* ::cuDestroyExternalSemaphore,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuSignalExternalSemaphoresAsync(void *conn) {
    CUexternalSemaphore extSemArray;
    CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS paramsArray;
    unsigned int numExtSems;
    CUstream stream;

    if (rpc_read(conn, &extSemArray, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_read(conn, &paramsArray, sizeof(CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS)) < 0 ||
        rpc_read(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSignalExternalSemaphoresAsync(&extSemArray, &paramsArray, numExtSems, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Waits on a set of external semaphore objects
*
* Enqueues a wait operation on a set of externally allocated
* semaphore object in the specified stream. The operations will be
* executed when all prior operations in the stream complete.
*
* The exact semantics of waiting on a semaphore depends on the type
* of the object.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT
* then waiting on the semaphore will wait until the semaphore reaches
* the signaled state. The semaphore will then be reset to the
* unsignaled state. Therefore for every signal operation, there can
* only be one wait operation.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D12_FENCE,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_FENCE,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_FD,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_TIMELINE_SEMAPHORE_WIN32
* then waiting on the semaphore will wait until the value of the
* semaphore is greater than or equal to
* ::CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS::params::fence::value.
*
* If the semaphore object is of the type ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC
* then, waiting on the semaphore will wait until the
* ::CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS::params::nvSciSync::fence is signaled by the
* signaler of the NvSciSyncObj that was associated with this semaphore object.
* By default, waiting on such an external semaphore object causes appropriate
* memory synchronization operations to be performed over all external memory objects
* that are imported as ::CU_EXTERNAL_MEMORY_HANDLE_TYPE_NVSCIBUF. This ensures that
* any subsequent accesses made by other importers of the same set of NvSciBuf memory
* object(s) are coherent. These operations can be skipped by specifying the flag
* ::CUDA_EXTERNAL_SEMAPHORE_WAIT_SKIP_NVSCIBUF_MEMSYNC, which can be used as a
* performance optimization when data coherency is not required. But specifying this
* flag in scenarios where data coherency is required results in undefined behavior.
* Also, for semaphore object of the type ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_NVSCISYNC,
* if the NvSciSyncAttrList used to create the NvSciSyncObj had not set the flags in
* ::cuDeviceGetNvSciSyncAttributes to CUDA_NVSCISYNC_ATTR_WAIT, this API will return
* CUDA_ERROR_NOT_SUPPORTED.
*
* If the semaphore object is any one of the following types:
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX,
* ::CU_EXTERNAL_SEMAPHORE_HANDLE_TYPE_D3D11_KEYED_MUTEX_KMT
* then the keyed mutex will be acquired when it is released with the key 
* specified in ::CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS::params::keyedmutex::key 
* or until the timeout specified by
* ::CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS::params::keyedmutex::timeoutMs
* has lapsed. The timeout interval can either be a finite value
* specified in milliseconds or an infinite value. In case an infinite
* value is specified the timeout never elapses. The windows INFINITE
* macro must be used to specify infinite timeout.
*
* \param extSemArray - External semaphores to be waited on
* \param paramsArray - Array of semaphore parameters
* \param numExtSems  - Number of semaphores to wait on
* \param stream      - Stream to enqueue the wait operations in
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_TIMEOUT
* \notefnerr
*
* \sa ::cuImportExternalSemaphore,
* ::cuDestroyExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync
*/
int handle_cuWaitExternalSemaphoresAsync(void *conn) {
    CUexternalSemaphore extSemArray;
    CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS paramsArray;
    unsigned int numExtSems;
    CUstream stream;

    if (rpc_read(conn, &extSemArray, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_read(conn, &paramsArray, sizeof(CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS)) < 0 ||
        rpc_read(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuWaitExternalSemaphoresAsync(&extSemArray, &paramsArray, numExtSems, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an external semaphore
*
* Destroys an external semaphore object and releases any references
* to the underlying resource. Any outstanding signals or waits must
* have completed before the semaphore is destroyed.
*
* \param extSem - External semaphore to be destroyed
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa ::cuImportExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuDestroyExternalSemaphore(void *conn) {
    CUexternalSemaphore extSem;

    if (rpc_read(conn, &extSem, sizeof(CUexternalSemaphore)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDestroyExternalSemaphore(extSem);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Wait on a memory location
*
* Enqueues a synchronization of the stream on the given memory location. Work
* ordered after the operation will block until the given condition on the
* memory is satisfied. By default, the condition is to wait for
* (int32_t)(*addr - value) >= 0, a cyclic greater-or-equal.
* Other condition types can be specified via \p flags.
*
* If the memory was registered via ::cuMemHostRegister(), the device pointer
* should be obtained with ::cuMemHostGetDevicePointer(). This function cannot
* be used with managed memory (::cuMemAllocManaged).
*
* Support for CU_STREAM_WAIT_VALUE_NOR can be queried with ::cuDeviceGetAttribute() and
* ::CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR_V2.
*
* \note
* Warning:
* Improper use of this API may deadlock the application. Synchronization 
* ordering established through this API is not visible to CUDA. CUDA tasks 
* that are (even indirectly) ordered by this API should also have that order
* expressed with CUDA-visible dependencies such as events. This ensures that 
* the scheduler does not serialize them in an improper order. For more 
* information, see the Stream Memory Operations section in the programming 
* guide(https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).
*
* \param stream The stream to synchronize on the memory location.
* \param addr The memory location to wait on.
* \param value The value to compare with the memory location.
* \param flags See ::CUstreamWaitValue_flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuStreamWaitValue64,
* ::cuStreamWriteValue32,
* ::cuStreamWriteValue64,
* ::cuStreamBatchMemOp,
* ::cuMemHostRegister,
* ::cuStreamWaitEvent
*/
int handle_cuStreamWaitValue32_v2(void *conn) {
    CUstream stream;
    CUdeviceptr addr;
    cuuint32_t value;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &value, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamWaitValue32_v2(stream, addr, value, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Wait on a memory location
*
* Enqueues a synchronization of the stream on the given memory location. Work
* ordered after the operation will block until the given condition on the
* memory is satisfied. By default, the condition is to wait for
* (int64_t)(*addr - value) >= 0, a cyclic greater-or-equal.
* Other condition types can be specified via \p flags.
*
* If the memory was registered via ::cuMemHostRegister(), the device pointer
* should be obtained with ::cuMemHostGetDevicePointer().
*
* Support for this can be queried with ::cuDeviceGetAttribute() and
* ::CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS.
*
* \note
* Warning:
* Improper use of this API may deadlock the application. Synchronization 
* ordering established through this API is not visible to CUDA. CUDA tasks 
* that are (even indirectly) ordered by this API should also have that order
* expressed with CUDA-visible dependencies such as events. This ensures that 
* the scheduler does not serialize them in an improper order. For more 
* information, see the Stream Memory Operations section in the programming 
* guide(https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).
*
* \param stream The stream to synchronize on the memory location.
* \param addr The memory location to wait on.
* \param value The value to compare with the memory location.
* \param flags See ::CUstreamWaitValue_flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuStreamWaitValue32,
* ::cuStreamWriteValue32,
* ::cuStreamWriteValue64,
* ::cuStreamBatchMemOp,
* ::cuMemHostRegister,
* ::cuStreamWaitEvent
*/
int handle_cuStreamWaitValue64_v2(void *conn) {
    CUstream stream;
    CUdeviceptr addr;
    cuuint64_t value;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &value, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamWaitValue64_v2(stream, addr, value, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Write a value to memory
*
* Write a value to memory.
*
* If the memory was registered via ::cuMemHostRegister(), the device pointer
* should be obtained with ::cuMemHostGetDevicePointer(). This function cannot
* be used with managed memory (::cuMemAllocManaged).
*
* \param stream The stream to do the write in.
* \param addr The device address to write to.
* \param value The value to write.
* \param flags See ::CUstreamWriteValue_flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuStreamWriteValue64,
* ::cuStreamWaitValue32,
* ::cuStreamWaitValue64,
* ::cuStreamBatchMemOp,
* ::cuMemHostRegister,
* ::cuEventRecord
*/
int handle_cuStreamWriteValue32_v2(void *conn) {
    CUstream stream;
    CUdeviceptr addr;
    cuuint32_t value;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &value, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamWriteValue32_v2(stream, addr, value, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Write a value to memory
*
* Write a value to memory.
*
* If the memory was registered via ::cuMemHostRegister(), the device pointer
* should be obtained with ::cuMemHostGetDevicePointer().
*
* Support for this can be queried with ::cuDeviceGetAttribute() and
* ::CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS.
*
* \param stream The stream to do the write in.
* \param addr The device address to write to.
* \param value The value to write.
* \param flags See ::CUstreamWriteValue_flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuStreamWriteValue32,
* ::cuStreamWaitValue32,
* ::cuStreamWaitValue64,
* ::cuStreamBatchMemOp,
* ::cuMemHostRegister,
* ::cuEventRecord
*/
int handle_cuStreamWriteValue64_v2(void *conn) {
    CUstream stream;
    CUdeviceptr addr;
    cuuint64_t value;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &value, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamWriteValue64_v2(stream, addr, value, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Batch operations to synchronize the stream via memory operations
*
* This is a batch version of ::cuStreamWaitValue32() and ::cuStreamWriteValue32().
* Batching operations may avoid some performance overhead in both the API call
* and the device execution versus adding them to the stream in separate API
* calls. The operations are enqueued in the order they appear in the array.
*
* See ::CUstreamBatchMemOpType for the full set of supported operations, and
* ::cuStreamWaitValue32(), ::cuStreamWaitValue64(), ::cuStreamWriteValue32(),
* and ::cuStreamWriteValue64() for details of specific operations.
*
* See related APIs for details on querying support for specific operations.
*
* \note
* Warning:
* Improper use of this API may deadlock the application. Synchronization 
* ordering established through this API is not visible to CUDA. CUDA tasks 
* that are (even indirectly) ordered by this API should also have that order
* expressed with CUDA-visible dependencies such as events. This ensures that 
* the scheduler does not serialize them in an improper order. For more 
* information, see the Stream Memory Operations section in the programming 
* guide(https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).
*
* \param stream The stream to enqueue the operations in.
* \param count The number of operations in the array. Must be less than 256.
* \param paramArray The types and parameters of the individual operations.
* \param flags Reserved for future expansion; must be 0.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \notefnerr
*
* \sa ::cuStreamWaitValue32,
* ::cuStreamWaitValue64,
* ::cuStreamWriteValue32,
* ::cuStreamWriteValue64,
* ::cuMemHostRegister
*/
int handle_cuStreamBatchMemOp_v2(void *conn) {
    CUstream stream;
    unsigned int count;
    CUstreamBatchMemOpParams paramArray;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &paramArray, sizeof(CUstreamBatchMemOpParams)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuStreamBatchMemOp_v2(stream, count, &paramArray, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &paramArray, sizeof(CUstreamBatchMemOpParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about a function
*
* Returns in \p *pi the integer value of the attribute \p attrib on the kernel
* given by \p hfunc. The supported attributes are:
* - ::CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK: The maximum number of threads
*   per block, beyond which a launch of the function would fail. This number
*   depends on both the function and the device on which the function is
*   currently loaded.
* - ::CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES: The size in bytes of
*   statically-allocated shared memory per block required by this function.
*   This does not include dynamically-allocated shared memory requested by
*   the user at runtime.
* - ::CU_FUNC_ATTRIBUTE_CONST_SIZE_BYTES: The size in bytes of user-allocated
*   constant memory required by this function.
* - ::CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES: The size in bytes of local memory
*   used by each thread of this function.
* - ::CU_FUNC_ATTRIBUTE_NUM_REGS: The number of registers used by each thread
*   of this function.
* - ::CU_FUNC_ATTRIBUTE_PTX_VERSION: The PTX virtual architecture version for
*   which the function was compiled. This value is the major PTX version * 10
*   + the minor PTX version, so a PTX version 1.3 function would return the
*   value 13. Note that this may return the undefined value of 0 for cubins
*   compiled prior to CUDA 3.0.
* - ::CU_FUNC_ATTRIBUTE_BINARY_VERSION: The binary architecture version for
*   which the function was compiled. This value is the major binary
*   version * 10 + the minor binary version, so a binary version 1.3 function
*   would return the value 13. Note that this will return a value of 10 for
*   legacy cubins that do not have a properly-encoded binary architecture
*   version.
* - ::CU_FUNC_CACHE_MODE_CA: The attribute to indicate whether the function has
*   been compiled with user specified option "-Xptxas --dlcm=ca" set .
* - ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES: The maximum size in bytes of
*   dynamically-allocated shared memory.
* - ::CU_FUNC_ATTRIBUTE_PREFERRED_SHARED_MEMORY_CARVEOUT: Preferred shared memory-L1
*   cache split ratio in percent of total shared memory.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SIZE_MUST_BE_SET: If this attribute is set, the
*   kernel must launch with a valid cluster size specified.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_WIDTH: The required cluster width in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_HEIGHT: The required cluster height in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_DEPTH: The required cluster depth in
*   blocks.
* - ::CU_FUNC_ATTRIBUTE_NON_PORTABLE_CLUSTER_SIZE_ALLOWED: Indicates whether
*   the function can be launched with non-portable cluster size. 1 is allowed,
*   0 is disallowed. A non-portable cluster size may only function on the
*   specific SKUs the program is tested on. The launch might fail if the
*   program is run on a different hardware platform. CUDA API provides
*   cudaOccupancyMaxActiveClusters to assist with checking whether the desired
*   size can be launched on the current device. A portable cluster size is
*   guaranteed to be functional on all compute capabilities higher than the
*   target compute capability. The portable cluster size for sm_90 is 8 blocks
*   per cluster. This value may increase for future compute capabilities. The
*   specific hardware unit may support higher cluster sizes that’s not
*   guaranteed to be portable.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE: The block
*   scheduling policy of a function. The value type is CUclusterSchedulingPolicy.
*
* \param pi     - Returned attribute value
* \param attrib - Attribute requested
* \param hfunc  - Function to query attribute of
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuLaunchKernel,
* ::cudaFuncGetAttributes,
* ::cudaFuncSetAttribute,
* ::cuKernelGetAttribute
*/
int handle_cuFuncGetAttribute(void *conn) {
    int pi;
    CUfunction_attribute attrib;
    CUfunction hfunc;

    if (rpc_read(conn, &pi, sizeof(int)) < 0 ||
        rpc_read(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncGetAttribute(&pi, attrib, hfunc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pi, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets information about a function
*
* This call sets the value of a specified attribute \p attrib on the kernel given
* by \p hfunc to an integer value specified by \p val
* This function returns CUDA_SUCCESS if the new value of the attribute could be
* successfully set. If the set fails, this call will return an error.
* Not all attributes can have values set. Attempting to set a value on a read-only
* attribute will result in an error (CUDA_ERROR_INVALID_VALUE)
*
* Supported attributes for the cuFuncSetAttribute call are:
* - ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES: This maximum size in bytes of
*   dynamically-allocated shared memory. The value should contain the requested
*   maximum size of dynamically-allocated shared memory. The sum of this value and
*   the function attribute ::CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES cannot exceed the
*   device attribute ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN.
*   The maximal size of requestable dynamic shared memory may differ by GPU
*   architecture.
* - ::CU_FUNC_ATTRIBUTE_PREFERRED_SHARED_MEMORY_CARVEOUT: On devices where the L1
*   cache and shared memory use the same hardware resources, this sets the shared memory
*   carveout preference, in percent of the total shared memory. 
*   See ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR
*   This is only a hint, and the driver can choose a different ratio if required to execute the function.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_WIDTH: The required cluster width in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_HEIGHT: The required cluster height in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_REQUIRED_CLUSTER_DEPTH: The required cluster depth in
*   blocks. The width, height, and depth values must either all be 0 or all be
*   positive. The validity of the cluster dimensions is checked at launch time.
*   If the value is set during compile time, it cannot be set at runtime.
*   Setting it at runtime will return CUDA_ERROR_NOT_PERMITTED.
* - ::CU_FUNC_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE: The block
*   scheduling policy of a function. The value type is CUclusterSchedulingPolicy.
*
* \param hfunc  - Function to query attribute of
* \param attrib - Attribute requested
* \param value   - The value to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuLaunchKernel,
* ::cudaFuncGetAttributes,
* ::cudaFuncSetAttribute,
* ::cuKernelSetAttribute
*/
int handle_cuFuncSetAttribute(void *conn) {
    CUfunction hfunc;
    CUfunction_attribute attrib;
    int value;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncSetAttribute(hfunc, attrib, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for a device function
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p config the preferred cache configuration for
* the device function \p hfunc. This is only a preference. The driver will use
* the requested configuration if possible, but it is free to choose a different
* configuration if required to execute \p hfunc.  Any context-wide preference
* set via ::cuCtxSetCacheConfig() will be overridden by this per-function
* setting unless the per-function setting is ::CU_FUNC_CACHE_PREFER_NONE. In
* that case, the current context-wide setting will be used.
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
*
* The supported cache configurations are:
* - ::CU_FUNC_CACHE_PREFER_NONE: no preference for shared memory or L1 (default)
* - ::CU_FUNC_CACHE_PREFER_SHARED: prefer larger shared memory and smaller L1 cache
* - ::CU_FUNC_CACHE_PREFER_L1: prefer larger L1 cache and smaller shared memory
* - ::CU_FUNC_CACHE_PREFER_EQUAL: prefer equal sized L1 cache and shared memory
*
* \param hfunc  - Kernel to configure cache for
* \param config - Requested cache configuration
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cuLaunchKernel,
* ::cudaFuncSetCacheConfig,
* ::cuKernelSetCacheConfig
*/
int handle_cuFuncSetCacheConfig(void *conn) {
    CUfunction hfunc;
    CUfunc_cache config;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &config, sizeof(CUfunc_cache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncSetCacheConfig(hfunc, config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the shared memory configuration for a device function.
*
* On devices with configurable shared memory banks, this function will
* force all subsequent launches of the specified device function to have
* the given shared memory bank size configuration. On any given launch of the
* function, the shared memory configuration of the device will be temporarily
* changed if needed to suit the function's preferred configuration. Changes in
* shared memory configuration between subsequent launches of functions,
* may introduce a device side synchronization point.
*
* Any per-function setting of shared memory bank size set via
* ::cuFuncSetSharedMemConfig will override the context wide setting set with
* ::cuCtxSetSharedMemConfig.
*
* Changing the shared memory bank size will not increase shared memory usage
* or affect occupancy of kernels, but may have major effects on performance.
* Larger bank sizes will allow for greater potential bandwidth to shared memory,
* but will change what kinds of accesses to shared memory will result in bank
* conflicts.
*
* This function will do nothing on devices with fixed shared memory bank size.
*
* The supported bank configurations are:
* - ::CU_SHARED_MEM_CONFIG_DEFAULT_BANK_SIZE: use the context's shared memory
*   configuration when launching this function.
* - ::CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE: set shared memory bank width to
*   be natively four bytes when launching this function.
* - ::CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE: set shared memory bank width to
*   be natively eight bytes when launching this function.
*
* \param hfunc  - kernel to be given a shared memory config
* \param config - requested shared memory configuration
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuCtxGetSharedMemConfig,
* ::cuCtxSetSharedMemConfig,
* ::cuFuncGetAttribute,
* ::cuLaunchKernel,
* ::cudaFuncSetSharedMemConfig
*/
int handle_cuFuncSetSharedMemConfig(void *conn) {
    CUfunction hfunc;
    CUsharedconfig config;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &config, sizeof(CUsharedconfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncSetSharedMemConfig(hfunc, config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a module handle
*
* Returns in \p *hmod the handle of the module that function \p hfunc
* is located in. The lifetime of the module corresponds to the lifetime of
* the context it was loaded in or until the module is explicitly unloaded.
*
* The CUDA runtime manages its own modules loaded into the primary context.
* If the handle returned by this API refers to a module loaded by the CUDA runtime,
* calling ::cuModuleUnload() on that module will result in undefined behavior.
*
* \param hmod - Returned module handle
* \param hfunc   - Function to retrieve module for
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_FOUND
* \notefnerr
*
*/
int handle_cuFuncGetModule(void *conn) {
    CUmodule hmod;
    CUfunction hfunc;

    if (rpc_read(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncGetModule(&hmod, hfunc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &hmod, sizeof(CUmodule)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function ::CUfunction or a CUDA kernel ::CUkernel
*
* Invokes the function ::CUfunction or the kernel ::CUkernel \p f
* on a \p gridDimX x \p gridDimY x \p gridDimZ grid of blocks.
* Each block contains \p blockDimX x \p blockDimY x
* \p blockDimZ threads.
*
* \p sharedMemBytes sets the amount of dynamic shared memory that will be
* available to each thread block.
*
* Kernel parameters to \p f can be specified in one of two ways:
*
* 1) Kernel parameters can be specified via \p kernelParams.  If \p f
* has N parameters, then \p kernelParams needs to be an array of N
* pointers.  Each of \p kernelParams[0] through \p kernelParams[N-1]
* must point to a region of memory from which the actual kernel
* parameter will be copied.  The number of kernel parameters and their
* offsets and sizes do not need to be specified as that information is
* retrieved directly from the kernel's image.
*
* 2) Kernel parameters can also be packaged by the application into
* a single buffer that is passed in via the \p extra parameter.
* This places the burden on the application of knowing each kernel
* parameter's size and alignment/padding within the buffer.  Here is
* an example of using the \p extra parameter in this manner:
* \code
    size_t argBufferSize;
    char argBuffer[256];
    // populate argBuffer and argBufferSize
    void *config[] = {
        CU_LAUNCH_PARAM_BUFFER_POINTER, argBuffer,
        CU_LAUNCH_PARAM_BUFFER_SIZE,    &argBufferSize,
        CU_LAUNCH_PARAM_END
    };
    status = cuLaunchKernel(f, gx, gy, gz, bx, by, bz, sh, s, NULL, config);
* \endcode
*
* The \p extra parameter exists to allow ::cuLaunchKernel to take
* additional less commonly used arguments.  \p extra specifies a list of
* names of extra settings and their corresponding values.  Each extra
* setting name is immediately followed by the corresponding value.  The
* list must be terminated with either NULL or ::CU_LAUNCH_PARAM_END.
*
* - ::CU_LAUNCH_PARAM_END, which indicates the end of the \p extra
*   array;
* - ::CU_LAUNCH_PARAM_BUFFER_POINTER, which specifies that the next
*   value in \p extra will be a pointer to a buffer containing all
*   the kernel parameters for launching kernel \p f;
* - ::CU_LAUNCH_PARAM_BUFFER_SIZE, which specifies that the next
*   value in \p extra will be a pointer to a size_t containing the
*   size of the buffer specified with ::CU_LAUNCH_PARAM_BUFFER_POINTER;
*
* The error ::CUDA_ERROR_INVALID_VALUE will be returned if kernel
* parameters are specified with both \p kernelParams and \p extra
* (i.e. both \p kernelParams and \p extra are non-NULL).
*
* Calling ::cuLaunchKernel() invalidates the persistent function state
* set through the following deprecated APIs:
*  ::cuFuncSetBlockShape(),
*  ::cuFuncSetSharedSize(),
*  ::cuParamSetSize(),
*  ::cuParamSeti(),
*  ::cuParamSetf(),
*  ::cuParamSetv().
*
* Note that to use ::cuLaunchKernel(), the kernel \p f must either have
* been compiled with toolchain version 3.2 or later so that it will
* contain kernel parameter information, or have no kernel parameters.
* If either of these conditions is not met, then ::cuLaunchKernel() will
* return ::CUDA_ERROR_INVALID_IMAGE.
*
* Note that the API can also be used to launch context-less kernel ::CUkernel
* by querying the handle using ::cuLibraryGetKernel() and then passing it
* to the API by casting to ::CUfunction. Here, the context to launch
* the kernel on will either be taken from the specified stream \p hStream
* or the current context in case of NULL stream.
*
* \param f              - Function ::CUfunction or Kernel ::CUkernel to launch
* \param gridDimX       - Width of grid in blocks
* \param gridDimY       - Height of grid in blocks
* \param gridDimZ       - Depth of grid in blocks
* \param blockDimX      - X dimension of each thread block
* \param blockDimY      - Y dimension of each thread block
* \param blockDimZ      - Z dimension of each thread block
* \param sharedMemBytes - Dynamic shared-memory size per thread block in bytes
* \param hStream        - Stream identifier
* \param kernelParams   - Array of pointers to kernel parameters
* \param extra          - Extra options
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_NOT_FOUND
* \note_null_stream
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cudaLaunchKernel,
* ::cuLibraryGetKernel,
* ::cuKernelSetCacheConfig,
* ::cuKernelGetAttribute,
* ::cuKernelSetAttribute
*/
int handle_cuLaunchKernel(void *conn) {
    CUfunction f;
    unsigned int gridDimX;
    unsigned int gridDimY;
    unsigned int gridDimZ;
    unsigned int blockDimX;
    unsigned int blockDimY;
    unsigned int blockDimZ;
    unsigned int sharedMemBytes;
    CUstream hStream;
    void* kernelParams;
    void* extra;

    if (rpc_read(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &gridDimX, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gridDimY, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gridDimZ, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimX, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimY, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimZ, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &sharedMemBytes, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &kernelParams, sizeof(void*)) < 0 ||
        rpc_read(conn, &extra, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchKernel(f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, &kernelParams, &extra);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &kernelParams, sizeof(void*)) < 0 ||
        rpc_write(conn, &extra, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function ::CUfunction or a CUDA kernel ::CUkernel with launch-time configuration
*
* Invokes the function ::CUfunction or the kernel ::CUkernel \p f with the specified launch-time configuration
* \p config.
*
* The ::CUlaunchConfig structure is defined as:
* \code
        typedef struct CUlaunchConfig_st {
            unsigned int gridDimX;
            unsigned int gridDimY;
            unsigned int gridDimZ;
            unsigned int blockDimX;
            unsigned int blockDimY;
            unsigned int blockDimZ;
            unsigned int sharedMemBytes;
            CUstream hStream;
            CUlaunchAttribute *attrs;
            unsigned int numAttrs;
        } CUlaunchConfig;
* \endcode
* where:
* - ::CUlaunchConfig::gridDimX is the width of the grid in blocks.
* - ::CUlaunchConfig::gridDimY is the height of the grid in blocks.
* - ::CUlaunchConfig::gridDimZ is the depth of the grid in blocks.
* - ::CUlaunchConfig::blockDimX is the X dimension of each thread block.
* - ::CUlaunchConfig::blockDimX is the Y dimension of each thread block.
* - ::CUlaunchConfig::blockDimZ is the Z dimension of each thread block.
* - ::CUlaunchConfig::sharedMemBytes is the dynamic shared-memory size per
*   thread block in bytes.
* - ::CUlaunchConfig::hStream is the handle to the stream to perform the launch
*   in. The CUDA context associated with this stream must match that associated
*   with function f.
* - ::CUlaunchConfig::attrs is an array of ::CUlaunchConfig::numAttrs
*   continguous ::CUlaunchAttribute elements. The value of this pointer is not
*   considered if ::CUlaunchConfig::numAttrs is zero. However, in that case, it
*   is recommended to set the pointer to NULL.
* - ::CUlaunchConfig::numAttrs is the numbers of attributes populating the
*   first ::CUlaunchConfig::numAttrs positions of the ::CUlaunchConfig::attrs
*   array.
*
* Launch-time configuration is specified by adding entries to
* ::CUlaunchConfig::attrs. Each entry is an attribute ID and a corresponding
* attribute value.
*
* The ::CUlaunchAttribute structure is defined as:
* \code
        typedef struct CUlaunchAttribute_st {
            CUlaunchAttributeID id;
            CUlaunchAttributeValue value;
        } CUlaunchAttribute;
* \endcode
* where:
* - ::CUlaunchAttribute::id is a unique enum identifying the attribute.
* - ::CUlaunchAttribute::value is a union that hold the attribute value.
*
* An example of using the \p config parameter:
* \code
        CUlaunchAttribute coopAttr = {.id = CU_LAUNCH_ATTRIBUTE_COOPERATIVE,
                                      .value = 1};
        CUlaunchConfig config = {... // set block and grid dimensions
                                 .attrs = &coopAttr,
                                 .numAttrs = 1};
        cuLaunchKernelEx(&config, kernel, NULL, NULL);
* \endcode
*
* The ::CUlaunchAttributeID enum is defined as:
* \code
        typedef enum CUlaunchAttributeID_enum {
            CU_LAUNCH_ATTRIBUTE_IGNORE = 0,
            CU_LAUNCH_ATTRIBUTE_ACCESS_POLICY_WINDOW   = 1,
            CU_LAUNCH_ATTRIBUTE_COOPERATIVE            = 2,
            CU_LAUNCH_ATTRIBUTE_SYNCHRONIZATION_POLICY = 3,
            CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION                    = 4,
            CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE = 5,
            CU_LAUNCH_ATTRIBUTE_PROGRAMMATIC_STREAM_SERIALIZATION    = 6,
            CU_LAUNCH_ATTRIBUTE_PROGRAMMATIC_EVENT                   = 7,
        } CUlaunchAttributeID;
* \endcode
*
* and the corresponding ::CUlaunchAttributeValue union as :
* \code
        typedef union CUlaunchAttributeValue_union {
            cuuint64_t pad[8];
            CUaccessPolicyWindow accessPolicyWindow;
            int cooperative;
            CUsynchronizationPolicy syncPolicy;
            struct {
                unsigned int x;
                unsigned int y;
                unsigned int z;
            } clusterDim;
            CUclusterSchedulingPolicy clusterSchedulingPolicyPreference;
            int programmaticStreamSerializationAllowed;
            struct {
                CUevent event;
                int flags;
                int triggerAtBlockStart;
            } programmaticEvent;
        } CUlaunchAttributeValue;
* \endcode
*
* Setting ::CU_LAUNCH_ATTRIBUTE_COOPERATIVE to a non-zero value causes the
* kernel launch to be a cooperative launch, with exactly the same usage and
* semantics of ::cuLaunchCooperativeKernel.
*
* Setting ::CU_LAUNCH_ATTRIBUTE_PROGRAMMATIC_STREAM_SERIALIZATION to a non-zero
* values causes the kernel to use programmatic means to resolve its stream
* dependency -- enabling the CUDA runtime to opportunistically allow the grid's
* execution to overlap with the previous kernel in the stream, if that kernel
* requests the overlap.
*
* ::CU_LAUNCH_ATTRIBUTE_PROGRAMMATIC_EVENT records an event along with the
* kernel launch. Event recorded through this launch attribute is guaranteed to
* only trigger after all block in the associated kernel trigger the event. A
* block can trigger the event through PTX launchdep.release or CUDA builtin
* function cudaTriggerProgrammaticLaunchCompletion(). A trigger can also be
* inserted at the beginning of each block's execution if triggerAtBlockStart is
* set to non-0. Note that dependents (including the CPU thread calling
* cuEventSynchronize()) are not guaranteed to observe the release precisely
* when it is released. For example, cuEventSynchronize() may only observe the
* event trigger long after the associated kernel has completed. This recording
* type is primarily meant for establishing programmatic dependency between
* device tasks. The event supplied must not be an interprocess or interop
* event. The event must disable timing (i.e. created with
* ::CU_EVENT_DISABLE_TIMING flag set).
*
* The effect of other attributes is consistent with their effect when set via
* persistent APIs.
*
* See ::cuStreamSetAttribute for
* - ::CU_LAUNCH_ATTRIBUTE_ACCESS_POLICY_WINDOW
* - ::CU_LAUNCH_ATTRIBUTE_SYNCHRONIZATION_POLICY
*
* See ::cuFunctionSetAttribute for
* - ::CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION
* - ::CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE
*
* Kernel parameters to \p f can be specified in the same ways that they can be
* using ::cuLaunchKernel.
*
* Note that the API can also be used to launch context-less kernel ::CUkernel
* by querying the handle using ::cuLibraryGetKernel() and then passing it
* to the API by casting to ::CUfunction. Here, the context to launch
* the kernel on will either be taken from the specified stream ::CUlaunchConfig::hStream
* or the current context in case of NULL stream.
*
* \param config         - Config to launch
* \param f              - Function ::CUfunction or Kernel ::CUkernel to launch
* \param kernelParams   - Array of pointers to kernel parameters
* \param extra          - Extra options
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_NOT_FOUND
* \note_null_stream
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cudaLaunchKernel,
* ::cudaLaunchKernelEx,
* ::cuLibraryGetKernel,
* ::cuKernelSetCacheConfig,
* ::cuKernelGetAttribute,
* ::cuKernelSetAttribute
*/
int handle_cuLaunchKernelEx(void *conn) {
    CUlaunchConfig config;
    CUfunction f;
    void* kernelParams;
    void* extra;

    if (rpc_read(conn, &config, sizeof(CUlaunchConfig)) < 0 ||
        rpc_read(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &kernelParams, sizeof(void*)) < 0 ||
        rpc_read(conn, &extra, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchKernelEx(&config, f, &kernelParams, &extra);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &kernelParams, sizeof(void*)) < 0 ||
        rpc_write(conn, &extra, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function ::CUfunction or a CUDA kernel ::CUkernel where thread blocks
* can cooperate and synchronize as they execute
*
* Invokes the function ::CUfunction or the kernel ::CUkernel \p f on a \p gridDimX x \p gridDimY x \p gridDimZ
* grid of blocks. Each block contains \p blockDimX x \p blockDimY x
* \p blockDimZ threads.
*
* Note that the API can also be used to launch context-less kernel ::CUkernel
* by querying the handle using ::cuLibraryGetKernel() and then passing it
* to the API by casting to ::CUfunction. Here, the context to launch
* the kernel on will either be taken from the specified stream \p hStream
* or the current context in case of NULL stream.
*
* \p sharedMemBytes sets the amount of dynamic shared memory that will be
* available to each thread block.
*
* The device on which this kernel is invoked must have a non-zero value for
* the device attribute ::CU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH.
*
* The total number of blocks launched cannot exceed the maximum number of blocks per
* multiprocessor as returned by ::cuOccupancyMaxActiveBlocksPerMultiprocessor (or
* ::cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
* as specified by the device attribute ::CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT.
*
* The kernel cannot make use of CUDA dynamic parallelism.
*
* Kernel parameters must be specified via \p kernelParams.  If \p f
* has N parameters, then \p kernelParams needs to be an array of N
* pointers.  Each of \p kernelParams[0] through \p kernelParams[N-1]
* must point to a region of memory from which the actual kernel
* parameter will be copied.  The number of kernel parameters and their
* offsets and sizes do not need to be specified as that information is
* retrieved directly from the kernel's image.
*
* Calling ::cuLaunchCooperativeKernel() sets persistent function state that is
* the same as function state set through ::cuLaunchKernel API
*
* When the kernel \p f is launched via ::cuLaunchCooperativeKernel(), the previous
* block shape, shared size and parameter info associated with \p f
* is overwritten.
*
* Note that to use ::cuLaunchCooperativeKernel(), the kernel \p f must either have
* been compiled with toolchain version 3.2 or later so that it will
* contain kernel parameter information, or have no kernel parameters.
* If either of these conditions is not met, then ::cuLaunchCooperativeKernel() will
* return ::CUDA_ERROR_INVALID_IMAGE.
*
* Note that the API can also be used to launch context-less kernel ::CUkernel
* by querying the handle using ::cuLibraryGetKernel() and then passing it
* to the API by casting to ::CUfunction. Here, the context to launch
* the kernel on will either be taken from the specified stream \p hStream
* or the current context in case of NULL stream.
*
* \param f              - Function ::CUfunction or Kernel ::CUkernel to launch
* \param gridDimX       - Width of grid in blocks
* \param gridDimY       - Height of grid in blocks
* \param gridDimZ       - Depth of grid in blocks
* \param blockDimX      - X dimension of each thread block
* \param blockDimY      - Y dimension of each thread block
* \param blockDimZ      - Z dimension of each thread block
* \param sharedMemBytes - Dynamic shared-memory size per thread block in bytes
* \param hStream        - Stream identifier
* \param kernelParams   - Array of pointers to kernel parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
* ::CUDA_ERROR_NOT_FOUND
* \note_null_stream
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cuLaunchCooperativeKernelMultiDevice,
* ::cudaLaunchCooperativeKernel,
* ::cuLibraryGetKernel,
* ::cuKernelSetCacheConfig,
* ::cuKernelGetAttribute,
* ::cuKernelSetAttribute
*/
int handle_cuLaunchCooperativeKernel(void *conn) {
    CUfunction f;
    unsigned int gridDimX;
    unsigned int gridDimY;
    unsigned int gridDimZ;
    unsigned int blockDimX;
    unsigned int blockDimY;
    unsigned int blockDimZ;
    unsigned int sharedMemBytes;
    CUstream hStream;
    void* kernelParams;

    if (rpc_read(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &gridDimX, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gridDimY, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &gridDimZ, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimX, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimY, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &blockDimZ, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &sharedMemBytes, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &kernelParams, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchCooperativeKernel(f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, &kernelParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &kernelParams, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches CUDA functions on multiple devices where thread blocks can cooperate and synchronize as they execute
*
* \deprecated This function is deprecated as of CUDA 11.3.
*
* Invokes kernels as specified in the \p launchParamsList array where each element
* of the array specifies all the parameters required to perform a single kernel launch.
* These kernels can cooperate and synchronize as they execute. The size of the array is
* specified by \p numDevices.
*
* No two kernels can be launched on the same device. All the devices targeted by this
* multi-device launch must be identical. All devices must have a non-zero value for the
* device attribute ::CU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH.
*
* All kernels launched must be identical with respect to the compiled code. Note that
* any __device__, __constant__ or __managed__ variables present in the module that owns
* the kernel launched on each device, are independently instantiated on every device.
* It is the application's responsibility to ensure these variables are initialized and
* used appropriately.
*
* The size of the grids as specified in blocks, the size of the blocks themselves
* and the amount of shared memory used by each thread block must also match across
* all launched kernels.
*
* The streams used to launch these kernels must have been created via either ::cuStreamCreate
* or ::cuStreamCreateWithPriority. The NULL stream or ::CU_STREAM_LEGACY or ::CU_STREAM_PER_THREAD
* cannot be used.
*
* The total number of blocks launched per kernel cannot exceed the maximum number of blocks
* per multiprocessor as returned by ::cuOccupancyMaxActiveBlocksPerMultiprocessor (or
* ::cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
* as specified by the device attribute ::CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT. Since the
* total number of blocks launched per device has to match across all devices, the maximum
* number of blocks that can be launched per device will be limited by the device with the
* least number of multiprocessors.
*
* The kernels cannot make use of CUDA dynamic parallelism.
*
* The ::CUDA_LAUNCH_PARAMS structure is defined as:
* \code
        typedef struct CUDA_LAUNCH_PARAMS_st
        {
            CUfunction function;
            unsigned int gridDimX;
            unsigned int gridDimY;
            unsigned int gridDimZ;
            unsigned int blockDimX;
            unsigned int blockDimY;
            unsigned int blockDimZ;
            unsigned int sharedMemBytes;
            CUstream hStream;
            void **kernelParams;
        } CUDA_LAUNCH_PARAMS;
* \endcode
* where:
* - ::CUDA_LAUNCH_PARAMS::function specifies the kernel to be launched. All functions must
*   be identical with respect to the compiled code.
*   Note that you can also specify context-less kernel ::CUkernel by querying the handle
*   using ::cuLibraryGetKernel() and then casting to ::CUfunction. In this case, the context to
*   launch the kernel on be taken from the specified stream ::CUDA_LAUNCH_PARAMS::hStream.
* - ::CUDA_LAUNCH_PARAMS::gridDimX is the width of the grid in blocks. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::gridDimY is the height of the grid in blocks. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::gridDimZ is the depth of the grid in blocks. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::blockDimX is the X dimension of each thread block. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::blockDimX is the Y dimension of each thread block. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::blockDimZ is the Z dimension of each thread block. This must match across
*   all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::sharedMemBytes is the dynamic shared-memory size per thread block in bytes.
*   This must match across all kernels launched.
* - ::CUDA_LAUNCH_PARAMS::hStream is the handle to the stream to perform the launch in. This cannot
*   be the NULL stream or ::CU_STREAM_LEGACY or ::CU_STREAM_PER_THREAD. The CUDA context associated
*   with this stream must match that associated with ::CUDA_LAUNCH_PARAMS::function.
* - ::CUDA_LAUNCH_PARAMS::kernelParams is an array of pointers to kernel parameters. If
*   ::CUDA_LAUNCH_PARAMS::function has N parameters, then ::CUDA_LAUNCH_PARAMS::kernelParams
*   needs to be an array of N pointers. Each of ::CUDA_LAUNCH_PARAMS::kernelParams[0] through
*   ::CUDA_LAUNCH_PARAMS::kernelParams[N-1] must point to a region of memory from which the actual
*   kernel parameter will be copied. The number of kernel parameters and their offsets and sizes
*   do not need to be specified as that information is retrieved directly from the kernel's image.
*
* By default, the kernel won't begin execution on any GPU until all prior work in all the specified
* streams has completed. This behavior can be overridden by specifying the flag
* ::CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_PRE_LAUNCH_SYNC. When this flag is specified, each kernel
* will only wait for prior work in the stream corresponding to that GPU to complete before it begins
* execution.
*
* Similarly, by default, any subsequent work pushed in any of the specified streams will not begin
* execution until the kernels on all GPUs have completed. This behavior can be overridden by specifying
* the flag ::CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_POST_LAUNCH_SYNC. When this flag is specified,
* any subsequent work pushed in any of the specified streams will only wait for the kernel launched
* on the GPU corresponding to that stream to complete before it begins execution.
*
* Calling ::cuLaunchCooperativeKernelMultiDevice() sets persistent function state that is
* the same as function state set through ::cuLaunchKernel API when called individually for each
* element in \p launchParamsList.
*
* When kernels are launched via ::cuLaunchCooperativeKernelMultiDevice(), the previous
* block shape, shared size and parameter info associated with each ::CUDA_LAUNCH_PARAMS::function
* in \p launchParamsList is overwritten.
*
* Note that to use ::cuLaunchCooperativeKernelMultiDevice(), the kernels must either have
* been compiled with toolchain version 3.2 or later so that it will
* contain kernel parameter information, or have no kernel parameters.
* If either of these conditions is not met, then ::cuLaunchCooperativeKernelMultiDevice() will
* return ::CUDA_ERROR_INVALID_IMAGE.
*
* \param launchParamsList - List of launch parameters, one per device
* \param numDevices       - Size of the \p launchParamsList array
* \param flags            - Flags to control launch behavior
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_IMAGE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED
* \note_null_stream
* \notefnerr
*
* \sa ::cuCtxGetCacheConfig,
* ::cuCtxSetCacheConfig,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cuLaunchCooperativeKernel,
* ::cudaLaunchCooperativeKernelMultiDevice
*/
int handle_cuLaunchCooperativeKernelMultiDevice(void *conn) {
    CUDA_LAUNCH_PARAMS launchParamsList;
    unsigned int numDevices;
    unsigned int flags;

    if (rpc_read(conn, &launchParamsList, sizeof(CUDA_LAUNCH_PARAMS)) < 0 ||
        rpc_read(conn, &numDevices, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchCooperativeKernelMultiDevice(&launchParamsList, numDevices, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &launchParamsList, sizeof(CUDA_LAUNCH_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Enqueues a host function call in a stream
*
* Enqueues a host function to run in a stream.  The function will be called
* after currently enqueued work and will block work added after it.
*
* The host function must not make any CUDA API calls.  Attempting to use a
* CUDA API may result in ::CUDA_ERROR_NOT_PERMITTED, but this is not required.
* The host function must not perform any synchronization that may depend on
* outstanding CUDA work not mandated to run earlier.  Host functions without a
* mandated order (such as in independent streams) execute in undefined order
* and may be serialized.
*
* For the purposes of Unified Memory, execution makes a number of guarantees:
* <ul>
*   <li>The stream is considered idle for the duration of the function's
*   execution.  Thus, for example, the function may always use memory attached
*   to the stream it was enqueued in.</li>
*   <li>The start of execution of the function has the same effect as
*   synchronizing an event recorded in the same stream immediately prior to
*   the function.  It thus synchronizes streams which have been "joined"
*   prior to the function.</li>
*   <li>Adding device work to any stream does not have the effect of making
*   the stream active until all preceding host functions and stream callbacks
*   have executed.  Thus, for
*   example, a function might use global attached memory even if work has
*   been added to another stream, if the work has been ordered behind the
*   function call with an event.</li>
*   <li>Completion of the function does not cause a stream to become
*   active except as described above.  The stream will remain idle
*   if no device work follows the function, and will remain idle across
*   consecutive host functions or stream callbacks without device work in
*   between.  Thus, for example,
*   stream synchronization can be done by signaling from a host function at the
*   end of the stream.</li>
* </ul>
*
* Note that, in contrast to ::cuStreamAddCallback, the function will not be
* called in the event of an error in the CUDA context.
*
* \param hStream  - Stream to enqueue function call in
* \param fn       - The function to call once preceding stream operations are complete
* \param userData - User-specified data to be passed to the function
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_SUPPORTED
* \note_null_stream
* \notefnerr
*
* \sa ::cuStreamCreate,
* ::cuStreamQuery,
* ::cuStreamSynchronize,
* ::cuStreamWaitEvent,
* ::cuStreamDestroy,
* ::cuMemAllocManaged,
* ::cuStreamAttachMemAsync,
* ::cuStreamAddCallback
*/
int handle_cuLaunchHostFunc(void *conn) {
    CUstream hStream;
    CUhostFn fn;
    void* userData;

    if (rpc_read(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &fn, sizeof(CUhostFn)) < 0 ||
        rpc_read(conn, &userData, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchHostFunc(hStream, fn, &userData);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &userData, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the block-dimensions for the function
*
* \deprecated
*
* Specifies the \p x, \p y, and \p z dimensions of the thread blocks that are
* created when the kernel given by \p hfunc is launched.
*
* \param hfunc - Kernel to specify dimensions of
* \param x     - X dimension
* \param y     - Y dimension
* \param z     - Z dimension
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetSharedSize,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSeti,
* ::cuParamSetf,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuFuncSetBlockShape(void *conn) {
    CUfunction hfunc;
    int x;
    int y;
    int z;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &x, sizeof(int)) < 0 ||
        rpc_read(conn, &y, sizeof(int)) < 0 ||
        rpc_read(conn, &z, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncSetBlockShape(hfunc, x, y, z);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the dynamic shared-memory size for the function
*
* \deprecated
*
* Sets through \p bytes the amount of dynamic shared memory that will be
* available to each thread block when the kernel given by \p hfunc is launched.
*
* \param hfunc - Kernel to specify dynamic shared-memory size for
* \param bytes - Dynamic shared-memory size per thread in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetCacheConfig,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSeti,
* ::cuParamSetf,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuFuncSetSharedSize(void *conn) {
    CUfunction hfunc;
    unsigned int bytes;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &bytes, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuFuncSetSharedSize(hfunc, bytes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameter size for the function
*
* \deprecated
*
* Sets through \p numbytes the total size in bytes needed by the function
* parameters of the kernel corresponding to \p hfunc.
*
* \param hfunc    - Kernel to set parameter size for
* \param numbytes - Size of parameter list in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetf,
* ::cuParamSeti,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuParamSetSize(void *conn) {
    CUfunction hfunc;
    unsigned int numbytes;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &numbytes, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuParamSetSize(hfunc, numbytes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Adds an integer parameter to the function's argument list
*
* \deprecated
*
* Sets an integer parameter that will be specified the next time the
* kernel corresponding to \p hfunc will be invoked. \p offset is a byte offset.
*
* \param hfunc  - Kernel to add parameter to
* \param offset - Offset to add parameter to argument list
* \param value  - Value of parameter
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSetf,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuParamSeti(void *conn) {
    CUfunction hfunc;
    int offset;
    unsigned int value;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0 ||
        rpc_read(conn, &value, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuParamSeti(hfunc, offset, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Adds a floating-point parameter to the function's argument list
*
* \deprecated
*
* Sets a floating-point parameter that will be specified the next time the
* kernel corresponding to \p hfunc will be invoked. \p offset is a byte offset.
*
* \param hfunc  - Kernel to add parameter to
* \param offset - Offset to add parameter to argument list
* \param value  - Value of parameter
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSeti,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuParamSetf(void *conn) {
    CUfunction hfunc;
    int offset;
    float value;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0 ||
        rpc_read(conn, &value, sizeof(float)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuParamSetf(hfunc, offset, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Adds arbitrary data to the function's argument list
*
* \deprecated
*
* Copies an arbitrary amount of data (specified in \p numbytes) from \p ptr
* into the parameter space of the kernel corresponding to \p hfunc. \p offset
* is a byte offset.
*
* \param hfunc    - Kernel to add data to
* \param offset   - Offset to add data to argument list
* \param ptr      - Pointer to arbitrary data
* \param numbytes - Size of data to copy in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSetf,
* ::cuParamSeti,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuParamSetv(void *conn) {
    CUfunction hfunc;
    int offset;
    void* ptr;
    unsigned int numbytes;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &offset, sizeof(int)) < 0 ||
        rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &numbytes, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuParamSetv(hfunc, offset, &ptr, numbytes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function
*
* \deprecated
*
* Invokes the kernel \p f on a 1 x 1 x 1 grid of blocks. The block
* contains the number of threads specified by a previous call to
* ::cuFuncSetBlockShape().
*
* The block shape, dynamic shared memory size, and parameter information
* must be set using
*  ::cuFuncSetBlockShape(),
*  ::cuFuncSetSharedSize(),
*  ::cuParamSetSize(),
*  ::cuParamSeti(),
*  ::cuParamSetf(), and
*  ::cuParamSetv()
* prior to calling this function.
*
* Launching a function via ::cuLaunchKernel() invalidates the function's
* block shape, dynamic shared memory size, and parameter information. After
* launching via cuLaunchKernel, this state must be re-initialized prior to
* calling this function. Failure to do so results in undefined behavior.
*
* \param f - Kernel to launch
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSetf,
* ::cuParamSeti,
* ::cuParamSetv,
* ::cuLaunchGrid,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuLaunch(void *conn) {
    CUfunction f;

    if (rpc_read(conn, &f, sizeof(CUfunction)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunch(f);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function
*
* \deprecated
*
* Invokes the kernel \p f on a \p grid_width x \p grid_height grid of
* blocks. Each block contains the number of threads specified by a previous
* call to ::cuFuncSetBlockShape().
*
* The block shape, dynamic shared memory size, and parameter information
* must be set using
*  ::cuFuncSetBlockShape(),
*  ::cuFuncSetSharedSize(),
*  ::cuParamSetSize(),
*  ::cuParamSeti(),
*  ::cuParamSetf(), and
*  ::cuParamSetv()
* prior to calling this function.
*
* Launching a function via ::cuLaunchKernel() invalidates the function's
* block shape, dynamic shared memory size, and parameter information. After
* launching via cuLaunchKernel, this state must be re-initialized prior to
* calling this function. Failure to do so results in undefined behavior.
*
* \param f           - Kernel to launch
* \param grid_width  - Width of grid in blocks
* \param grid_height - Height of grid in blocks
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSetf,
* ::cuParamSeti,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGridAsync,
* ::cuLaunchKernel
*/
int handle_cuLaunchGrid(void *conn) {
    CUfunction f;
    int grid_width;
    int grid_height;

    if (rpc_read(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &grid_width, sizeof(int)) < 0 ||
        rpc_read(conn, &grid_height, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchGrid(f, grid_width, grid_height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function
*
* \deprecated
*
* Invokes the kernel \p f on a \p grid_width x \p grid_height grid of
* blocks. Each block contains the number of threads specified by a previous
* call to ::cuFuncSetBlockShape().
*
* The block shape, dynamic shared memory size, and parameter information
* must be set using
*  ::cuFuncSetBlockShape(),
*  ::cuFuncSetSharedSize(),
*  ::cuParamSetSize(),
*  ::cuParamSeti(),
*  ::cuParamSetf(), and
*  ::cuParamSetv()
* prior to calling this function.
*
* Launching a function via ::cuLaunchKernel() invalidates the function's
* block shape, dynamic shared memory size, and parameter information. After
* launching via cuLaunchKernel, this state must be re-initialized prior to
* calling this function. Failure to do so results in undefined behavior.
*
* \param f           - Kernel to launch
* \param grid_width  - Width of grid in blocks
* \param grid_height - Height of grid in blocks
* \param hStream     - Stream identifier
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_LAUNCH_FAILED,
* ::CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES,
* ::CUDA_ERROR_LAUNCH_TIMEOUT,
* ::CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING,
* ::CUDA_ERROR_SHARED_OBJECT_INIT_FAILED
*
* \note In certain cases where cubins are created with no ABI (i.e., using \p ptxas \p --abi-compile \p no),
*       this function may serialize kernel launches. The CUDA driver retains asynchronous behavior by
*       growing the per-thread stack as needed per launch and not shrinking it afterwards.
*
* \note_null_stream
* \notefnerr
*
* \sa ::cuFuncSetBlockShape,
* ::cuFuncSetSharedSize,
* ::cuFuncGetAttribute,
* ::cuParamSetSize,
* ::cuParamSetf,
* ::cuParamSeti,
* ::cuParamSetv,
* ::cuLaunch,
* ::cuLaunchGrid,
* ::cuLaunchKernel
*/
int handle_cuLaunchGridAsync(void *conn) {
    CUfunction f;
    int grid_width;
    int grid_height;
    CUstream hStream;

    if (rpc_read(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &grid_width, sizeof(int)) < 0 ||
        rpc_read(conn, &grid_height, sizeof(int)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuLaunchGridAsync(f, grid_width, grid_height, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Adds a texture-reference to the function's argument list
*
* \deprecated
*
* Makes the CUDA array or linear memory bound to the texture reference
* \p hTexRef available to a device program as a texture. In this version of
* CUDA, the texture-reference must be obtained via ::cuModuleGetTexRef() and
* the \p texunit parameter must be set to ::CU_PARAM_TR_DEFAULT.
*
* \param hfunc   - Kernel to add texture-reference to
* \param texunit - Texture unit (must be ::CU_PARAM_TR_DEFAULT)
* \param hTexRef - Texture-reference to add to argument list
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*/
int handle_cuParamSetTexRef(void *conn) {
    CUfunction hfunc;
    int texunit;
    CUtexref hTexRef;

    if (rpc_read(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &texunit, sizeof(int)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuParamSetTexRef(hfunc, texunit, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a graph
*
* Creates an empty graph, which is returned via \p phGraph.
*
* \param phGraph - Returns newly created graph
* \param flags   - Graph creation flags, must be 0
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode,
* ::cuGraphInstantiate,
* ::cuGraphDestroy,
* ::cuGraphGetNodes,
* ::cuGraphGetRootNodes,
* ::cuGraphGetEdges,
* ::cuGraphClone
*/
int handle_cuGraphCreate(void *conn) {
    CUgraph phGraph;
    unsigned int flags;

    if (rpc_read(conn, &phGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphCreate(&phGraph, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraph, sizeof(CUgraph)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a kernel execution node and adds it to a graph
*
* Creates a new kernel execution node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* The CUDA_KERNEL_NODE_PARAMS structure is defined as:
*
* \code
*  typedef struct CUDA_KERNEL_NODE_PARAMS_st {
*      CUfunction func;
*      unsigned int gridDimX;
*      unsigned int gridDimY;
*      unsigned int gridDimZ;
*      unsigned int blockDimX;
*      unsigned int blockDimY;
*      unsigned int blockDimZ;
*      unsigned int sharedMemBytes;
*      void **kernelParams;
*      void **extra;
*  } CUDA_KERNEL_NODE_PARAMS;
* \endcode
*
* When the graph is launched, the node will invoke kernel \p func on a (\p gridDimX x
* \p gridDimY x \p gridDimZ) grid of blocks. Each block contains
* (\p blockDimX x \p blockDimY x \p blockDimZ) threads.
*
* \p sharedMemBytes sets the amount of dynamic shared memory that will be
* available to each thread block.
*
* Kernel parameters to \p func can be specified in one of two ways:
*
* 1) Kernel parameters can be specified via \p kernelParams. If the kernel has N
* parameters, then \p kernelParams needs to be an array of N pointers. Each pointer,
* from \p kernelParams[0] to \p kernelParams[N-1], points to the region of memory from which the actual
* parameter will be copied. The number of kernel parameters and their offsets and sizes do not need
* to be specified as that information is retrieved directly from the kernel's image.
*
* 2) Kernel parameters for non-cooperative kernels can also be packaged by the application into a single
* buffer that is passed in via \p extra. This places the burden on the application of knowing each
* kernel parameter's size and alignment/padding within the buffer. The \p extra parameter exists
* to allow this function to take additional less commonly used arguments. \p extra specifies
* a list of names of extra settings and their corresponding values. Each extra setting name is
* immediately followed by the corresponding value. The list must be terminated with either NULL or
* CU_LAUNCH_PARAM_END.
*
* - ::CU_LAUNCH_PARAM_END, which indicates the end of the \p extra
*   array;
* - ::CU_LAUNCH_PARAM_BUFFER_POINTER, which specifies that the next
*   value in \p extra will be a pointer to a buffer
*   containing all the kernel parameters for launching kernel
*   \p func;
* - ::CU_LAUNCH_PARAM_BUFFER_SIZE, which specifies that the next
*   value in \p extra will be a pointer to a size_t
*   containing the size of the buffer specified with
*   ::CU_LAUNCH_PARAM_BUFFER_POINTER;
*
* The error ::CUDA_ERROR_INVALID_VALUE will be returned if kernel parameters are specified with both
* \p kernelParams and \p extra (i.e. both \p kernelParams and \p extra are non-NULL).
* ::CUDA_ERROR_INVALID_VALUE will be returned if \p extra is used for a cooperative kernel.
*
* The \p kernelParams or \p extra array, as well as the argument values it points to,
* are copied during this call.
*
* \note Kernels launched using graphs must not use texture and surface references. Reading or
*       writing through any texture or surface reference is undefined behavior.
*       This restriction does not apply to texture and surface objects.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the GPU execution node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchKernel,
* ::cuLaunchCooperativeKernel,
* ::cuGraphKernelNodeGetParams,
* ::cuGraphKernelNodeSetParams,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddKernelNode_v2(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_KERNEL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddKernelNode_v2(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a kernel node's parameters
*
* Returns the parameters of kernel node \p hNode in \p nodeParams.
* The \p kernelParams or \p extra array returned in \p nodeParams,
* as well as the argument values it points to, are owned by the node.
* This memory remains valid until the node is destroyed or its
* parameters are modified, and should not be modified
* directly. Use ::cuGraphKernelNodeSetParams to update the
* parameters of this node.
*
* The params will contain either \p kernelParams or \p extra,
* according to which of these was most recently set on the node.
*
* \param hNode      - Node to get the parameters for
* \param nodeParams - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchKernel,
* ::cuGraphAddKernelNode,
* ::cuGraphKernelNodeSetParams
*/
int handle_cuGraphKernelNodeGetParams_v2(void *conn) {
    CUgraphNode hNode;
    CUDA_KERNEL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphKernelNodeGetParams_v2(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a kernel node's parameters
*
* Sets the parameters of kernel node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchKernel,
* ::cuGraphAddKernelNode,
* ::cuGraphKernelNodeGetParams
*/
int handle_cuGraphKernelNodeSetParams_v2(void *conn) {
    CUgraphNode hNode;
    CUDA_KERNEL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphKernelNodeSetParams_v2(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memcpy node and adds it to a graph
*
* Creates a new memcpy node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* When the graph is launched, the node will perform the memcpy described by \p copyParams.
* See ::cuMemcpy3D() for a description of the structure and its restrictions.
*
* Memcpy nodes have some additional restrictions with regards to managed memory, if the
* system contains at least one device which has a zero value for the device attribute
* ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS. If one or more of the operands refer
* to managed memory, then using the memory type ::CU_MEMORYTYPE_UNIFIED is disallowed
* for those operand(s). The managed memory will be treated as residing on either the
* host or the device, depending on which memory type is specified.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param copyParams      - Parameters for the memory copy
* \param ctx             - Context on which to run the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemcpy3D,
* ::cuGraphMemcpyNodeGetParams,
* ::cuGraphMemcpyNodeSetParams,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddMemcpyNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_MEMCPY3D copyParams;
    CUcontext ctx;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &copyParams, sizeof(CUDA_MEMCPY3D)) < 0 ||
        rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddMemcpyNode(&phGraphNode, hGraph, &dependencies, numDependencies, &copyParams, ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memcpy node's parameters
*
* Returns the parameters of memcpy node \p hNode in \p nodeParams.
*
* \param hNode      - Node to get the parameters for
* \param nodeParams - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemcpy3D,
* ::cuGraphAddMemcpyNode,
* ::cuGraphMemcpyNodeSetParams
*/
int handle_cuGraphMemcpyNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_MEMCPY3D nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_MEMCPY3D)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemcpyNodeGetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodeParams, sizeof(CUDA_MEMCPY3D)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a memcpy node's parameters
*
* Sets the parameters of memcpy node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemcpy3D,
* ::cuGraphAddMemcpyNode,
* ::cuGraphMemcpyNodeGetParams
*/
int handle_cuGraphMemcpyNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_MEMCPY3D nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_MEMCPY3D)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemcpyNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memset node and adds it to a graph
*
* Creates a new memset node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* The element size must be 1, 2, or 4 bytes.
* When the graph is launched, the node will perform the memset described by \p memsetParams.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param memsetParams    - Parameters for the memory set
* \param ctx             - Context on which to run the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_CONTEXT
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemsetD2D32,
* ::cuGraphMemsetNodeGetParams,
* ::cuGraphMemsetNodeSetParams,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode
*/
int handle_cuGraphAddMemsetNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_MEMSET_NODE_PARAMS memsetParams;
    CUcontext ctx;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &memsetParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddMemsetNode(&phGraphNode, hGraph, &dependencies, numDependencies, &memsetParams, ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memset node's parameters
*
* Returns the parameters of memset node \p hNode in \p nodeParams.
*
* \param hNode      - Node to get the parameters for
* \param nodeParams - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemsetD2D32,
* ::cuGraphAddMemsetNode,
* ::cuGraphMemsetNodeSetParams
*/
int handle_cuGraphMemsetNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_MEMSET_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemsetNodeGetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodeParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a memset node's parameters
*
* Sets the parameters of memset node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuMemsetD2D32,
* ::cuGraphAddMemsetNode,
* ::cuGraphMemsetNodeGetParams
*/
int handle_cuGraphMemsetNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_MEMSET_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemsetNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a host execution node and adds it to a graph
*
* Creates a new CPU execution node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* When the graph is launched, the node will invoke the specified CPU function.
* Host nodes are not supported under MPS with pre-Volta GPUs.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the host node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchHostFunc,
* ::cuGraphHostNodeGetParams,
* ::cuGraphHostNodeSetParams,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddHostNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_HOST_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddHostNode(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a host node's parameters
*
* Returns the parameters of host node \p hNode in \p nodeParams.
*
* \param hNode      - Node to get the parameters for
* \param nodeParams - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchHostFunc,
* ::cuGraphAddHostNode,
* ::cuGraphHostNodeSetParams
*/
int handle_cuGraphHostNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_HOST_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphHostNodeGetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a host node's parameters
*
* Sets the parameters of host node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchHostFunc,
* ::cuGraphAddHostNode,
* ::cuGraphHostNodeGetParams
*/
int handle_cuGraphHostNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_HOST_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphHostNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a child graph node and adds it to a graph
*
* Creates a new node which executes an embedded graph, and adds it to \p hGraph with
* \p numDependencies dependencies specified via \p dependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* If \p hGraph contains allocation or free nodes, this call will return an error.
*
* The node executes an embedded child graph. The child graph is cloned in this call.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param childGraph      - The graph to clone into this node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphChildGraphNodeGetGraph,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode,
* ::cuGraphClone
*/
int handle_cuGraphAddChildGraphNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUgraph childGraph;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &childGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddChildGraphNode(&phGraphNode, hGraph, &dependencies, numDependencies, childGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a handle to the embedded graph of a child graph node
*
* Gets a handle to the embedded graph in a child graph node. This call
* does not clone the graph. Changes to the graph will be reflected in
* the node, and the node retains ownership of the graph.
*
* Allocation and free nodes cannot be added to the returned graph.
* Attempting to do so will return an error.
*
* \param hNode   - Node to get the embedded graph for
* \param phGraph - Location to store a handle to the graph
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddChildGraphNode,
* ::cuGraphNodeFindInClone
*/
int handle_cuGraphChildGraphNodeGetGraph(void *conn) {
    CUgraphNode hNode;
    CUgraph phGraph;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &phGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphChildGraphNodeGetGraph(hNode, &phGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraph, sizeof(CUgraph)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an empty node and adds it to a graph
*
* Creates a new node which performs no operation, and adds it to \p hGraph with
* \p numDependencies dependencies specified via \p dependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* An empty node performs no operation during execution, but can be used for
* transitive ordering. For example, a phased execution graph with 2 groups of n
* nodes with a barrier between them can be represented using an empty node and
* 2*n dependency edges, rather than no empty node and n^2 dependency edges.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddEmptyNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddEmptyNode(&phGraphNode, hGraph, &dependencies, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an event record node and adds it to a graph
*
* Creates a new event record node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and event specified in \p event.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* Each launch of the graph will record \p event to capture execution of the
* node's dependencies.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param event           - Event for the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventWaitNode,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddEventRecordNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUevent event;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddEventRecordNode(&phGraphNode, hGraph, &dependencies, numDependencies, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the event associated with an event record node
*
* Returns the event of event record node \p hNode in \p event_out.
*
* \param hNode     - Node to get the event for
* \param event_out - Pointer to return the event
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventRecordNode,
* ::cuGraphEventRecordNodeSetEvent,
* ::cuGraphEventWaitNodeGetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent
*/
int handle_cuGraphEventRecordNodeGetEvent(void *conn) {
    CUgraphNode hNode;
    CUevent event_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event_out, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphEventRecordNodeGetEvent(hNode, &event_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event_out, sizeof(CUevent)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets an event record node's event
*
* Sets the event of event record node \p hNode to \p event.
*
* \param hNode - Node to set the event for
* \param event - Event to use
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventRecordNode,
* ::cuGraphEventRecordNodeGetEvent,
* ::cuGraphEventWaitNodeSetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent
*/
int handle_cuGraphEventRecordNodeSetEvent(void *conn) {
    CUgraphNode hNode;
    CUevent event;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphEventRecordNodeSetEvent(hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an event wait node and adds it to a graph
*
* Creates a new event wait node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and event specified in \p event.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* The graph node will wait for all work captured in \p event.  See ::cuEventRecord()
* for details on what is captured by an event. \p event may be from a different context
* or device than the launch stream.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param event           - Event for the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventRecordNode,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddEventWaitNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUevent event;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddEventWaitNode(&phGraphNode, hGraph, &dependencies, numDependencies, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the event associated with an event wait node
*
* Returns the event of event wait node \p hNode in \p event_out.
*
* \param hNode     - Node to get the event for
* \param event_out - Pointer to return the event
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventWaitNode,
* ::cuGraphEventWaitNodeSetEvent,
* ::cuGraphEventRecordNodeGetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent
*/
int handle_cuGraphEventWaitNodeGetEvent(void *conn) {
    CUgraphNode hNode;
    CUevent event_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event_out, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphEventWaitNodeGetEvent(hNode, &event_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event_out, sizeof(CUevent)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets an event wait node's event
*
* Sets the event of event wait node \p hNode to \p event.
*
* \param hNode - Node to set the event for
* \param event - Event to use
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventWaitNode,
* ::cuGraphEventWaitNodeGetEvent,
* ::cuGraphEventRecordNodeSetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent
*/
int handle_cuGraphEventWaitNodeSetEvent(void *conn) {
    CUgraphNode hNode;
    CUevent event;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphEventWaitNodeSetEvent(hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an external semaphore signal node and adds it to a graph
*
* Creates a new external semaphore signal node and adds it to \p hGraph with \p
* numDependencies dependencies specified via \p dependencies and arguments specified
* in \p nodeParams. It is possible for \p numDependencies to be 0, in which case the
* node will be placed at the root of the graph. \p dependencies may not have any
* duplicate entries. A handle to the new node will be returned in \p phGraphNode.
*
* Performs a signal operation on a set of externally allocated semaphore objects
* when the node is launched.  The operation(s) will occur after all of the node's
* dependencies have completed.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphExternalSemaphoresSignalNodeGetParams,
* ::cuGraphExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuImportExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddEventRecordNode,
* ::cuGraphAddEventWaitNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddExternalSemaphoresSignalNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_EXT_SEM_SIGNAL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddExternalSemaphoresSignalNode(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns an external semaphore signal node's parameters
*
* Returns the parameters of an external semaphore signal node \p hNode in \p params_out.
* The \p extSemArray and \p paramsArray returned in \p params_out,
* are owned by the node.  This memory remains valid until the node is destroyed or its
* parameters are modified, and should not be modified
* directly. Use ::cuGraphExternalSemaphoresSignalNodeSetParams to update the
* parameters of this node.
*
* \param hNode      - Node to get the parameters for
* \param params_out - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchKernel,
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuGraphExternalSemaphoresSignalNodeSetParams,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuGraphExternalSemaphoresSignalNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_EXT_SEM_SIGNAL_NODE_PARAMS params_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &params_out, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExternalSemaphoresSignalNodeGetParams(hNode, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets an external semaphore signal node's parameters
*
* Sets the parameters of an external semaphore signal node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuGraphExternalSemaphoresSignalNodeSetParams,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuGraphExternalSemaphoresSignalNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_EXT_SEM_SIGNAL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExternalSemaphoresSignalNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an external semaphore wait node and adds it to a graph
*
* Creates a new external semaphore wait node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries. A handle
* to the new node will be returned in \p phGraphNode.
*
* Performs a wait operation on a set of externally allocated semaphore objects
* when the node is launched.  The node's dependencies will not be launched until
* the wait operation has completed.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphExternalSemaphoresWaitNodeGetParams,
* ::cuGraphExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuImportExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddEventRecordNode,
* ::cuGraphAddEventWaitNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddExternalSemaphoresWaitNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_EXT_SEM_WAIT_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddExternalSemaphoresWaitNode(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns an external semaphore wait node's parameters
*
* Returns the parameters of an external semaphore wait node \p hNode in \p params_out.
* The \p extSemArray and \p paramsArray returned in \p params_out,
* are owned by the node.  This memory remains valid until the node is destroyed or its
* parameters are modified, and should not be modified
* directly. Use ::cuGraphExternalSemaphoresSignalNodeSetParams to update the
* parameters of this node.
*
* \param hNode      - Node to get the parameters for
* \param params_out - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuLaunchKernel,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuGraphExternalSemaphoresWaitNodeSetParams,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuGraphExternalSemaphoresWaitNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_EXT_SEM_WAIT_NODE_PARAMS params_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &params_out, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExternalSemaphoresWaitNodeGetParams(hNode, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets an external semaphore wait node's parameters
*
* Sets the parameters of an external semaphore wait node \p hNode to \p nodeParams.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuGraphExternalSemaphoresWaitNodeSetParams,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync
*/
int handle_cuGraphExternalSemaphoresWaitNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_EXT_SEM_WAIT_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExternalSemaphoresWaitNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a batch memory operation node and adds it to a graph
*
* Creates a new batch memory operation node and adds it to \p hGraph with \p
* numDependencies dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p phGraphNode.
*
* When the node is added, the paramArray inside \p nodeParams is copied and therefore it can be
* freed after the call returns.
*
* \note
* Warning:
* Improper use of this API may deadlock the application. Synchronization 
* ordering established through this API is not visible to CUDA. CUDA tasks 
* that are (even indirectly) ordered by this API should also have that order
* expressed with CUDA-visible dependencies such as events. This ensures that 
* the scheduler does not serialize them in an improper order. For more 
* information, see the Stream Memory Operations section in the programming 
* guide(https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuStreamBatchMemOp,
* ::cuStreamWaitValue32,
* ::cuStreamWriteValue32,
* ::cuStreamWaitValue64,
* ::cuStreamWriteValue64,
* ::cuGraphBatchMemOpNodeGetParams,
* ::cuGraphBatchMemOpNodeSetParams,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddBatchMemOpNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_BATCH_MEM_OP_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddBatchMemOpNode(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a batch mem op node's parameters
*
* Returns the parameters of batch mem op node \p hNode in \p nodeParams_out.
* The \p paramArray returned in \p nodeParams_out is owned by the node.
* This memory remains valid until the node is destroyed or its
* parameters are modified, and should not be modified
* directly. Use ::cuGraphBatchMemOpNodeSetParams to update the
* parameters of this node.
*
* \param hNode          - Node to get the parameters for
* \param nodeParams_out - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuStreamBatchMemOp,
* ::cuGraphAddBatchMemOpNode,
* ::cuGraphBatchMemOpNodeSetParams
*/
int handle_cuGraphBatchMemOpNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_BATCH_MEM_OP_NODE_PARAMS nodeParams_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams_out, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphBatchMemOpNodeGetParams(hNode, &nodeParams_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodeParams_out, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a batch mem op node's parameters
*
* Sets the parameters of batch mem op node \p hNode to \p nodeParams.
*
* The paramArray inside \p nodeParams is copied and therefore it can be
* freed after the call returns.
*
* \param hNode      - Node to set the parameters for
* \param nodeParams - Parameters to copy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuStreamBatchMemOp,
* ::cuGraphAddBatchMemOpNode,
* ::cuGraphBatchMemOpNodeGetParams
*/
int handle_cuGraphBatchMemOpNodeSetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_BATCH_MEM_OP_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphBatchMemOpNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a batch mem op node in the given graphExec
*
* Sets the parameters of a batch mem op node in an executable graph \p hGraphExec.
* The node is identified by the corresponding node \p hNode in the
* non-executable graph, from which the executable graph was instantiated.
*
* The following fields on operations may be modified on an executable graph:
*
*  op.waitValue.address
*  op.waitValue.value[64]
*  op.waitValue.flags bits corresponding to wait type (i.e. CU_STREAM_WAIT_VALUE_FLUSH bit cannot be modified)
*  op.writeValue.address
*  op.writeValue.value[64]
*
* Other fields, such as the context, count or type of operations, and other types of operations such as membars, 
* may not be modified.
*
* \p hNode must not have been removed from the original graph.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* The paramArray inside \p nodeParams is copied and therefore it can be
* freed after the call returns.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Batch mem op node from the graph from which graphExec was instantiated
* \param nodeParams - Updated Parameters to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuStreamBatchMemOp,
* ::cuGraphAddBatchMemOpNode,
* ::cuGraphBatchMemOpNodeGetParams,
* ::cuGraphBatchMemOpNodeSetParams,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecBatchMemOpNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_BATCH_MEM_OP_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecBatchMemOpNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an allocation node and adds it to a graph
*
* Creates a new allocation node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries. A handle
* to the new node will be returned in \p phGraphNode.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param nodeParams      - Parameters for the node
*
* When ::cuGraphAddMemAllocNode creates an allocation node, it returns the address of the allocation in
* \p nodeParams.dptr.  The allocation's address remains fixed across instantiations and launches.
*
* If the allocation is freed in the same graph, by creating a free node using ::cuGraphAddMemFreeNode,
* the allocation can be accessed by nodes ordered after the allocation node but before the free node.
* These allocations cannot be freed outside the owning graph, and they can only be freed once in the
* owning graph.
*
* If the allocation is not freed in the same graph, then it can be accessed not only by nodes in the
* graph which are ordered after the allocation node, but also by stream operations ordered after the
* graph's execution but before the allocation is freed.
*
* Allocations which are not freed in the same graph can be freed by:
* - passing the allocation to ::cuMemFreeAsync or ::cuMemFree;
* - launching a graph with a free node for that allocation; or
* - specifying ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH during instantiation, which makes
* each launch behave as though it called ::cuMemFreeAsync for every unfreed allocation.
* 
* It is not possible to free an allocation in both the owning graph and another graph.  If the allocation
* is freed in the same graph, a free node cannot be added to another graph.  If the allocation is freed
* in another graph, a free node can no longer be added to the owning graph.
*
* The following restrictions apply to graphs which contain allocation and/or memory free nodes:
* - Nodes and edges of the graph cannot be deleted.
* - The graph cannot be used in a child node.
* - Only one instantiation of the graph may exist at any point in time.
* - The graph cannot be cloned.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemFreeNode,
* ::cuGraphMemAllocNodeGetParams,
* ::cuDeviceGraphMemTrim,
* ::cuDeviceGetGraphMemAttribute,
* ::cuDeviceSetGraphMemAttribute,
* ::cuMemAllocAsync,
* ::cuMemFreeAsync,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddEventRecordNode,
* ::cuGraphAddEventWaitNode,
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddMemAllocNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUDA_MEM_ALLOC_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddMemAllocNode(&phGraphNode, hGraph, &dependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memory alloc node's parameters
*
* Returns the parameters of a memory alloc node \p hNode in \p params_out.
* The \p poolProps and \p accessDescs returned in \p params_out, are owned by the
* node.  This memory remains valid until the node is destroyed.  The returned
* parameters must not be modified.
*
* \param hNode      - Node to get the parameters for
* \param params_out - Pointer to return the parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemAllocNode,
* ::cuGraphMemFreeNodeGetParams
*/
int handle_cuGraphMemAllocNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUDA_MEM_ALLOC_NODE_PARAMS params_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &params_out, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemAllocNodeGetParams(hNode, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memory free node and adds it to a graph
*
* Creates a new memory free node and adds it to \p hGraph with \p numDependencies
* dependencies specified via \p dependencies and arguments specified in \p nodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p dependencies may not have any duplicate entries. A handle
* to the new node will be returned in \p phGraphNode.
*
* \param phGraphNode     - Returns newly created node
* \param hGraph          - Graph to which to add the node
* \param dependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param dptr            - Address of memory to free
*
* ::cuGraphAddMemFreeNode will return ::CUDA_ERROR_INVALID_VALUE if the user attempts to free:
* - an allocation twice in the same graph.
* - an address that was not returned by an allocation node.
* - an invalid address.
*
* The following restrictions apply to graphs which contain allocation and/or memory free nodes:
* - Nodes and edges of the graph cannot be deleted.
* - The graph cannot be used in a child node.
* - Only one instantiation of the graph may exist at any point in time.
* - The graph cannot be cloned.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_NOT_SUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemAllocNode,
* ::cuGraphMemFreeNodeGetParams,
* ::cuDeviceGraphMemTrim,
* ::cuDeviceGetGraphMemAttribute,
* ::cuDeviceSetGraphMemAttribute,
* ::cuMemAllocAsync,
* ::cuMemFreeAsync,
* ::cuGraphCreate,
* ::cuGraphDestroyNode,
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddEventRecordNode,
* ::cuGraphAddEventWaitNode,
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphAddMemFreeNode(void *conn) {
    CUgraphNode phGraphNode;
    CUgraph hGraph;
    CUgraphNode dependencies;
    size_t numDependencies;
    CUdeviceptr dptr;

    if (rpc_read(conn, &phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddMemFreeNode(&phGraphNode, hGraph, &dependencies, numDependencies, dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memory free node's parameters
*
* Returns the address of a memory free node \p hNode in \p dptr_out.
*
* \param hNode    - Node to get the parameters for
* \param dptr_out - Pointer to return the device address
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemFreeNode,
* ::cuGraphMemAllocNodeGetParams
*/
int handle_cuGraphMemFreeNodeGetParams(void *conn) {
    CUgraphNode hNode;
    CUdeviceptr dptr_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &dptr_out, sizeof(CUdeviceptr)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphMemFreeNodeGetParams(hNode, &dptr_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr_out, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Free unused memory that was cached on the specified device for use with graphs back to the OS.
*
* Blocks which are not in use by a graph that is either currently executing or scheduled to execute are
* freed back to the operating system.
*
* \param device - The device for which cached memory should be freed.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_DEVICE
*
* \sa
* ::cuGraphAddMemAllocNode,
* ::cuGraphAddMemFreeNode,
* ::cuDeviceSetGraphMemAttribute,
* ::cuDeviceGetGraphMemAttribute
*/
int handle_cuDeviceGraphMemTrim(void *conn) {
    CUdevice device;

    if (rpc_read(conn, &device, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGraphMemTrim(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Query asynchronous allocation attributes related to graphs
*
* Valid attributes are:
*
* - ::CU_GRAPH_MEM_ATTR_USED_MEM_CURRENT: Amount of memory, in bytes, currently associated with graphs
* - ::CU_GRAPH_MEM_ATTR_USED_MEM_HIGH: High watermark of memory, in bytes, associated with graphs since the
*   last time it was reset.  High watermark can only be reset to zero.
* - ::CU_GRAPH_MEM_ATTR_RESERVED_MEM_CURRENT: Amount of memory, in bytes, currently allocated for use by
*   the CUDA graphs asynchronous allocator.
* - ::CU_GRAPH_MEM_ATTR_RESERVED_MEM_HIGH: High watermark of memory, in bytes, currently allocated for use by
*   the CUDA graphs asynchronous allocator.
*
* \param device - Specifies the scope of the query
* \param attr - attribute to get
* \param value - retrieved value
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_DEVICE
*
* \sa
* ::cuDeviceSetGraphMemAttribute,
* ::cuGraphAddMemAllocNode,
* ::cuGraphAddMemFreeNode
*/
int handle_cuDeviceGetGraphMemAttribute(void *conn) {
    CUdevice device;
    CUgraphMem_attribute attr;
    void* value;

    if (rpc_read(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUgraphMem_attribute)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetGraphMemAttribute(device, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Set asynchronous allocation attributes related to graphs
*
* Valid attributes are:
*
* - ::CU_GRAPH_MEM_ATTR_USED_MEM_HIGH: High watermark of memory, in bytes, associated with graphs since the
*   last time it was reset.  High watermark can only be reset to zero.
* - ::CU_GRAPH_MEM_ATTR_RESERVED_MEM_HIGH: High watermark of memory, in bytes, currently allocated for use by
*   the CUDA graphs asynchronous allocator.
*
* \param device - Specifies the scope of the query
* \param attr - attribute to get
* \param value - pointer to value to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_DEVICE
*
* \sa
* ::cuDeviceGetGraphMemAttribute,
* ::cuGraphAddMemAllocNode,
* ::cuGraphAddMemFreeNode
*/
int handle_cuDeviceSetGraphMemAttribute(void *conn) {
    CUdevice device;
    CUgraphMem_attribute attr;
    void* value;

    if (rpc_read(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUgraphMem_attribute)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceSetGraphMemAttribute(device, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Clones a graph
*
* This function creates a copy of \p originalGraph and returns it in \p phGraphClone.
* All parameters are copied into the cloned graph. The original graph may be modified
* after this call without affecting the clone.
*
* Child graph nodes in the original graph are recursively copied into the clone.
*
* \param phGraphClone  - Returns newly created cloned graph
* \param originalGraph - Graph to clone
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OUT_OF_MEMORY
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate,
* ::cuGraphNodeFindInClone
*/
int handle_cuGraphClone(void *conn) {
    CUgraph phGraphClone;
    CUgraph originalGraph;

    if (rpc_read(conn, &phGraphClone, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &originalGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphClone(&phGraphClone, originalGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphClone, sizeof(CUgraph)) < 0)
        return -1;

    return result;
}

/**
* \brief Finds a cloned version of a node
*
* This function returns the node in \p hClonedGraph corresponding to \p hOriginalNode
* in the original graph.
*
* \p hClonedGraph must have been cloned from \p hOriginalGraph via ::cuGraphClone.
* \p hOriginalNode must have been in \p hOriginalGraph at the time of the call to
* ::cuGraphClone, and the corresponding cloned node in \p hClonedGraph must not have
* been removed. The cloned node is then returned via \p phClonedNode.
*
* \param phNode  - Returns handle to the cloned node
* \param hOriginalNode - Handle to the original node
* \param hClonedGraph - Cloned graph to query
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphClone
*/
int handle_cuGraphNodeFindInClone(void *conn) {
    CUgraphNode phNode;
    CUgraphNode hOriginalNode;
    CUgraph hClonedGraph;

    if (rpc_read(conn, &phNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hOriginalNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &hClonedGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeFindInClone(&phNode, hOriginalNode, hClonedGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phNode, sizeof(CUgraphNode)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's type
*
* Returns the node type of \p hNode in \p type.
*
* \param hNode - Node to query
* \param type  - Pointer to return the node type
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphGetNodes,
* ::cuGraphGetRootNodes,
* ::cuGraphChildGraphNodeGetGraph,
* ::cuGraphKernelNodeGetParams,
* ::cuGraphKernelNodeSetParams,
* ::cuGraphHostNodeGetParams,
* ::cuGraphHostNodeSetParams,
* ::cuGraphMemcpyNodeGetParams,
* ::cuGraphMemcpyNodeSetParams,
* ::cuGraphMemsetNodeGetParams,
* ::cuGraphMemsetNodeSetParams
*/
int handle_cuGraphNodeGetType(void *conn) {
    CUgraphNode hNode;
    CUgraphNodeType type;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &type, sizeof(CUgraphNodeType)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeGetType(hNode, &type);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &type, sizeof(CUgraphNodeType)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's nodes
*
* Returns a list of \p hGraph's nodes. \p nodes may be NULL, in which case this
* function will return the number of nodes in \p numNodes. Otherwise,
* \p numNodes entries will be filled in. If \p numNodes is higher than the actual
* number of nodes, the remaining entries in \p nodes will be set to NULL, and the
* number of nodes actually obtained will be returned in \p numNodes.
*
* \param hGraph   - Graph to query
* \param nodes    - Pointer to return the nodes
* \param numNodes - See description
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate,
* ::cuGraphGetRootNodes,
* ::cuGraphGetEdges,
* ::cuGraphNodeGetType,
* ::cuGraphNodeGetDependencies,
* ::cuGraphNodeGetDependentNodes
*/
int handle_cuGraphGetNodes(void *conn) {
    CUgraph hGraph;
    CUgraphNode nodes;
    size_t numNodes;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &nodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphGetNodes(hGraph, &nodes, &numNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's root nodes
*
* Returns a list of \p hGraph's root nodes. \p rootNodes may be NULL, in which case this
* function will return the number of root nodes in \p numRootNodes. Otherwise,
* \p numRootNodes entries will be filled in. If \p numRootNodes is higher than the actual
* number of root nodes, the remaining entries in \p rootNodes will be set to NULL, and the
* number of nodes actually obtained will be returned in \p numRootNodes.
*
* \param hGraph       - Graph to query
* \param rootNodes    - Pointer to return the root nodes
* \param numRootNodes - See description
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate,
* ::cuGraphGetNodes,
* ::cuGraphGetEdges,
* ::cuGraphNodeGetType,
* ::cuGraphNodeGetDependencies,
* ::cuGraphNodeGetDependentNodes
*/
int handle_cuGraphGetRootNodes(void *conn) {
    CUgraph hGraph;
    CUgraphNode rootNodes;
    size_t numRootNodes;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &rootNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numRootNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphGetRootNodes(hGraph, &rootNodes, &numRootNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &rootNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numRootNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's dependency edges
*
* Returns a list of \p hGraph's dependency edges. Edges are returned via corresponding
* indices in \p from and \p to; that is, the node in \p to[i] has a dependency on the
* node in \p from[i]. \p from and \p to may both be NULL, in which
* case this function only returns the number of edges in \p numEdges. Otherwise,
* \p numEdges entries will be filled in. If \p numEdges is higher than the actual
* number of edges, the remaining entries in \p from and \p to will be set to NULL, and
* the number of edges actually returned will be written to \p numEdges.
*
* \param hGraph   - Graph to get the edges from
* \param from     - Location to return edge endpoints
* \param to       - Location to return edge endpoints
* \param numEdges - See description
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphGetNodes,
* ::cuGraphGetRootNodes,
* ::cuGraphAddDependencies,
* ::cuGraphRemoveDependencies,
* ::cuGraphNodeGetDependencies,
* ::cuGraphNodeGetDependentNodes
*/
int handle_cuGraphGetEdges(void *conn) {
    CUgraph hGraph;
    CUgraphNode from;
    CUgraphNode to;
    size_t numEdges;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &from, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &to, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numEdges, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphGetEdges(hGraph, &from, &to, &numEdges);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &from, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &to, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numEdges, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's dependencies
*
* Returns a list of \p node's dependencies. \p dependencies may be NULL, in which case this
* function will return the number of dependencies in \p numDependencies. Otherwise,
* \p numDependencies entries will be filled in. If \p numDependencies is higher than the actual
* number of dependencies, the remaining entries in \p dependencies will be set to NULL, and the
* number of nodes actually obtained will be returned in \p numDependencies.
*
* \param hNode           - Node to query
* \param dependencies    - Pointer to return the dependencies
* \param numDependencies - See description
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphNodeGetDependentNodes,
* ::cuGraphGetNodes,
* ::cuGraphGetRootNodes,
* ::cuGraphGetEdges,
* ::cuGraphAddDependencies,
* ::cuGraphRemoveDependencies
*/
int handle_cuGraphNodeGetDependencies(void *conn) {
    CUgraphNode hNode;
    CUgraphNode dependencies;
    size_t numDependencies;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeGetDependencies(hNode, &dependencies, &numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's dependent nodes
*
* Returns a list of \p node's dependent nodes. \p dependentNodes may be NULL, in which
* case this function will return the number of dependent nodes in \p numDependentNodes.
* Otherwise, \p numDependentNodes entries will be filled in. If \p numDependentNodes is
* higher than the actual number of dependent nodes, the remaining entries in
* \p dependentNodes will be set to NULL, and the number of nodes actually obtained will
* be returned in \p numDependentNodes.
*
* \param hNode             - Node to query
* \param dependentNodes    - Pointer to return the dependent nodes
* \param numDependentNodes - See description
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphNodeGetDependencies,
* ::cuGraphGetNodes,
* ::cuGraphGetRootNodes,
* ::cuGraphGetEdges,
* ::cuGraphAddDependencies,
* ::cuGraphRemoveDependencies
*/
int handle_cuGraphNodeGetDependentNodes(void *conn) {
    CUgraphNode hNode;
    CUgraphNode dependentNodes;
    size_t numDependentNodes;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependentNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeGetDependentNodes(hNode, &dependentNodes, &numDependentNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numDependentNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Adds dependency edges to a graph
*
* The number of dependencies to be added is defined by \p numDependencies
* Elements in \p from and \p to at corresponding indices define a dependency.
* Each node in \p from and \p to must belong to \p hGraph.
*
* If \p numDependencies is 0, elements in \p from and \p to will be ignored.
* Specifying an existing dependency will return an error.
*
* \param hGraph - Graph to which dependencies are added
* \param from - Array of nodes that provide the dependencies
* \param to - Array of dependent nodes
* \param numDependencies - Number of dependencies to be added
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphRemoveDependencies,
* ::cuGraphGetEdges,
* ::cuGraphNodeGetDependencies,
* ::cuGraphNodeGetDependentNodes
*/
int handle_cuGraphAddDependencies(void *conn) {
    CUgraph hGraph;
    CUgraphNode from;
    CUgraphNode to;
    size_t numDependencies;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &from, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &to, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphAddDependencies(hGraph, &from, &to, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Removes dependency edges from a graph
*
* The number of \p dependencies to be removed is defined by \p numDependencies.
* Elements in \p from and \p to at corresponding indices define a dependency.
* Each node in \p from and \p to must belong to \p hGraph.
*
* If \p numDependencies is 0, elements in \p from and \p to will be ignored.
* Specifying a non-existing dependency will return an error.
*
* Dependencies cannot be removed from graphs which contain allocation or free nodes.
* Any attempt to do so will return an error.
*
* \param hGraph - Graph from which to remove dependencies
* \param from - Array of nodes that provide the dependencies
* \param to - Array of dependent nodes
* \param numDependencies - Number of dependencies to be removed
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddDependencies,
* ::cuGraphGetEdges,
* ::cuGraphNodeGetDependencies,
* ::cuGraphNodeGetDependentNodes
*/
int handle_cuGraphRemoveDependencies(void *conn) {
    CUgraph hGraph;
    CUgraphNode from;
    CUgraphNode to;
    size_t numDependencies;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &from, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &to, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphRemoveDependencies(hGraph, &from, &to, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Remove a node from the graph
*
* Removes \p hNode from its graph. This operation also severs any dependencies of other nodes
* on \p hNode and vice versa.
*
* Nodes which belong to a graph which contains allocation or free nodes cannot be destroyed.
* Any attempt to do so will return an error.
*
* \param hNode  - Node to remove
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddChildGraphNode,
* ::cuGraphAddEmptyNode,
* ::cuGraphAddKernelNode,
* ::cuGraphAddHostNode,
* ::cuGraphAddMemcpyNode,
* ::cuGraphAddMemsetNode
*/
int handle_cuGraphDestroyNode(void *conn) {
    CUgraphNode hNode;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphDestroyNode(hNode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an executable graph from a graph
*
* Instantiates \p hGraph as an executable graph. The graph is validated for any
* structural constraints or intra-node constraints which were not previously
* validated. If instantiation is successful, a handle to the instantiated graph
* is returned in \p phGraphExec.
*
* The \p flags parameter controls the behavior of instantiation and subsequent
* graph launches.  Valid flags are:
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH, which configures a
* graph containing memory allocation nodes to automatically free any
* unfreed memory allocations before the graph is relaunched.
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_DEVICE_LAUNCH, which configures the graph for launch
* from the device. If this flag is passed, the executable graph handle returned can be
* used to launch the graph from both the host and device. This flag can only be used
* on platforms which support unified addressing. This flag cannot be used in
* conjunction with ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH.
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_USE_NODE_PRIORITY, which causes the graph
* to use the priorities from the per-node attributes rather than the priority
* of the launch stream during execution. Note that priorities are only available
* on kernel nodes, and are copied from stream priority during stream capture.
*
* If \p hGraph contains any allocation or free nodes, there can be at most one
* executable graph in existence for that graph at a time. An attempt to instantiate
* a second executable graph before destroying the first with ::cuGraphExecDestroy
* will result in an error.
*
* If \p hGraph contains kernels which call device-side cudaGraphLaunch() from multiple
* contexts, this will result in an error.
*
* Graphs instantiated for launch on the device have additional restrictions which do not
* apply to host graphs:
*
* - The graph's nodes must reside on a single context.
* - The graph can only contain kernel nodes, memcpy nodes, memset nodes, and child graph nodes.
*   Operation-specific restrictions are outlined below.
* - Kernel nodes:
*   - Use of CUDA Dynamic Parallelism is not permitted.
*   - Cooperative launches are permitted as long as MPS is not in use.
* - Memcpy nodes:
*   - Only copies involving device memory and/or pinned device-mapped host memory are permitted.
*   - Copies involving CUDA arrays are not permitted.
*   - Both operands must be accessible from the current context, and the current context must
*     match the context of other nodes in the graph.
*
* \param phGraphExec - Returns instantiated graph
* \param hGraph      - Graph to instantiate
* \param flags       - Flags to control instantiation.  See ::CUgraphInstantiate_flags.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate,
* ::cuGraphCreate,
* ::cuGraphUpload,
* ::cuGraphLaunch,
* ::cuGraphExecDestroy
*/
int handle_cuGraphInstantiateWithFlags(void *conn) {
    CUgraphExec phGraphExec;
    CUgraph hGraph;
    unsigned long long flags;

    if (rpc_read(conn, &phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphInstantiateWithFlags(&phGraphExec, hGraph, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphExec, sizeof(CUgraphExec)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an executable graph from a graph
*
* Instantiates \p hGraph as an executable graph according to the \p instantiateParams structure.
* The graph is validated for any structural constraints or intra-node constraints
* which were not previously validated. If instantiation is successful, a handle to
* the instantiated graph is returned in \p phGraphExec.
*
* \p instantiateParams controls the behavior of instantiation and subsequent
* graph launches, as well as returning more detailed information in the event of an error.
* ::CUDA_GRAPH_INSTANTIATE_PARAMS is defined as:
*
* \code
    typedef struct {
        cuuint64_t flags;
        CUstream hUploadStream;
        CUgraphNode hErrNode_out;
        CUgraphInstantiateResult result_out;
    } CUDA_GRAPH_INSTANTIATE_PARAMS;
* \endcode
*
* The \p flags field controls the behavior of instantiation and subsequent
* graph launches. Valid flags are:
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH, which configures a
* graph containing memory allocation nodes to automatically free any
* unfreed memory allocations before the graph is relaunched.
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_UPLOAD, which will perform an upload of the graph
* into \p hUploadStream once the graph has been instantiated.
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_DEVICE_LAUNCH, which configures the graph for launch
* from the device. If this flag is passed, the executable graph handle returned can be
* used to launch the graph from both the host and device. This flag can only be used
* on platforms which support unified addressing. This flag cannot be used in
* conjunction with ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH.
*
* - ::CUDA_GRAPH_INSTANTIATE_FLAG_USE_NODE_PRIORITY, which causes the graph
* to use the priorities from the per-node attributes rather than the priority
* of the launch stream during execution. Note that priorities are only available
* on kernel nodes, and are copied from stream priority during stream capture.
*
* If \p hGraph contains any allocation or free nodes, there can be at most one
* executable graph in existence for that graph at a time. An attempt to instantiate a
* second executable graph before destroying the first with ::cuGraphExecDestroy will
* result in an error.
*
* If \p hGraph contains kernels which call device-side cudaGraphLaunch() from multiple
* contexts, this will result in an error.
*
* Graphs instantiated for launch on the device have additional restrictions which do not
* apply to host graphs:
*
* - The graph's nodes must reside on a single context.
* - The graph can only contain kernel nodes, memcpy nodes, memset nodes, and child graph nodes.
*   Operation-specific restrictions are outlined below.
* - Kernel nodes:
*   - Use of CUDA Dynamic Parallelism is not permitted.
*   - Cooperative launches are permitted as long as MPS is not in use.
* - Memcpy nodes:
*   - Only copies involving device memory and/or pinned device-mapped host memory are permitted.
*   - Copies involving CUDA arrays are not permitted.
*   - Both operands must be accessible from the current context, and the current context must
*     match the context of other nodes in the graph.
*
* In the event of an error, the \p result_out and \p hErrNode_out fields will contain more
* information about the nature of the error. Possible error reporting includes:
*
* - ::CUDA_GRAPH_INSTANTIATE_ERROR, if passed an invalid value or if an unexpected error occurred
*   which is described by the return value of the function. \p hErrNode_out will be set to NULL.
* - ::CUDA_GRAPH_INSTANTIATE_INVALID_STRUCTURE, if the graph structure is invalid. \p hErrNode_out
*   will be set to one of the offending nodes.
* - ::CUDA_GRAPH_INSTANTIATE_NODE_OPERATION_NOT_SUPPORTED, if the graph is instantiated for device
*   launch but contains a node of an unsupported node type, or a node which performs unsupported
*   operations, such as use of CUDA dynamic parallelism within a kernel node. \p hErrNode_out will
*   be set to this node.
* - ::CUDA_GRAPH_INSTANTIATE_MULTIPLE_CTXS_NOT_SUPPORTED, if the graph is instantiated for device
*   launch but a node’s context differs from that of another node. This error can also be returned
*   if a graph is not instantiated for device launch and it contains kernels which call device-side
*   cudaGraphLaunch() from multiple contexts. \p hErrNode_out will be set to this node.
*
* If instantiation is successful, \p result_out will be set to ::CUDA_GRAPH_INSTANTIATE_SUCCESS,
* and \p hErrNode_out will be set to NULL.
*
* \param phGraphExec       - Returns instantiated graph
* \param hGraph            - Graph to instantiate
* \param instantiateParams - Instantiation parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate,
* ::cuGraphInstantiate,
* ::cuGraphExecDestroy
*/
int handle_cuGraphInstantiateWithParams(void *conn) {
    CUgraphExec phGraphExec;
    CUgraph hGraph;
    CUDA_GRAPH_INSTANTIATE_PARAMS instantiateParams;

    if (rpc_read(conn, &phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &instantiateParams, sizeof(CUDA_GRAPH_INSTANTIATE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphInstantiateWithParams(&phGraphExec, hGraph, &instantiateParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &instantiateParams, sizeof(CUDA_GRAPH_INSTANTIATE_PARAMS)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the instantiation flags of an executable graph
*
* Returns the flags that were passed to instantiation for the given executable graph.
* ::CUDA_GRAPH_INSTANTIATE_FLAG_UPLOAD will not be returned by this API as it does
* not affect the resulting executable graph.
*
* \param hGraphExec - The executable graph to query
* \param flags      - Returns the instantiation flags
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate,
* ::cuGraphInstantiateWithParams
*/
int handle_cuGraphExecGetFlags(void *conn) {
    CUgraphExec hGraphExec;
    cuuint64_t flags;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &flags, sizeof(cuuint64_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecGetFlags(hGraphExec, &flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(cuuint64_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a kernel node in the given graphExec
*
* Sets the parameters of a kernel node in an executable graph \p hGraphExec. 
* The node is identified by the corresponding node \p hNode in the 
* non-executable graph, from which the executable graph was instantiated. 
*
* \p hNode must not have been removed from the original graph. All \p nodeParams 
* fields may change, but the following restrictions apply to \p func updates: 
*
*   - The owning context of the function cannot change.
*   - A node whose function originally did not use CUDA dynamic parallelism cannot be updated
*     to a function which uses CDP
*   - If \p hGraphExec was not instantiated for device launch, a node whose function originally
*     did not use device-side cudaGraphLaunch() cannot be updated to a function which uses
*     device-side cudaGraphLaunch() unless the node resides on the same context as nodes which
*     contained such calls at instantiate-time. If no such calls were present at instantiation,
*     these updates cannot be performed at all.
*
* The modifications only affect future launches of \p hGraphExec. Already 
* enqueued or running launches of \p hGraphExec are not affected by this call. 
* \p hNode is also not modified by this call.
* 
* \param hGraphExec  - The executable graph in which to set the specified node
* \param hNode       - kernel node from the graph from which graphExec was instantiated
* \param nodeParams  - Updated Parameters to set
* 
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddKernelNode,
* ::cuGraphKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecKernelNodeSetParams_v2(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_KERNEL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecKernelNodeSetParams_v2(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a memcpy node in the given graphExec.
*
* Updates the work represented by \p hNode in \p hGraphExec as though \p hNode had 
* contained \p copyParams at instantiation.  hNode must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from hNode are ignored.
*
* The source and destination memory in \p copyParams must be allocated from the same 
* contexts as the original source and destination memory.  Both the instantiation-time 
* memory operands and the memory operands in \p copyParams must be 1-dimensional.
* Zero-length operations are not supported.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  hNode is also 
* not modified by this call.
*
* Returns CUDA_ERROR_INVALID_VALUE if the memory operands' mappings changed or
* either the original or new memory operands are multidimensional.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Memcpy node from the graph which was used to instantiate graphExec
* \param copyParams - The updated parameters to set
* \param ctx        - Context on which to run the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemcpyNode,
* ::cuGraphMemcpyNodeSetParams,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecMemcpyNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_MEMCPY3D copyParams;
    CUcontext ctx;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &copyParams, sizeof(CUDA_MEMCPY3D)) < 0 ||
        rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecMemcpyNodeSetParams(hGraphExec, hNode, &copyParams, ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a memset node in the given graphExec.
*
* Updates the work represented by \p hNode in \p hGraphExec as though \p hNode had 
* contained \p memsetParams at instantiation.  hNode must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from hNode are ignored.
*
* The destination memory in \p memsetParams must be allocated from the same 
* contexts as the original destination memory.  Both the instantiation-time 
* memory operand and the memory operand in \p memsetParams must be 1-dimensional.
* Zero-length operations are not supported.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  hNode is also 
* not modified by this call.
*
* Returns CUDA_ERROR_INVALID_VALUE if the memory operand's mappings changed or
* either the original or new memory operand are multidimensional.
*
* \param hGraphExec   - The executable graph in which to set the specified node
* \param hNode        - Memset node from the graph which was used to instantiate graphExec
* \param memsetParams - The updated parameters to set
* \param ctx          - Context on which to run the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddMemsetNode,
* ::cuGraphMemsetNodeSetParams,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecMemsetNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_MEMSET_NODE_PARAMS memsetParams;
    CUcontext ctx;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &memsetParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &ctx, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecMemsetNodeSetParams(hGraphExec, hNode, &memsetParams, ctx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a host node in the given graphExec.
*
* Updates the work represented by \p hNode in \p hGraphExec as though \p hNode had 
* contained \p nodeParams at instantiation.  hNode must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from hNode are ignored.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  hNode is also 
* not modified by this call.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Host node from the graph which was used to instantiate graphExec
* \param nodeParams - The updated parameters to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddHostNode,
* ::cuGraphHostNodeSetParams,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecHostNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_HOST_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecHostNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Updates node parameters in the child graph node in the given graphExec.
*
* Updates the work represented by \p hNode in \p hGraphExec as though the nodes contained
* in \p hNode's graph had the parameters contained in \p childGraph's nodes at instantiation.
* \p hNode must remain in the graph which was used to instantiate \p hGraphExec.
* Changed edges to and from \p hNode are ignored.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  \p hNode is also 
* not modified by this call.
*
* The topology of \p childGraph, as well as the node insertion order,  must match that
* of the graph contained in \p hNode.  See ::cuGraphExecUpdate() for a list of restrictions
* on what can be updated in an instantiated graph.  The update is recursive, so child graph
* nodes contained within the top level child graph will also be updated.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Host node from the graph which was used to instantiate graphExec
* \param childGraph - The graph supplying the updated parameters
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddChildGraphNode,
* ::cuGraphChildGraphNodeGetGraph,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecChildGraphNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUgraph childGraph;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &childGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecChildGraphNodeSetParams(hGraphExec, hNode, childGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the event for an event record node in the given graphExec
*
* Sets the event of an event record node in an executable graph \p hGraphExec.
* The node is identified by the corresponding node \p hNode in the
* non-executable graph, from which the executable graph was instantiated.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - event record node from the graph from which graphExec was instantiated
* \param event      - Updated event to use
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventRecordNode,
* ::cuGraphEventRecordNodeGetEvent,
* ::cuGraphEventWaitNodeSetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecEventRecordNodeSetEvent(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUevent event;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecEventRecordNodeSetEvent(hGraphExec, hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the event for an event wait node in the given graphExec
*
* Sets the event of an event wait node in an executable graph \p hGraphExec.
* The node is identified by the corresponding node \p hNode in the
* non-executable graph, from which the executable graph was instantiated.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - event wait node from the graph from which graphExec was instantiated
* \param event      - Updated event to use
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddEventWaitNode,
* ::cuGraphEventWaitNodeGetEvent,
* ::cuGraphEventRecordNodeSetEvent,
* ::cuEventRecordWithFlags,
* ::cuStreamWaitEvent,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecEventWaitNodeSetEvent(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUevent event;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &event, sizeof(CUevent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecEventWaitNodeSetEvent(hGraphExec, hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for an external semaphore signal node in the given graphExec
*
* Sets the parameters of an external semaphore signal node in an executable graph \p hGraphExec.
* The node is identified by the corresponding node \p hNode in the
* non-executable graph, from which the executable graph was instantiated.
*
* \p hNode must not have been removed from the original graph.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* Changing \p nodeParams->numExtSems is not supported.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - semaphore signal node from the graph from which graphExec was instantiated
* \param nodeParams - Updated Parameters to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddExternalSemaphoresSignalNode,
* ::cuImportExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecExternalSemaphoresSignalNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_EXT_SEM_SIGNAL_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecExternalSemaphoresSignalNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for an external semaphore wait node in the given graphExec
*
* Sets the parameters of an external semaphore wait node in an executable graph \p hGraphExec.
* The node is identified by the corresponding node \p hNode in the
* non-executable graph, from which the executable graph was instantiated.
*
* \p hNode must not have been removed from the original graph.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* Changing \p nodeParams->numExtSems is not supported.
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - semaphore wait node from the graph from which graphExec was instantiated
* \param nodeParams - Updated Parameters to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphAddExternalSemaphoresWaitNode,
* ::cuImportExternalSemaphore,
* ::cuSignalExternalSemaphoresAsync,
* ::cuWaitExternalSemaphoresAsync,
* ::cuGraphExecKernelNodeSetParams,
* ::cuGraphExecMemcpyNodeSetParams,
* ::cuGraphExecMemsetNodeSetParams,
* ::cuGraphExecHostNodeSetParams,
* ::cuGraphExecChildGraphNodeSetParams,
* ::cuGraphExecEventRecordNodeSetEvent,
* ::cuGraphExecEventWaitNodeSetEvent,
* ::cuGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
*/
int handle_cuGraphExecExternalSemaphoresWaitNodeSetParams(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    CUDA_EXT_SEM_WAIT_NODE_PARAMS nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecExternalSemaphoresWaitNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Enables or disables the specified node in the given graphExec
*
* Sets \p hNode to be either enabled or disabled. Disabled nodes are functionally equivalent 
* to empty nodes until they are reenabled. Existing node parameters are not affected by 
* disabling/enabling the node.
*  
* The node is identified by the corresponding node \p hNode in the non-executable 
* graph, from which the executable graph was instantiated.   
*
* \p hNode must not have been removed from the original graph.
*
* The modifications only affect future launches of \p hGraphExec. Already
* enqueued or running launches of \p hGraphExec are not affected by this call.
* \p hNode is also not modified by this call.
*
* \note Currently only kernel, memset and memcpy nodes are supported. 
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Node from the graph from which graphExec was instantiated
* \param isEnabled  - Node is enabled if != 0, otherwise the node is disabled
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphNodeGetEnabled,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
* ::cuGraphLaunch
*/
int handle_cuGraphNodeSetEnabled(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    unsigned int isEnabled;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeSetEnabled(hGraphExec, hNode, isEnabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Query whether a node in the given graphExec is enabled
*
* Sets isEnabled to 1 if \p hNode is enabled, or 0 if \p hNode is disabled.
*
* The node is identified by the corresponding node \p hNode in the non-executable 
* graph, from which the executable graph was instantiated.   
*
* \p hNode must not have been removed from the original graph.
*
* \note Currently only kernel, memset and memcpy nodes are supported. 
*
* \param hGraphExec - The executable graph in which to set the specified node
* \param hNode      - Node from the graph from which graphExec was instantiated
* \param isEnabled  - Location to return the enabled status of the node
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphNodeSetEnabled,
* ::cuGraphExecUpdate,
* ::cuGraphInstantiate
* ::cuGraphLaunch
*/
int handle_cuGraphNodeGetEnabled(void *conn) {
    CUgraphExec hGraphExec;
    CUgraphNode hNode;
    unsigned int isEnabled;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphNodeGetEnabled(hGraphExec, hNode, &isEnabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Uploads an executable graph in a stream
*
* Uploads \p hGraphExec to the device in \p hStream without executing it. Uploads of
* the same \p hGraphExec will be serialized. Each upload is ordered behind both any
* previous work in \p hStream and any previous launches of \p hGraphExec.
* Uses memory cached by \p stream to back the allocations owned by \p hGraphExec.
*
* \param hGraphExec - Executable graph to upload
* \param hStream    - Stream in which to upload the graph
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate,
* ::cuGraphLaunch,
* ::cuGraphExecDestroy
*/
int handle_cuGraphUpload(void *conn) {
    CUgraphExec hGraphExec;
    CUstream hStream;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphUpload(hGraphExec, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Launches an executable graph in a stream
*
* Executes \p hGraphExec in \p hStream. Only one instance of \p hGraphExec may be executing
* at a time. Each launch is ordered behind both any previous work in \p hStream
* and any previous launches of \p hGraphExec. To execute a graph concurrently, it must be
* instantiated multiple times into multiple executable graphs.
*
* If any allocations created by \p hGraphExec remain unfreed (from a previous launch) and
* \p hGraphExec was not instantiated with ::CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH,
* the launch will fail with ::CUDA_ERROR_INVALID_VALUE.
*
* \param hGraphExec - Executable graph to launch
* \param hStream    - Stream in which to launch the graph
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate,
* ::cuGraphUpload,
* ::cuGraphExecDestroy
*/
int handle_cuGraphLaunch(void *conn) {
    CUgraphExec hGraphExec;
    CUstream hStream;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphLaunch(hGraphExec, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an executable graph
*
* Destroys the executable graph specified by \p hGraphExec, as well
* as all of its executable nodes. If the executable graph is
* in-flight, it will not be terminated, but rather freed
* asynchronously on completion.
*
* \param hGraphExec - Executable graph to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate,
* ::cuGraphUpload,
* ::cuGraphLaunch
*/
int handle_cuGraphExecDestroy(void *conn) {
    CUgraphExec hGraphExec;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecDestroy(hGraphExec);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a graph
*
* Destroys the graph specified by \p hGraph, as well as all of its nodes.
*
* \param hGraph - Graph to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_VALUE
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphCreate
*/
int handle_cuGraphDestroy(void *conn) {
    CUgraph hGraph;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphDestroy(hGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Check whether an executable graph can be updated with a graph and perform the update if possible
*
* Updates the node parameters in the instantiated graph specified by \p hGraphExec with the
* node parameters in a topologically identical graph specified by \p hGraph.
*
* Limitations:
*
* - Kernel nodes:
*   - The owning context of the function cannot change.
*   - A node whose function originally did not use CUDA dynamic parallelism cannot be updated
*     to a function which uses CDP.
*   - A cooperative node cannot be updated to a non-cooperative node, and vice-versa.
*   - If the graph was instantiated with CUDA_GRAPH_INSTANTIATE_FLAG_USE_NODE_PRIORITY, the
*     priority attribute cannot change. Equality is checked on the originally requested
*     priority values, before they are clamped to the device's supported range.
*   - If \p hGraphExec was not instantiated for device launch, a node whose function originally
*     did not use device-side cudaGraphLaunch() cannot be updated to a function which uses
*     device-side cudaGraphLaunch() unless the node resides on the same context as nodes which
*     contained such calls at instantiate-time. If no such calls were present at instantiation,
*     these updates cannot be performed at all.
* - Memset and memcpy nodes:
*   - The CUDA device(s) to which the operand(s) was allocated/mapped cannot change.
*   - The source/destination memory must be allocated from the same contexts as the original
*     source/destination memory.
*   - Only 1D memsets can be changed.
* - Additional memcpy node restrictions:
*   - Changing either the source or destination memory type(i.e. CU_MEMORYTYPE_DEVICE,
*     CU_MEMORYTYPE_ARRAY, etc.) is not supported.
* - External semaphore wait nodes and record nodes:
*   - Changing the number of semaphores is not supported.
*
* Note:  The API may add further restrictions in future releases.  The return code should always be checked.
*
* cuGraphExecUpdate sets the result member of \p resultInfo to CU_GRAPH_EXEC_UPDATE_ERROR_TOPOLOGY_CHANGED
* under the following conditions:
* - The count of nodes directly in \p hGraphExec and \p hGraph differ, in which case resultInfo->errorNode
*   is set to NULL.
* - \p hGraph has more exit nodes than \p hGraph, in which case resultInfo->errorNode is set to one of
*   the exit nodes in hGraph. 
* - A node in \p hGraph has a different number of dependencies than the node from \p hGraphExec it is paired with,
*   in which case resultInfo->errorNode is set to the node from \p hGraph.
* - A node in \p hGraph has a dependency that does not match with the corresponding dependency of the paired node
*   from \p hGraphExec. resultInfo->errorNode will be set to the node from \p hGraph. resultInfo->errorFromNode
*   will be set to the mismatched dependency. The dependencies are paired based on edge order and a dependency
*   does not match when the nodes are already paired based on other edges examined in the graph.
*
* cuGraphExecUpdate sets the result member of \p resultInfo to: 
* - CU_GRAPH_EXEC_UPDATE_ERROR if passed an invalid value.
* - CU_GRAPH_EXEC_UPDATE_ERROR_TOPOLOGY_CHANGED if the graph topology changed
* - CU_GRAPH_EXEC_UPDATE_ERROR_NODE_TYPE_CHANGED if the type of a node changed, in which case
*   \p hErrorNode_out is set to the node from \p hGraph.
* - CU_GRAPH_EXEC_UPDATE_ERROR_UNSUPPORTED_FUNCTION_CHANGE if the function changed in an unsupported
*   way(see note above), in which case \p hErrorNode_out is set to the node from \p hGraph
* - CU_GRAPH_EXEC_UPDATE_ERROR_PARAMETERS_CHANGED if any parameters to a node changed in a way 
*   that is not supported, in which case \p hErrorNode_out is set to the node from \p hGraph.
* - CU_GRAPH_EXEC_UPDATE_ERROR_ATTRIBUTES_CHANGED if any attributes of a node changed in a way
*   that is not supported, in which case \p hErrorNode_out is set to the node from \p hGraph.
* - CU_GRAPH_EXEC_UPDATE_ERROR_NOT_SUPPORTED if something about a node is unsupported, like 
*   the node's type or configuration, in which case \p hErrorNode_out is set to the node from \p hGraph
*
* If the update fails for a reason not listed above, the result member of \p resultInfo will be set
* to CU_GRAPH_EXEC_UPDATE_ERROR. If the update succeeds, the result member will be set to CU_GRAPH_EXEC_UPDATE_SUCCESS.
*
* cuGraphExecUpdate returns CUDA_SUCCESS when the updated was performed successfully.  It returns
* CUDA_ERROR_GRAPH_EXEC_UPDATE_FAILURE if the graph update was not performed because it included 
* changes which violated constraints specific to instantiated graph update.
*
* \param hGraphExec The instantiated graph to be updated
* \param hGraph The graph containing the updated parameters
* \param resultInfo the error info structure 
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_GRAPH_EXEC_UPDATE_FAILURE,
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cuGraphInstantiate
*/
int handle_cuGraphExecUpdate_v2(void *conn) {
    CUgraphExec hGraphExec;
    CUgraph hGraph;
    CUgraphExecUpdateResultInfo resultInfo;

    if (rpc_read(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &resultInfo, sizeof(CUgraphExecUpdateResultInfo)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphExecUpdate_v2(hGraphExec, hGraph, &resultInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resultInfo, sizeof(CUgraphExecUpdateResultInfo)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies attributes from source node to destination node.
*
* Copies attributes from source node \p src to destination node \p dst.
* Both node must have the same context.
*
* \param[out] dst Destination node
* \param[in] src Source node
* For list of attributes see ::CUkernelNodeAttrID
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuGraphKernelNodeCopyAttributes(void *conn) {
    CUgraphNode dst;
    CUgraphNode src;

    if (rpc_read(conn, &dst, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &src, sizeof(CUgraphNode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphKernelNodeCopyAttributes(dst, src);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries node attribute.
* 
* Queries attribute \p attr from node \p hNode and stores it in corresponding
* member of \p value_out.
*
* \param[in] hNode
* \param[in] attr
* \param[out] value_out 
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*  
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuGraphKernelNodeGetAttribute(void *conn) {
    CUgraphNode hNode;
    CUkernelNodeAttrID attr;
    CUkernelNodeAttrValue value_out;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUkernelNodeAttrID)) < 0 ||
        rpc_read(conn, &value_out, sizeof(CUkernelNodeAttrValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphKernelNodeGetAttribute(hNode, attr, &value_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value_out, sizeof(CUkernelNodeAttrValue)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets node attribute.
* 
* Sets attribute \p attr on node \p hNode from corresponding attribute of
* \p value.
*
* \param[out] hNode
* \param[in] attr
* \param[out] value
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE
* \notefnerr
*
* \sa
* ::CUaccessPolicyWindow
*/
int handle_cuGraphKernelNodeSetAttribute(void *conn) {
    CUgraphNode hNode;
    CUkernelNodeAttrID attr;
    CUkernelNodeAttrValue value;

    if (rpc_read(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &attr, sizeof(CUkernelNodeAttrID)) < 0 ||
        rpc_read(conn, &value, sizeof(CUkernelNodeAttrValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphKernelNodeSetAttribute(hNode, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Write a DOT file describing graph structure
*
* Using the provided \p hGraph, write to \p path a DOT formatted description of the graph.
* By default this includes the graph topology, node types, node id, kernel names and memcpy direction.
* \p flags can be specified to write more detailed information about each node type such as
* parameter values, kernel attributes, node and function handles.
*
* \param hGraph - The graph to create a DOT file from
* \param path   - The path to write the DOT file to
* \param flags  - Flags from CUgraphDebugDot_flags for specifying which additional node information to write
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_OPERATING_SYSTEM
*/
int handle_cuGraphDebugDotPrint(void *conn) {
    CUgraph hGraph;
    char path;
    unsigned int flags;

    if (rpc_read(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &path, sizeof(char)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphDebugDotPrint(hGraph, &path, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Create a user object
*
* Create a user object with the specified destructor callback and initial reference count. The
* initial references are owned by the caller.
*
* Destructor callbacks cannot make CUDA API calls and should avoid blocking behavior, as they
* are executed by a shared internal thread. Another thread may be signaled to perform such
* actions, if it does not block forward progress of tasks scheduled through CUDA.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object_out      - Location to return the user object handle
* \param ptr             - The pointer to pass to the destroy function
* \param destroy         - Callback to free the user object when it is no longer in use
* \param initialRefcount - The initial refcount to create the object with, typically 1. The
*                          initial references are owned by the calling thread.
* \param flags           - Currently it is required to pass ::CU_USER_OBJECT_NO_DESTRUCTOR_SYNC,
*                          which is the only defined flag. This indicates that the destroy
*                          callback cannot be waited on by any CUDA API. Users requiring
*                          synchronization of the callback should signal its completion
*                          manually.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuUserObjectRetain,
* ::cuUserObjectRelease,
* ::cuGraphRetainUserObject,
* ::cuGraphReleaseUserObject,
* ::cuGraphCreate
*/
int handle_cuUserObjectCreate(void *conn) {
    CUuserObject object_out;
    void* ptr;
    CUhostFn destroy;
    unsigned int initialRefcount;
    unsigned int flags;

    if (rpc_read(conn, &object_out, sizeof(CUuserObject)) < 0 ||
        rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &destroy, sizeof(CUhostFn)) < 0 ||
        rpc_read(conn, &initialRefcount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuUserObjectCreate(&object_out, &ptr, destroy, initialRefcount, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &object_out, sizeof(CUuserObject)) < 0 ||
        rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Retain a reference to a user object
*
* Retains new references to a user object. The new references are owned by the caller.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object - The object to retain
* \param count  - The number of references to retain, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuUserObjectCreate,
* ::cuUserObjectRelease,
* ::cuGraphRetainUserObject,
* ::cuGraphReleaseUserObject,
* ::cuGraphCreate
*/
int handle_cuUserObjectRetain(void *conn) {
    CUuserObject object;
    unsigned int count;

    if (rpc_read(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuUserObjectRetain(object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Release a reference to a user object
*
* Releases user object references owned by the caller. The object's destructor is invoked if
* the reference count reaches zero.
*
* It is undefined behavior to release references not owned by the caller, or to use a user
* object handle after all references are released.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object - The object to release
* \param count  - The number of references to release, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuUserObjectCreate,
* ::cuUserObjectRetain,
* ::cuGraphRetainUserObject,
* ::cuGraphReleaseUserObject,
* ::cuGraphCreate
*/
int handle_cuUserObjectRelease(void *conn) {
    CUuserObject object;
    unsigned int count;

    if (rpc_read(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuUserObjectRelease(object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Retain a reference to a user object from a graph
*
* Creates or moves user object references that will be owned by a CUDA graph.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param graph  - The graph to associate the reference with
* \param object - The user object to retain a reference for
* \param count  - The number of references to add to the graph, typically 1. Must be
*                 nonzero and not larger than INT_MAX.
* \param flags  - The optional flag ::CU_GRAPH_USER_OBJECT_MOVE transfers references
*                 from the calling thread, rather than create new references. Pass 0
*                 to create new references.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuUserObjectCreate,
* ::cuUserObjectRetain,
* ::cuUserObjectRelease,
* ::cuGraphReleaseUserObject,
* ::cuGraphCreate
*/
int handle_cuGraphRetainUserObject(void *conn) {
    CUgraph graph;
    CUuserObject object;
    unsigned int count;
    unsigned int flags;

    if (rpc_read(conn, &graph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphRetainUserObject(graph, object, count, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Release a user object reference from a graph
*
* Releases user object references owned by a graph.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param graph  - The graph that will release the reference
* \param object - The user object to release a reference for
* \param count  - The number of references to release, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuUserObjectCreate,
* ::cuUserObjectRetain,
* ::cuUserObjectRelease,
* ::cuGraphRetainUserObject,
* ::cuGraphCreate
*/
int handle_cuGraphReleaseUserObject(void *conn) {
    CUgraph graph;
    CUuserObject object;
    unsigned int count;

    if (rpc_read(conn, &graph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphReleaseUserObject(graph, object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns occupancy of a function
*
* Returns in \p *numBlocks the number of the maximum active blocks per
* streaming multiprocessor.
*
* \param numBlocks       - Returned occupancy
* \param func            - Kernel for which occupancy is calculated
* \param blockSize       - Block size the kernel is intended to be launched with
* \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
*/
int handle_cuOccupancyMaxActiveBlocksPerMultiprocessor(void *conn) {
    int numBlocks;
    CUfunction func;
    int blockSize;
    size_t dynamicSMemSize;

    if (rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_read(conn, &dynamicSMemSize, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, func, blockSize, dynamicSMemSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numBlocks, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns occupancy of a function
*
* Returns in \p *numBlocks the number of the maximum active blocks per
* streaming multiprocessor.
*
* The \p Flags parameter controls how special cases are handled. The
* valid flags are:
*
* - ::CU_OCCUPANCY_DEFAULT, which maintains the default behavior as
*   ::cuOccupancyMaxActiveBlocksPerMultiprocessor;
*
* - ::CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE, which suppresses the
*   default behavior on platform where global caching affects
*   occupancy. On such platforms, if caching is enabled, but
*   per-block SM resource usage would result in zero occupancy, the
*   occupancy calculator will calculate the occupancy as if caching
*   is disabled. Setting ::CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE makes
*   the occupancy calculator to return 0 in such cases. More information
*   can be found about this feature in the "Unified L1/Texture Cache"
*   section of the Maxwell tuning guide.
*
* \param numBlocks       - Returned occupancy
* \param func            - Kernel for which occupancy is calculated
* \param blockSize       - Block size the kernel is intended to be launched with
* \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
* \param flags           - Requested behavior for the occupancy calculator
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
*/
int handle_cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(void *conn) {
    int numBlocks;
    CUfunction func;
    int blockSize;
    size_t dynamicSMemSize;
    unsigned int flags;

    if (rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_read(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(&numBlocks, func, blockSize, dynamicSMemSize, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numBlocks, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns dynamic shared memory available per block when launching \p numBlocks blocks on SM 
*
* Returns in \p *dynamicSmemSize the maximum size of dynamic shared memory to allow \p numBlocks blocks per SM. 
*
* \param dynamicSmemSize - Returned maximum dynamic shared memory 
* \param func            - Kernel function for which occupancy is calculated
* \param numBlocks       - Number of blocks to fit on SM 
* \param blockSize       - Size of the blocks
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*/
int handle_cuOccupancyAvailableDynamicSMemPerBlock(void *conn) {
    size_t dynamicSmemSize;
    CUfunction func;
    int numBlocks;
    int blockSize;

    if (rpc_read(conn, &dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuOccupancyAvailableDynamicSMemPerBlock(&dynamicSmemSize, func, numBlocks, blockSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dynamicSmemSize, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Given the kernel function (\p func) and launch configuration
* (\p config), return the maximum cluster size in \p *clusterSize.
*
* The cluster dimensions in \p config are ignored. If func has a required
* cluster size set (see ::cudaFuncGetAttributes / ::cuFuncGetAttribute),\p
* *clusterSize will reflect the required cluster size.
*
* By default this function will always return a value that's portable on
* future hardware. A higher value may be returned if the kernel function
* allows non-portable cluster sizes.
*
* This function will respect the compile time launch bounds.
*
* \param clusterSize - Returned maximum cluster size that can be launched
*                      for the given kernel function and launch configuration
* \param func        - Kernel function for which maximum cluster
*                      size is calculated
* \param config      - Launch configuration for the given kernel function
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cudaFuncGetAttributes,
* ::cuFuncGetAttribute
*/
int handle_cuOccupancyMaxPotentialClusterSize(void *conn) {
    int clusterSize;
    CUfunction func;
    CUlaunchConfig config;

    if (rpc_read(conn, &clusterSize, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &config, sizeof(CUlaunchConfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuOccupancyMaxPotentialClusterSize(&clusterSize, func, &config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clusterSize, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Given the kernel function (\p func) and launch configuration
* (\p config), return the maximum number of clusters that could co-exist
* on the target device in \p *numClusters.
*
* If the function has required cluster size already set (see
* ::cudaFuncGetAttributes / ::cuFuncGetAttribute), the cluster size
* from config must either be unspecified or match the required size.
* Without required sizes, the cluster size must be specified in config,
* else the function will return an error.
*
* Note that various attributes of the kernel function may affect occupancy
* calculation. Runtime environment may affect how the hardware schedules
* the clusters, so the calculated occupancy is not guaranteed to be achievable.
*
* \param numClusters - Returned maximum number of clusters that
*                      could co-exist on the target device
* \param func        - Kernel function for which maximum number
*                      of clusters are calculated
* \param config      - Launch configuration for the given kernel function
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_CLUSTER_SIZE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cudaFuncGetAttributes,
* ::cuFuncGetAttribute
*/
int handle_cuOccupancyMaxActiveClusters(void *conn) {
    int numClusters;
    CUfunction func;
    CUlaunchConfig config;

    if (rpc_read(conn, &numClusters, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &config, sizeof(CUlaunchConfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuOccupancyMaxActiveClusters(&numClusters, func, &config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numClusters, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Binds an array as a texture reference
*
* \deprecated
*
* Binds the CUDA array \p hArray to the texture reference \p hTexRef. Any
* previous address or CUDA array state associated with the texture reference
* is superseded by this function. \p Flags must be set to
* ::CU_TRSA_OVERRIDE_FORMAT. Any CUDA array previously bound to \p hTexRef is
* unbound.
*
* \param hTexRef - Texture reference to bind
* \param hArray  - Array to bind
* \param Flags   - Options (must be ::CU_TRSA_OVERRIDE_FORMAT)
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetArray(void *conn) {
    CUtexref hTexRef;
    CUarray hArray;
    unsigned int Flags;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetArray(hTexRef, hArray, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Binds a mipmapped array to a texture reference
*
* \deprecated
*
* Binds the CUDA mipmapped array \p hMipmappedArray to the texture reference \p hTexRef.
* Any previous address or CUDA array state associated with the texture reference
* is superseded by this function. \p Flags must be set to ::CU_TRSA_OVERRIDE_FORMAT.
* Any CUDA array previously bound to \p hTexRef is unbound.
*
* \param hTexRef         - Texture reference to bind
* \param hMipmappedArray - Mipmapped array to bind
* \param Flags           - Options (must be ::CU_TRSA_OVERRIDE_FORMAT)
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetMipmappedArray(void *conn) {
    CUtexref hTexRef;
    CUmipmappedArray hMipmappedArray;
    unsigned int Flags;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetMipmappedArray(hTexRef, hMipmappedArray, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Binds an address as a texture reference
*
* \deprecated
*
* Binds a linear address range to the texture reference \p hTexRef. Any
* previous address or CUDA array state associated with the texture reference
* is superseded by this function. Any memory previously bound to \p hTexRef
* is unbound.
*
* Since the hardware enforces an alignment requirement on texture base
* addresses, ::cuTexRefSetAddress() passes back a byte offset in
* \p *ByteOffset that must be applied to texture fetches in order to read from
* the desired memory. This offset must be divided by the texel size and
* passed to kernels that read from the texture so they can be applied to the
* ::tex1Dfetch() function.
*
* If the device memory pointer was returned from ::cuMemAlloc(), the offset
* is guaranteed to be 0 and NULL may be passed as the \p ByteOffset parameter.
*
* The total number of elements (or texels) in the linear address range
* cannot exceed ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH.
* The number of elements is computed as (\p bytes / bytesPerElement),
* where bytesPerElement is determined from the data format and number of
* components set using ::cuTexRefSetFormat().
*
* \param ByteOffset - Returned byte offset
* \param hTexRef    - Texture reference to bind
* \param dptr       - Device pointer to bind
* \param bytes      - Size of memory to bind in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetAddress_v2(void *conn) {
    size_t ByteOffset;
    CUtexref hTexRef;
    CUdeviceptr dptr;
    size_t bytes;

    if (rpc_read(conn, &ByteOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &bytes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetAddress_v2(&ByteOffset, hTexRef, dptr, bytes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ByteOffset, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Binds an address as a 2D texture reference
*
* \deprecated
*
* Binds a linear address range to the texture reference \p hTexRef. Any
* previous address or CUDA array state associated with the texture reference
* is superseded by this function. Any memory previously bound to \p hTexRef
* is unbound.
*
* Using a ::tex2D() function inside a kernel requires a call to either
* ::cuTexRefSetArray() to bind the corresponding texture reference to an
* array, or ::cuTexRefSetAddress2D() to bind the texture reference to linear
* memory.
*
* Function calls to ::cuTexRefSetFormat() cannot follow calls to
* ::cuTexRefSetAddress2D() for the same texture reference.
*
* It is required that \p dptr be aligned to the appropriate hardware-specific
* texture alignment. You can query this value using the device attribute
* ::CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT. If an unaligned \p dptr is
* supplied, ::CUDA_ERROR_INVALID_VALUE is returned.
*
* \p Pitch has to be aligned to the hardware-specific texture pitch alignment.
* This value can be queried using the device attribute
* ::CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT. If an unaligned \p Pitch is
* supplied, ::CUDA_ERROR_INVALID_VALUE is returned.
*
* Width and Height, which are specified in elements (or texels), cannot exceed
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH and
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT respectively.
* \p Pitch, which is specified in bytes, cannot exceed
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH.
*
* \param hTexRef - Texture reference to bind
* \param desc    - Descriptor of CUDA array
* \param dptr    - Device pointer to bind
* \param Pitch   - Line pitch in bytes
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetAddress2D_v3(void *conn) {
    CUtexref hTexRef;
    CUDA_ARRAY_DESCRIPTOR desc;
    CUdeviceptr dptr;
    size_t Pitch;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &desc, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &Pitch, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetAddress2D_v3(hTexRef, &desc, dptr, Pitch);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the format for a texture reference
*
* \deprecated
*
* Specifies the format of the data to be read by the texture reference
* \p hTexRef. \p fmt and \p NumPackedComponents are exactly analogous to the
* ::Format and ::NumChannels members of the ::CUDA_ARRAY_DESCRIPTOR structure:
* They specify the format of each component and the number of components per
* array element.
*
* \param hTexRef             - Texture reference
* \param fmt                 - Format to set
* \param NumPackedComponents - Number of components per array element
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat,
* ::cudaCreateChannelDesc
*/
int handle_cuTexRefSetFormat(void *conn) {
    CUtexref hTexRef;
    CUarray_format fmt;
    int NumPackedComponents;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &fmt, sizeof(CUarray_format)) < 0 ||
        rpc_read(conn, &NumPackedComponents, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetFormat(hTexRef, fmt, NumPackedComponents);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the addressing mode for a texture reference
*
* \deprecated
*
* Specifies the addressing mode \p am for the given dimension \p dim of the
* texture reference \p hTexRef. If \p dim is zero, the addressing mode is
* applied to the first parameter of the functions used to fetch from the
* texture; if \p dim is 1, the second, and so on. ::CUaddress_mode is defined
* as:
* \code
   typedef enum CUaddress_mode_enum {
      CU_TR_ADDRESS_MODE_WRAP = 0,
      CU_TR_ADDRESS_MODE_CLAMP = 1,
      CU_TR_ADDRESS_MODE_MIRROR = 2,
      CU_TR_ADDRESS_MODE_BORDER = 3
   } CUaddress_mode;
* \endcode
*
* Note that this call has no effect if \p hTexRef is bound to linear memory.
* Also, if the flag, ::CU_TRSF_NORMALIZED_COORDINATES, is not set, the only
* supported address mode is ::CU_TR_ADDRESS_MODE_CLAMP.
*
* \param hTexRef - Texture reference
* \param dim     - Dimension
* \param am      - Addressing mode to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetAddressMode(void *conn) {
    CUtexref hTexRef;
    int dim;
    CUaddress_mode am;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &dim, sizeof(int)) < 0 ||
        rpc_read(conn, &am, sizeof(CUaddress_mode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetAddressMode(hTexRef, dim, am);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the filtering mode for a texture reference
*
* \deprecated
*
* Specifies the filtering mode \p fm to be used when reading memory through
* the texture reference \p hTexRef. ::CUfilter_mode_enum is defined as:
*
* \code
   typedef enum CUfilter_mode_enum {
      CU_TR_FILTER_MODE_POINT = 0,
      CU_TR_FILTER_MODE_LINEAR = 1
   } CUfilter_mode;
* \endcode
*
* Note that this call has no effect if \p hTexRef is bound to linear memory.
*
* \param hTexRef - Texture reference
* \param fm      - Filtering mode to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetFilterMode(void *conn) {
    CUtexref hTexRef;
    CUfilter_mode fm;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &fm, sizeof(CUfilter_mode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetFilterMode(hTexRef, fm);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the mipmap filtering mode for a texture reference
*
* \deprecated
*
* Specifies the mipmap filtering mode \p fm to be used when reading memory through
* the texture reference \p hTexRef. ::CUfilter_mode_enum is defined as:
*
* \code
   typedef enum CUfilter_mode_enum {
      CU_TR_FILTER_MODE_POINT = 0,
      CU_TR_FILTER_MODE_LINEAR = 1
   } CUfilter_mode;
* \endcode
*
* Note that this call has no effect if \p hTexRef is not bound to a mipmapped array.
*
* \param hTexRef - Texture reference
* \param fm      - Filtering mode to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetMipmapFilterMode(void *conn) {
    CUtexref hTexRef;
    CUfilter_mode fm;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &fm, sizeof(CUfilter_mode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetMipmapFilterMode(hTexRef, fm);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the mipmap level bias for a texture reference
*
* \deprecated
*
* Specifies the mipmap level bias \p bias to be added to the specified mipmap level when
* reading memory through the texture reference \p hTexRef.
*
* Note that this call has no effect if \p hTexRef is not bound to a mipmapped array.
*
* \param hTexRef - Texture reference
* \param bias    - Mipmap level bias
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetMipmapLevelBias(void *conn) {
    CUtexref hTexRef;
    float bias;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &bias, sizeof(float)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetMipmapLevelBias(hTexRef, bias);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the mipmap min/max mipmap level clamps for a texture reference
*
* \deprecated
*
* Specifies the min/max mipmap level clamps, \p minMipmapLevelClamp and \p maxMipmapLevelClamp
* respectively, to be used when reading memory through the texture reference
* \p hTexRef.
*
* Note that this call has no effect if \p hTexRef is not bound to a mipmapped array.
*
* \param hTexRef        - Texture reference
* \param minMipmapLevelClamp - Mipmap min level clamp
* \param maxMipmapLevelClamp - Mipmap max level clamp
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetMipmapLevelClamp(void *conn) {
    CUtexref hTexRef;
    float minMipmapLevelClamp;
    float maxMipmapLevelClamp;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &minMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_read(conn, &maxMipmapLevelClamp, sizeof(float)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetMipmapLevelClamp(hTexRef, minMipmapLevelClamp, maxMipmapLevelClamp);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the maximum anisotropy for a texture reference
*
* \deprecated
*
* Specifies the maximum anisotropy \p maxAniso to be used when reading memory through
* the texture reference \p hTexRef.
*
* Note that this call has no effect if \p hTexRef is bound to linear memory.
*
* \param hTexRef  - Texture reference
* \param maxAniso - Maximum anisotropy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetMaxAnisotropy(void *conn) {
    CUtexref hTexRef;
    unsigned int maxAniso;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &maxAniso, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetMaxAnisotropy(hTexRef, maxAniso);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the border color for a texture reference
*
* \deprecated
*
* Specifies the value of the RGBA color via the \p pBorderColor to the texture reference
* \p hTexRef. The color value supports only float type and holds color components in
* the following sequence:
* pBorderColor[0] holds 'R' component
* pBorderColor[1] holds 'G' component
* pBorderColor[2] holds 'B' component
* pBorderColor[3] holds 'A' component
*
* Note that the color values can be set only when the Address mode is set to
* CU_TR_ADDRESS_MODE_BORDER using ::cuTexRefSetAddressMode.
* Applications using integer border color values have to "reinterpret_cast" their values to float.
*
* \param hTexRef       - Texture reference
* \param pBorderColor  - RGBA color
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddressMode,
* ::cuTexRefGetAddressMode, ::cuTexRefGetBorderColor
*/
int handle_cuTexRefSetBorderColor(void *conn) {
    CUtexref hTexRef;
    float pBorderColor;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &pBorderColor, sizeof(float)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetBorderColor(hTexRef, &pBorderColor);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pBorderColor, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the flags for a texture reference
*
* \deprecated
*
* Specifies optional flags via \p Flags to specify the behavior of data
* returned through the texture reference \p hTexRef. The valid flags are:
*
* - ::CU_TRSF_READ_AS_INTEGER, which suppresses the default behavior of
*   having the texture promote integer data to floating point data in the
*   range [0, 1]. Note that texture with 32-bit integer format
*   would not be promoted, regardless of whether or not this
*   flag is specified;
* - ::CU_TRSF_NORMALIZED_COORDINATES, which suppresses the
*   default behavior of having the texture coordinates range
*   from [0, Dim) where Dim is the width or height of the CUDA
*   array. Instead, the texture coordinates [0, 1.0) reference
*   the entire breadth of the array dimension;
* - ::CU_TRSF_DISABLE_TRILINEAR_OPTIMIZATION, which disables any trilinear
*   filtering optimizations. Trilinear optimizations improve texture filtering
*   performance by allowing bilinear filtering on textures in scenarios where
*   it can closely approximate the expected results.
*
* \param hTexRef - Texture reference
* \param Flags   - Optional flags to set
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefSetFlags(void *conn) {
    CUtexref hTexRef;
    unsigned int Flags;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefSetFlags(hTexRef, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the address associated with a texture reference
*
* \deprecated
*
* Returns in \p *pdptr the base address bound to the texture reference
* \p hTexRef, or returns ::CUDA_ERROR_INVALID_VALUE if the texture reference
* is not bound to any device memory range.
*
* \param pdptr   - Returned device address
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetAddress_v2(void *conn) {
    CUdeviceptr pdptr;
    CUtexref hTexRef;

    if (rpc_read(conn, &pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetAddress_v2(&pdptr, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pdptr, sizeof(CUdeviceptr)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the array bound to a texture reference
*
* \deprecated
*
* Returns in \p *phArray the CUDA array bound to the texture reference
* \p hTexRef, or returns ::CUDA_ERROR_INVALID_VALUE if the texture reference
* is not bound to any CUDA array.
*
* \param phArray - Returned array
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetArray(void *conn) {
    CUarray phArray;
    CUtexref hTexRef;

    if (rpc_read(conn, &phArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetArray(&phArray, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phArray, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the mipmapped array bound to a texture reference
*
* \deprecated
*
* Returns in \p *phMipmappedArray the CUDA mipmapped array bound to the texture
* reference \p hTexRef, or returns ::CUDA_ERROR_INVALID_VALUE if the texture reference
* is not bound to any CUDA mipmapped array.
*
* \param phMipmappedArray - Returned mipmapped array
* \param hTexRef          - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetMipmappedArray(void *conn) {
    CUmipmappedArray phMipmappedArray;
    CUtexref hTexRef;

    if (rpc_read(conn, &phMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetMipmappedArray(&phMipmappedArray, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phMipmappedArray, sizeof(CUmipmappedArray)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the addressing mode used by a texture reference
*
* \deprecated
*
* Returns in \p *pam the addressing mode corresponding to the
* dimension \p dim of the texture reference \p hTexRef. Currently, the only
* valid value for \p dim are 0 and 1.
*
* \param pam     - Returned addressing mode
* \param hTexRef - Texture reference
* \param dim     - Dimension
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetAddressMode(void *conn) {
    CUaddress_mode pam;
    CUtexref hTexRef;
    int dim;

    if (rpc_read(conn, &pam, sizeof(CUaddress_mode)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &dim, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetAddressMode(&pam, hTexRef, dim);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pam, sizeof(CUaddress_mode)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the filter-mode used by a texture reference
*
* \deprecated
*
* Returns in \p *pfm the filtering mode of the texture reference
* \p hTexRef.
*
* \param pfm     - Returned filtering mode
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetFilterMode(void *conn) {
    CUfilter_mode pfm;
    CUtexref hTexRef;

    if (rpc_read(conn, &pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetFilterMode(&pfm, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pfm, sizeof(CUfilter_mode)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the format used by a texture reference
*
* \deprecated
*
* Returns in \p *pFormat and \p *pNumChannels the format and number
* of components of the CUDA array bound to the texture reference \p hTexRef.
* If \p pFormat or \p pNumChannels is NULL, it will be ignored.
*
* \param pFormat      - Returned format
* \param pNumChannels - Returned number of components
* \param hTexRef      - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags
*/
int handle_cuTexRefGetFormat(void *conn) {
    CUarray_format pFormat;
    int pNumChannels;
    CUtexref hTexRef;

    if (rpc_read(conn, &pFormat, sizeof(CUarray_format)) < 0 ||
        rpc_read(conn, &pNumChannels, sizeof(int)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetFormat(&pFormat, &pNumChannels, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pFormat, sizeof(CUarray_format)) < 0 ||
        rpc_write(conn, &pNumChannels, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the mipmap filtering mode for a texture reference
*
* \deprecated
*
* Returns the mipmap filtering mode in \p pfm that's used when reading memory through
* the texture reference \p hTexRef.
*
* \param pfm     - Returned mipmap filtering mode
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetMipmapFilterMode(void *conn) {
    CUfilter_mode pfm;
    CUtexref hTexRef;

    if (rpc_read(conn, &pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetMipmapFilterMode(&pfm, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pfm, sizeof(CUfilter_mode)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the mipmap level bias for a texture reference
*
* \deprecated
*
* Returns the mipmap level bias in \p pBias that's added to the specified mipmap
* level when reading memory through the texture reference \p hTexRef.
*
* \param pbias   - Returned mipmap level bias
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetMipmapLevelBias(void *conn) {
    float pbias;
    CUtexref hTexRef;

    if (rpc_read(conn, &pbias, sizeof(float)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetMipmapLevelBias(&pbias, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pbias, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the min/max mipmap level clamps for a texture reference
*
* \deprecated
*
* Returns the min/max mipmap level clamps in \p pminMipmapLevelClamp and \p pmaxMipmapLevelClamp
* that's used when reading memory through the texture reference \p hTexRef.
*
* \param pminMipmapLevelClamp - Returned mipmap min level clamp
* \param pmaxMipmapLevelClamp - Returned mipmap max level clamp
* \param hTexRef              - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetMipmapLevelClamp(void *conn) {
    float pminMipmapLevelClamp;
    float pmaxMipmapLevelClamp;
    CUtexref hTexRef;

    if (rpc_read(conn, &pminMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_read(conn, &pmaxMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetMipmapLevelClamp(&pminMipmapLevelClamp, &pmaxMipmapLevelClamp, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pminMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_write(conn, &pmaxMipmapLevelClamp, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the maximum anisotropy for a texture reference
*
* \deprecated
*
* Returns the maximum anisotropy in \p pmaxAniso that's used when reading memory through
* the texture reference \p hTexRef.
*
* \param pmaxAniso - Returned maximum anisotropy
* \param hTexRef   - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFlags, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetMaxAnisotropy(void *conn) {
    int pmaxAniso;
    CUtexref hTexRef;

    if (rpc_read(conn, &pmaxAniso, sizeof(int)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetMaxAnisotropy(&pmaxAniso, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pmaxAniso, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the border color used by a texture reference
*
* \deprecated
*
* Returns in \p pBorderColor, values of the RGBA color used by
* the texture reference \p hTexRef.
* The color value is of type float and holds color components in
* the following sequence:
* pBorderColor[0] holds 'R' component
* pBorderColor[1] holds 'G' component
* pBorderColor[2] holds 'B' component
* pBorderColor[3] holds 'A' component
*
* \param hTexRef  - Texture reference
* \param pBorderColor   - Returned Type and Value of RGBA color
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddressMode,
* ::cuTexRefSetAddressMode, ::cuTexRefSetBorderColor
*/
int handle_cuTexRefGetBorderColor(void *conn) {
    float pBorderColor;
    CUtexref hTexRef;

    if (rpc_read(conn, &pBorderColor, sizeof(float)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetBorderColor(&pBorderColor, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pBorderColor, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the flags used by a texture reference
*
* \deprecated
*
* Returns in \p *pFlags the flags of the texture reference \p hTexRef.
*
* \param pFlags  - Returned flags
* \param hTexRef - Texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefSetAddress,
* ::cuTexRefSetAddress2D, ::cuTexRefSetAddressMode, ::cuTexRefSetArray,
* ::cuTexRefSetFilterMode, ::cuTexRefSetFlags, ::cuTexRefSetFormat,
* ::cuTexRefGetAddress, ::cuTexRefGetAddressMode, ::cuTexRefGetArray,
* ::cuTexRefGetFilterMode, ::cuTexRefGetFormat
*/
int handle_cuTexRefGetFlags(void *conn) {
    unsigned int pFlags;
    CUtexref hTexRef;

    if (rpc_read(conn, &pFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefGetFlags(&pFlags, hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pFlags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a texture reference
*
* \deprecated
*
* Creates a texture reference and returns its handle in \p *pTexRef. Once
* created, the application must call ::cuTexRefSetArray() or
* ::cuTexRefSetAddress() to associate the reference with allocated memory.
* Other texture reference functions are used to specify the format and
* interpretation (addressing, filtering, etc.) to be used when the memory is
* read through this texture reference.
*
* \param pTexRef - Returned texture reference
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefDestroy
*/
int handle_cuTexRefCreate(void *conn) {
    CUtexref pTexRef;

    if (rpc_read(conn, &pTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefCreate(&pTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexRef, sizeof(CUtexref)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a texture reference
*
* \deprecated
*
* Destroys the texture reference specified by \p hTexRef.
*
* \param hTexRef - Texture reference to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuTexRefCreate
*/
int handle_cuTexRefDestroy(void *conn) {
    CUtexref hTexRef;

    if (rpc_read(conn, &hTexRef, sizeof(CUtexref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexRefDestroy(hTexRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the CUDA array for a surface reference.
*
* \deprecated
*
* Sets the CUDA array \p hArray to be read and written by the surface reference
* \p hSurfRef.  Any previous CUDA array state associated with the surface
* reference is superseded by this function.  \p Flags must be set to 0.
* The ::CUDA_ARRAY3D_SURFACE_LDST flag must have been set for the CUDA array.
* Any CUDA array previously bound to \p hSurfRef is unbound.
* \param hSurfRef - Surface reference handle
* \param hArray - CUDA array handle
* \param Flags - set to 0
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuModuleGetSurfRef,
* ::cuSurfRefGetArray
*/
int handle_cuSurfRefSetArray(void *conn) {
    CUsurfref hSurfRef;
    CUarray hArray;
    unsigned int Flags;

    if (rpc_read(conn, &hSurfRef, sizeof(CUsurfref)) < 0 ||
        rpc_read(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSurfRefSetArray(hSurfRef, hArray, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Passes back the CUDA array bound to a surface reference.
*
* \deprecated
*
* Returns in \p *phArray the CUDA array bound to the surface reference
* \p hSurfRef, or returns ::CUDA_ERROR_INVALID_VALUE if the surface reference
* is not bound to any CUDA array.
* \param phArray - Surface reference handle
* \param hSurfRef - Surface reference handle
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa ::cuModuleGetSurfRef, ::cuSurfRefSetArray
*/
int handle_cuSurfRefGetArray(void *conn) {
    CUarray phArray;
    CUsurfref hSurfRef;

    if (rpc_read(conn, &phArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &hSurfRef, sizeof(CUsurfref)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSurfRefGetArray(&phArray, hSurfRef);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &phArray, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a texture object
*
* Creates a texture object and returns it in \p pTexObject. \p pResDesc describes
* the data to texture from. \p pTexDesc describes how the data should be sampled.
* \p pResViewDesc is an optional argument that specifies an alternate format for
* the data described by \p pResDesc, and also describes the subresource region
* to restrict access to when texturing. \p pResViewDesc can only be specified if
* the type of resource is a CUDA array or a CUDA mipmapped array.
*
* Texture objects are only supported on devices of compute capability 3.0 or higher.
* Additionally, a texture object is an opaque value, and, as such, should only be
* accessed through CUDA API calls.
*
* The ::CUDA_RESOURCE_DESC structure is defined as:
* \code
        typedef struct CUDA_RESOURCE_DESC_st
        {
            CUresourcetype resType;
            union {
                struct {
                    CUarray hArray;
                } array;
                struct {
                    CUmipmappedArray hMipmappedArray;
                } mipmap;
                struct {
                    CUdeviceptr devPtr;
                    CUarray_format format;
                    unsigned int numChannels;
                    size_t sizeInBytes;
                } linear;
                struct {
                    CUdeviceptr devPtr;
                    CUarray_format format;
                    unsigned int numChannels;
                    size_t width;
                    size_t height;
                    size_t pitchInBytes;
                } pitch2D;
            } res;
            unsigned int flags;
        } CUDA_RESOURCE_DESC;
* \endcode
* where:
* - ::CUDA_RESOURCE_DESC::resType specifies the type of resource to texture from.
* CUresourceType is defined as:
* \code
        typedef enum CUresourcetype_enum {
            CU_RESOURCE_TYPE_ARRAY           = 0x00,
            CU_RESOURCE_TYPE_MIPMAPPED_ARRAY = 0x01,
            CU_RESOURCE_TYPE_LINEAR          = 0x02,
            CU_RESOURCE_TYPE_PITCH2D         = 0x03
        } CUresourcetype;
* \endcode
*
* \par
* If ::CUDA_RESOURCE_DESC::resType is set to ::CU_RESOURCE_TYPE_ARRAY, ::CUDA_RESOURCE_DESC::res::array::hArray
* must be set to a valid CUDA array handle.
*
* \par
* If ::CUDA_RESOURCE_DESC::resType is set to ::CU_RESOURCE_TYPE_MIPMAPPED_ARRAY, ::CUDA_RESOURCE_DESC::res::mipmap::hMipmappedArray
* must be set to a valid CUDA mipmapped array handle.
*
* \par
* If ::CUDA_RESOURCE_DESC::resType is set to ::CU_RESOURCE_TYPE_LINEAR, ::CUDA_RESOURCE_DESC::res::linear::devPtr
* must be set to a valid device pointer, that is aligned to ::CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT.
* ::CUDA_RESOURCE_DESC::res::linear::format and ::CUDA_RESOURCE_DESC::res::linear::numChannels
* describe the format of each component and the number of components per array element. ::CUDA_RESOURCE_DESC::res::linear::sizeInBytes
* specifies the size of the array in bytes. The total number of elements in the linear address range cannot exceed
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH. The number of elements is computed as (sizeInBytes / (sizeof(format) * numChannels)).
*
* \par
* If ::CUDA_RESOURCE_DESC::resType is set to ::CU_RESOURCE_TYPE_PITCH2D, ::CUDA_RESOURCE_DESC::res::pitch2D::devPtr
* must be set to a valid device pointer, that is aligned to ::CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT.
* ::CUDA_RESOURCE_DESC::res::pitch2D::format and ::CUDA_RESOURCE_DESC::res::pitch2D::numChannels
* describe the format of each component and the number of components per array element. ::CUDA_RESOURCE_DESC::res::pitch2D::width
* and ::CUDA_RESOURCE_DESC::res::pitch2D::height specify the width and height of the array in elements, and cannot exceed
* ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH and ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT respectively.
* ::CUDA_RESOURCE_DESC::res::pitch2D::pitchInBytes specifies the pitch between two rows in bytes and has to be aligned to
* ::CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT. Pitch cannot exceed ::CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH.
*
* - ::flags must be set to zero.
*
*
* The ::CUDA_TEXTURE_DESC struct is defined as
* \code
        typedef struct CUDA_TEXTURE_DESC_st {
            CUaddress_mode addressMode[3];
            CUfilter_mode filterMode;
            unsigned int flags;
            unsigned int maxAnisotropy;
            CUfilter_mode mipmapFilterMode;
            float mipmapLevelBias;
            float minMipmapLevelClamp;
            float maxMipmapLevelClamp;
        } CUDA_TEXTURE_DESC;
* \endcode
* where
* - ::CUDA_TEXTURE_DESC::addressMode specifies the addressing mode for each dimension of the texture data. ::CUaddress_mode is defined as:
*   \code
        typedef enum CUaddress_mode_enum {
            CU_TR_ADDRESS_MODE_WRAP = 0,
            CU_TR_ADDRESS_MODE_CLAMP = 1,
            CU_TR_ADDRESS_MODE_MIRROR = 2,
            CU_TR_ADDRESS_MODE_BORDER = 3
        } CUaddress_mode;
*   \endcode
*   This is ignored if ::CUDA_RESOURCE_DESC::resType is ::CU_RESOURCE_TYPE_LINEAR. Also, if the flag, ::CU_TRSF_NORMALIZED_COORDINATES
*   is not set, the only supported address mode is ::CU_TR_ADDRESS_MODE_CLAMP.
*
* - ::CUDA_TEXTURE_DESC::filterMode specifies the filtering mode to be used when fetching from the texture. CUfilter_mode is defined as:
*   \code
        typedef enum CUfilter_mode_enum {
            CU_TR_FILTER_MODE_POINT = 0,
            CU_TR_FILTER_MODE_LINEAR = 1
        } CUfilter_mode;
*   \endcode
*   This is ignored if ::CUDA_RESOURCE_DESC::resType is ::CU_RESOURCE_TYPE_LINEAR.
*
* - ::CUDA_TEXTURE_DESC::flags can be any combination of the following:
*   - ::CU_TRSF_READ_AS_INTEGER, which suppresses the default behavior of
*   having the texture promote integer data to floating point data in the
*   range [0, 1]. Note that texture with 32-bit integer format would not be 
*   promoted, regardless of whether or not this flag is specified.
*   - ::CU_TRSF_NORMALIZED_COORDINATES, which suppresses the default behavior
*   of having the texture coordinates range from [0, Dim) where Dim is the 
*   width or height of the CUDA array. Instead, the texture coordinates 
*   [0, 1.0) reference the entire breadth of the array dimension; Note that
*   for CUDA mipmapped arrays, this flag has to be set.
*   - ::CU_TRSF_DISABLE_TRILINEAR_OPTIMIZATION, which disables any trilinear
*   filtering optimizations. Trilinear optimizations improve texture filtering
*   performance by allowing bilinear filtering on textures in scenarios where
*   it can closely approximate the expected results.
*   - ::CU_TRSF_SEAMLESS_CUBEMAP, which enables seamless cube map filtering. 
*   This flag can only be specified if the underlying resource is a CUDA array 
*   or a CUDA mipmapped array that was created with the flag ::CUDA_ARRAY3D_CUBEMAP.
*   When seamless cube map filtering is enabled, texture address modes specified 
*   by ::CUDA_TEXTURE_DESC::addressMode are ignored. Instead, if the ::CUDA_TEXTURE_DESC::filterMode 
*   is set to ::CU_TR_FILTER_MODE_POINT the address mode ::CU_TR_ADDRESS_MODE_CLAMP 
*   will be applied for all dimensions. If the ::CUDA_TEXTURE_DESC::filterMode is 
*   set to ::CU_TR_FILTER_MODE_LINEAR seamless cube map filtering will be performed
*   when sampling along the cube face borders.
*
* - ::CUDA_TEXTURE_DESC::maxAnisotropy specifies the maximum anisotropy ratio to be used when doing anisotropic filtering. This value will be
*   clamped to the range [1,16].
*
* - ::CUDA_TEXTURE_DESC::mipmapFilterMode specifies the filter mode when the calculated mipmap level lies between two defined mipmap levels.
*
* - ::CUDA_TEXTURE_DESC::mipmapLevelBias specifies the offset to be applied to the calculated mipmap level.
*
* - ::CUDA_TEXTURE_DESC::minMipmapLevelClamp specifies the lower end of the mipmap level range to clamp access to.
*
* - ::CUDA_TEXTURE_DESC::maxMipmapLevelClamp specifies the upper end of the mipmap level range to clamp access to.
*
*
* The ::CUDA_RESOURCE_VIEW_DESC struct is defined as
* \code
        typedef struct CUDA_RESOURCE_VIEW_DESC_st
        {
            CUresourceViewFormat format;
            size_t width;
            size_t height;
            size_t depth;
            unsigned int firstMipmapLevel;
            unsigned int lastMipmapLevel;
            unsigned int firstLayer;
            unsigned int lastLayer;
        } CUDA_RESOURCE_VIEW_DESC;
* \endcode
* where:
* - ::CUDA_RESOURCE_VIEW_DESC::format specifies how the data contained in the CUDA array or CUDA mipmapped array should
*   be interpreted. Note that this can incur a change in size of the texture data. If the resource view format is a block
*   compressed format, then the underlying CUDA array or CUDA mipmapped array has to have a base of format ::CU_AD_FORMAT_UNSIGNED_INT32.
*   with 2 or 4 channels, depending on the block compressed format. For ex., BC1 and BC4 require the underlying CUDA array to have
*   a format of ::CU_AD_FORMAT_UNSIGNED_INT32 with 2 channels. The other BC formats require the underlying resource to have the same base
*   format but with 4 channels.
*
* - ::CUDA_RESOURCE_VIEW_DESC::width specifies the new width of the texture data. If the resource view format is a block
*   compressed format, this value has to be 4 times the original width of the resource. For non block compressed formats,
*   this value has to be equal to that of the original resource.
*
* - ::CUDA_RESOURCE_VIEW_DESC::height specifies the new height of the texture data. If the resource view format is a block
*   compressed format, this value has to be 4 times the original height of the resource. For non block compressed formats,
*   this value has to be equal to that of the original resource.
*
* - ::CUDA_RESOURCE_VIEW_DESC::depth specifies the new depth of the texture data. This value has to be equal to that of the
*   original resource.
*
* - ::CUDA_RESOURCE_VIEW_DESC::firstMipmapLevel specifies the most detailed mipmap level. This will be the new mipmap level zero.
*   For non-mipmapped resources, this value has to be zero.::CUDA_TEXTURE_DESC::minMipmapLevelClamp and ::CUDA_TEXTURE_DESC::maxMipmapLevelClamp
*   will be relative to this value. For ex., if the firstMipmapLevel is set to 2, and a minMipmapLevelClamp of 1.2 is specified,
*   then the actual minimum mipmap level clamp will be 3.2.
*
* - ::CUDA_RESOURCE_VIEW_DESC::lastMipmapLevel specifies the least detailed mipmap level. For non-mipmapped resources, this value
*   has to be zero.
*
* - ::CUDA_RESOURCE_VIEW_DESC::firstLayer specifies the first layer index for layered textures. This will be the new layer zero.
*   For non-layered resources, this value has to be zero.
*
* - ::CUDA_RESOURCE_VIEW_DESC::lastLayer specifies the last layer index for layered textures. For non-layered resources,
*   this value has to be zero.
*
*
* \param pTexObject   - Texture object to create
* \param pResDesc     - Resource descriptor
* \param pTexDesc     - Texture descriptor
* \param pResViewDesc - Resource view descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexObjectDestroy,
* ::cudaCreateTextureObject
*/
int handle_cuTexObjectCreate(void *conn) {
    CUtexObject pTexObject;
    CUDA_RESOURCE_DESC pResDesc;
    CUDA_TEXTURE_DESC pTexDesc;
    CUDA_RESOURCE_VIEW_DESC pResViewDesc;

    if (rpc_read(conn, &pTexObject, sizeof(CUtexObject)) < 0 ||
        rpc_read(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_read(conn, &pTexDesc, sizeof(CUDA_TEXTURE_DESC)) < 0 ||
        rpc_read(conn, &pResViewDesc, sizeof(CUDA_RESOURCE_VIEW_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexObjectCreate(&pTexObject, &pResDesc, &pTexDesc, &pResViewDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexObject, sizeof(CUtexObject)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a texture object
*
* Destroys the texture object specified by \p texObject.
*
* \param texObject - Texture object to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexObjectCreate,
* ::cudaDestroyTextureObject
*/
int handle_cuTexObjectDestroy(void *conn) {
    CUtexObject texObject;

    if (rpc_read(conn, &texObject, sizeof(CUtexObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexObjectDestroy(texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's resource descriptor
*
* Returns the resource descriptor for the texture object specified by \p texObject.
*
* \param pResDesc  - Resource descriptor
* \param texObject - Texture object
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexObjectCreate,
* ::cudaGetTextureObjectResourceDesc,
*/
int handle_cuTexObjectGetResourceDesc(void *conn) {
    CUDA_RESOURCE_DESC pResDesc;
    CUtexObject texObject;

    if (rpc_read(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_read(conn, &texObject, sizeof(CUtexObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexObjectGetResourceDesc(&pResDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's texture descriptor
*
* Returns the texture descriptor for the texture object specified by \p texObject.
*
* \param pTexDesc  - Texture descriptor
* \param texObject - Texture object
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexObjectCreate,
* ::cudaGetTextureObjectTextureDesc
*/
int handle_cuTexObjectGetTextureDesc(void *conn) {
    CUDA_TEXTURE_DESC pTexDesc;
    CUtexObject texObject;

    if (rpc_read(conn, &pTexDesc, sizeof(CUDA_TEXTURE_DESC)) < 0 ||
        rpc_read(conn, &texObject, sizeof(CUtexObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexObjectGetTextureDesc(&pTexDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexDesc, sizeof(CUDA_TEXTURE_DESC)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's resource view descriptor
*
* Returns the resource view descriptor for the texture object specified by \p texObject.
* If no resource view was set for \p texObject, the ::CUDA_ERROR_INVALID_VALUE is returned.
*
* \param pResViewDesc - Resource view descriptor
* \param texObject    - Texture object
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTexObjectCreate,
* ::cudaGetTextureObjectResourceViewDesc
*/
int handle_cuTexObjectGetResourceViewDesc(void *conn) {
    CUDA_RESOURCE_VIEW_DESC pResViewDesc;
    CUtexObject texObject;

    if (rpc_read(conn, &pResViewDesc, sizeof(CUDA_RESOURCE_VIEW_DESC)) < 0 ||
        rpc_read(conn, &texObject, sizeof(CUtexObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTexObjectGetResourceViewDesc(&pResViewDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResViewDesc, sizeof(CUDA_RESOURCE_VIEW_DESC)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a surface object
*
* Creates a surface object and returns it in \p pSurfObject. \p pResDesc describes
* the data to perform surface load/stores on. ::CUDA_RESOURCE_DESC::resType must be
* ::CU_RESOURCE_TYPE_ARRAY and  ::CUDA_RESOURCE_DESC::res::array::hArray
* must be set to a valid CUDA array handle. ::CUDA_RESOURCE_DESC::flags must be set to zero.
*
* Surface objects are only supported on devices of compute capability 3.0 or higher.
* Additionally, a surface object is an opaque value, and, as such, should only be
* accessed through CUDA API calls.
*
* \param pSurfObject - Surface object to create
* \param pResDesc    - Resource descriptor
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuSurfObjectDestroy,
* ::cudaCreateSurfaceObject
*/
int handle_cuSurfObjectCreate(void *conn) {
    CUsurfObject pSurfObject;
    CUDA_RESOURCE_DESC pResDesc;

    if (rpc_read(conn, &pSurfObject, sizeof(CUsurfObject)) < 0 ||
        rpc_read(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSurfObjectCreate(&pSurfObject, &pResDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pSurfObject, sizeof(CUsurfObject)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a surface object
*
* Destroys the surface object specified by \p surfObject.
*
* \param surfObject - Surface object to destroy
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuSurfObjectCreate,
* ::cudaDestroySurfaceObject
*/
int handle_cuSurfObjectDestroy(void *conn) {
    CUsurfObject surfObject;

    if (rpc_read(conn, &surfObject, sizeof(CUsurfObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSurfObjectDestroy(surfObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a surface object's resource descriptor
*
* Returns the resource descriptor for the surface object specified by \p surfObject.
*
* \param pResDesc   - Resource descriptor
* \param surfObject - Surface object
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuSurfObjectCreate,
* ::cudaGetSurfaceObjectResourceDesc
*/
int handle_cuSurfObjectGetResourceDesc(void *conn) {
    CUDA_RESOURCE_DESC pResDesc;
    CUsurfObject surfObject;

    if (rpc_read(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_read(conn, &surfObject, sizeof(CUsurfObject)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuSurfObjectGetResourceDesc(&pResDesc, surfObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0)
        return -1;

    return result;
}

/**
* \brief Create a tensor map descriptor object representing tiled memory region
*
* Creates a descriptor for Tensor Memory Access (TMA) object specified
* by the parameters describing a tiled region and returns it in \p tensorMap.
*
* Tensor map objects are only supported on devices of compute capability 9.0 or higher.
* Additionally, a tensor map object is an opaque value, and, as such, should only be
* accessed through CUDA API calls.
*
* The parameters passed are bound to the following requirements:
*
* - \p tensorMap address must be aligned to 64 bytes.
*
* - \p tensorDataType has to be an enum from ::CUtensorMapDataType which is defined as:
* \code
    typedef enum CUtensorMapDataType_enum {
        CU_TENSOR_MAP_DATA_TYPE_UINT8 = 0,       // 1 byte
        CU_TENSOR_MAP_DATA_TYPE_UINT16,          // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_UINT32,          // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_INT32,           // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_UINT64,          // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_INT64,           // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT16,         // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT32,         // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT64,         // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,        // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT32_FTZ,     // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_TFLOAT32,        // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_TFLOAT32_FTZ     // 4 bytes
    } CUtensorMapDataType;
* \endcode
*
* - \p tensorRank must be non-zero and less than or equal to the maximum supported dimensionality of 5. If \p interleave is not
* ::CU_TENSOR_MAP_INTERLEAVE_NONE, then \p tensorRank must additionally be greater than or equal to 3.
*
* - \p globalAddress, which specifies the starting address of the memory region described, must be 32 byte aligned when \p interleave is
* ::CU_TENSOR_MAP_INTERLEAVE_32B and 16 byte aligned otherwise.
*
* - \p globalDim array, which specifies tensor size of each of the \p tensorRank dimensions, must be non-zero and less than or
* equal to 2^32.
*
* - \p globalStrides array, which specifies tensor stride of each of the lower \p tensorRank - 1 dimensions in bytes, must be a
* multiple of 16 and less than 2^40. Additionally, the stride must be a multiple of 32 when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_32B.
* Each following dimension specified includes previous dimension stride:
* \code
    globalStrides[0] = globalDim[0] * elementSizeInBytes(tensorDataType) + padding[0];
    for (i = 1; i < tensorRank - 1; i++)
        globalStrides[i] = globalStrides[i – 1] * globalStrides[i] + padding[i];
    assert(globalStrides[i] >= globalDim[i]);
* \endcode
*
* - \p boxDim array, which specifies number of elements to be traversed along each of the \p tensorRank dimensions, must be less
* than or equal to 8.
* When \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_NONE, { \p boxDim[0] * elementSizeInBytes( \p tensorDataType ) } must be a multiple
* of 16 bytes.
*
* - \p elementStrides array, which specifies the iteration step along each of the \p tensorRank dimensions, must be non-zero and less
* than or equal to 8. Note that when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_NONE, the first element of this array is ignored since
* TMA doesn’t support the stride for dimension zero.
* When all elemets of \p elementStrides array is one, \p boxDim specifies the number of elements to load. However, if the \p elementStrides[i]
* is not equal to one, then TMA loads ceil( \p boxDim[i] / \p elementStrides[i]) number of elements along i-th dimension. To load N elements along
* i-th dimension, \p boxDim[i] must be set to N * \p elementStrides[i].
*
* - \p interleave specifies the interleaved layout of type ::CUtensorMapInterleave, which is defined as:
* \code
    typedef enum CUtensorMapInterleave_enum {
        CU_TENSOR_MAP_INTERLEAVE_NONE = 0,
        CU_TENSOR_MAP_INTERLEAVE_16B,
        CU_TENSOR_MAP_INTERLEAVE_32B
    } CUtensorMapInterleave;
* \endcode
* TMA supports interleaved layouts like NC/8HWC8 where C8 utilizes 16 bytes in memory assuming 2 byte per channel or NC/16HWC16 where C16
* uses 32 bytes.
* When \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_NONE and \p swizzle is not ::CU_TENSOR_MAP_SWIZZLE_NONE, the bounding box inner dimension
* (computed as \p boxDim[0] multiplied by element size derived from \p tensorDataType) must be less than or equal to the swizzle size.
*    - CU_TENSOR_MAP_SWIZZLE_32B implies the bounding box inner dimension will be <= 32.
*    - CU_TENSOR_MAP_SWIZZLE_64B implies the bounding box inner dimension will be <= 64.
*    - CU_TENSOR_MAP_SWIZZLE_128B implies the bounding box inner dimension will be <= 128.
*
* - \p swizzle, which specifies the shared memory bank swizzling pattern, has to be of type ::CUtensorMapSwizzle which is defined as:
* \code
    typedef enum CUtensorMapSwizzle_enum {
        CU_TENSOR_MAP_SWIZZLE_NONE = 0,
        CU_TENSOR_MAP_SWIZZLE_32B,
        CU_TENSOR_MAP_SWIZZLE_64B,
        CU_TENSOR_MAP_SWIZZLE_128B
    } CUtensorMapSwizzle;
* \endcode
* Data is organized in specific order in global memory; however, it may not match the order in which data are accessed by application in
* the shared memory. This difference in data organization may cause bank conflicts when shared memory is accessed. In order to avoid this
* problem, data can be loaded to shard memory with shuffling across shared memory banks.
* Note that it’s expected that when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_32B, \p swizzle should be ::CU_TENSOR_MAP_SWIZZLE_32B mode.
* Other interleave modes can have any swizzling patterns.
*
* - \p l2Promotion specifies L2 fetch size which indicates the byte granurality at which L2 requests is filled from DRAM. It must be of
* type ::CUtensorMapL2promotion, which is defined as:
* \code
    typedef enum CUtensorMapL2promotion_enum {
        CU_TENSOR_MAP_L2_PROMOTION_NONE = 0,
        CU_TENSOR_MAP_L2_PROMOTION_L2_64B,
        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,
        CU_TENSOR_MAP_L2_PROMOTION_L2_256B
    } CUtensorMapL2promotion;
* \endcode
*
* - \p oobFill, which indicates whether zero or a special NaN constant should be used to fill out-of-bound elements, must be of type
* ::CUtensorMapFloatOOBfill which is defined as:
* \code
    typedef enum CUtensorMapFloatOOBfill_enum {
        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE = 0,
        CU_TENSOR_MAP_FLOAT_OOB_FILL_NAN_REQUEST_ZERO_FMA
    } CUtensorMapFloatOOBfill;
* \endcode
* Note that ::CU_TENSOR_MAP_FLOAT_OOB_FILL_NAN_REQUEST_ZERO_FMA can only be used when \p tensorDataType represents a floating data type.
*
* \param tensorMap         - Tensor map object to create
* \param tensorDataType    - Tensor data type
* \param tensorRank        - Dimensionality of tensor
* \param globalAddress     - Starting address of memory region described by tensor
* \param globalDim         - Array containing tensor size (number of elements) along each of the \p tensorRank dimensions
* \param globalStrides     - Array containing stride size (in bytes) along each of the \p tensorRank - 1 dimensions
* \param boxDim            - Array containing traversal box size (number of elments) along each of the \p tensorRank dimensions. Specifies how many elements to be traversed along each tensor dimension.
* \param elementStrides    - Array containing traversal stride in each of the \p tensorRank dimensions
* \param interleave        - Type of interleaved layout the tensor addresses
* \param swizzle           - Bank swizzling pattern inside shared memory
* \param l2Promotion       - L2 promotion size
* \param oobFill           - Indicate whether zero or special NaN constant must be used to fill out-of-bound elements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTensorMapEncodeIm2col,
* ::cuTensorMapReplaceAddress
*/
int handle_cuTensorMapEncodeTiled(void *conn) {
    CUtensorMap tensorMap;
    CUtensorMapDataType tensorDataType;
    cuuint32_t tensorRank;
    void* globalAddress;
    cuuint64_t globalDim;
    cuuint64_t globalStrides;
    cuuint32_t boxDim;
    cuuint32_t elementStrides;
    CUtensorMapInterleave interleave;
    CUtensorMapSwizzle swizzle;
    CUtensorMapL2promotion l2Promotion;
    CUtensorMapFloatOOBfill oobFill;

    if (rpc_read(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_read(conn, &tensorDataType, sizeof(CUtensorMapDataType)) < 0 ||
        rpc_read(conn, &tensorRank, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &globalAddress, sizeof(void*)) < 0 ||
        rpc_read(conn, &globalDim, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &globalStrides, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &boxDim, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &elementStrides, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &interleave, sizeof(CUtensorMapInterleave)) < 0 ||
        rpc_read(conn, &swizzle, sizeof(CUtensorMapSwizzle)) < 0 ||
        rpc_read(conn, &l2Promotion, sizeof(CUtensorMapL2promotion)) < 0 ||
        rpc_read(conn, &oobFill, sizeof(CUtensorMapFloatOOBfill)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTensorMapEncodeTiled(&tensorMap, tensorDataType, tensorRank, &globalAddress, &globalDim, &globalStrides, &boxDim, &elementStrides, interleave, swizzle, l2Promotion, oobFill);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_write(conn, &globalAddress, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Create a tensor map descriptor object representing im2col memory region
*
* Creates a descriptor for Tensor Memory Access (TMA) object specified
* by the parameters describing a im2col memory layout and returns it in \p tensorMap.
*
* Tensor map objects are only supported on devices of compute capability 9.0 or higher.
* Additionally, a tensor map object is an opaque value, and, as such, should only be
* accessed through CUDA API calls.
*
* The parameters passed are bound to the following requirements:
*
* - \p tensorMap address must be aligned to 64 bytes.
*
* - \p tensorDataType has to be an enum from ::CUtensorMapDataType which is defined as:
* \code
    typedef enum CUtensorMapDataType_enum {
        CU_TENSOR_MAP_DATA_TYPE_UINT8 = 0,       // 1 byte
        CU_TENSOR_MAP_DATA_TYPE_UINT16,          // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_UINT32,          // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_INT32,           // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_UINT64,          // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_INT64,           // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT16,         // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT32,         // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT64,         // 8 bytes
        CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,        // 2 bytes
        CU_TENSOR_MAP_DATA_TYPE_FLOAT32_FTZ,     // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_TFLOAT32,        // 4 bytes
        CU_TENSOR_MAP_DATA_TYPE_TFLOAT32_FTZ     // 4 bytes
    } CUtensorMapDataType;
* \endcode
*
* - \p tensorRank must be one of dimensions 3, 4, or 5.
*
* - \p globalAddress, which specifies the starting address of the memory region described, must be 32 byte aligned when \p interleave is
* ::CU_TENSOR_MAP_INTERLEAVE_32B and 16 byte aligned otherwise.
*
* - \p globalDim array, which specifies tensor size of each of the \p tensorRank dimensions, must be non-zero and less than or
* equal to 2^32.
*
* - \p globalStrides array, which specifies tensor stride of each of the lower \p tensorRank - 1 dimensions in bytes, must be a
* multiple of 16 and less than 2^40. Additionally, the stride must be a multiple of 32 when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_32B.
* Each following dimension specified includes previous dimension stride:
* \code
    globalStrides[0] = globalDim[0] * elementSizeInBytes(tensorDataType) + padding[0];
    for (i = 1; i < tensorRank - 1; i++)
        globalStrides[i] = globalStrides[i – 1] * globalStrides[i] + padding[i];
    assert(globalStrides[i] >= globalDim[i]);
* \endcode
*
* - \p pixelBoxLowerCorner array specifies the coordinate offsets {D, H, W} of the bounding box from top/left/front corner. The number of
* offsets and their precision depends on the tensor dimensionality:
*    - When \p tensorRank is 3, one signed offset within range [-32768, 32767] is supported.
*    - When \p tensorRank is 4, two signed offsets each within range [-128, 127] are supported.
*    - When \p tensorRank is 5, three offsets each within range [-16, 15] are supported.
*
* - \p pixelBoxUpperCorner array specifies the coordinate offsets {D, H, W} of the bounding box from bottom/right/back corner. The number of
* offsets and their precision depends on the tensor dimensionality:
*    - When \p tensorRank is 3, one signed offset within range [-32768, 32767] is supported.
*    - When \p tensorRank is 4, two signed offsets each within range [-128, 127] are supported.
*    - When \p tensorRank is 5, three offsets each within range [-16, 15] are supported.
* The bounding box specified by \p pixelBoxLowerCorner and \p pixelBoxUpperCorner must have non-zero area.
*
* - \p channelsPerPixel, which specifies the number of elements which must be accessed along C dimension, must be less than or equal to 256.
*
* - \p pixelsPerColumn, which specifies the number of elements that must be accessed along the {N, D, H, W} dimensions, must be less than or
* equal to 1024.
*
* - \p elementStrides array, which specifies the iteration step along each of the \p tensorRank dimensions, must be non-zero and less
* than or equal to 8. Note that when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_NONE, the first element of this array is ignored since
* TMA doesn’t support the stride for dimension zero.
* When all elemets of \p elementStrides array is one, \p boxDim specifies the number of elements to load. However, if the \p elementStrides[i]
* is not equal to one, then TMA loads ceil( \p boxDim[i] / \p elementStrides[i]) number of elements along i-th dimension. To load N elements along
* i-th dimension, \p boxDim[i] must be set to N * \p elementStrides[i].
*
* - \p interleave specifies the interleaved layout of type ::CUtensorMapInterleave, which is defined as:
* \code
    typedef enum CUtensorMapInterleave_enum {
        CU_TENSOR_MAP_INTERLEAVE_NONE = 0,
        CU_TENSOR_MAP_INTERLEAVE_16B,
        CU_TENSOR_MAP_INTERLEAVE_32B
    } CUtensorMapInterleave;
* \endcode
* TMA supports interleaved layouts like NC/8HWC8 where C8 utilizes 16 bytes in memory assuming 2 byte per channel or NC/16HWC16 where C16
* uses 32 bytes.
* When \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_NONE and \p swizzle is not ::CU_TENSOR_MAP_SWIZZLE_NONE, the bounding box inner dimension
* (computed as \p boxDim[0] multiplied by element size derived from \p tensorDataType) must be less than or equal to the swizzle size.
*    - CU_TENSOR_MAP_SWIZZLE_32B implies the bounding box inner dimension will be <= 32.
*    - CU_TENSOR_MAP_SWIZZLE_64B implies the bounding box inner dimension will be <= 64.
*    - CU_TENSOR_MAP_SWIZZLE_128B implies the bounding box inner dimension will be <= 128.
*
* - \p swizzle, which specifies the shared memory bank swizzling pattern, has to be of type ::CUtensorMapSwizzle which is defined as:
* \code
    typedef enum CUtensorMapSwizzle_enum {
        CU_TENSOR_MAP_SWIZZLE_NONE = 0,
        CU_TENSOR_MAP_SWIZZLE_32B,
        CU_TENSOR_MAP_SWIZZLE_64B,
        CU_TENSOR_MAP_SWIZZLE_128B
    } CUtensorMapSwizzle;
* \endcode
* Data is organized in specific order in global memory; however, it may not match the order in which data are accessed by application in
* the shared memory. This difference in data organization may cause bank conflicts when shared memory is accessed. In order to avoid this
* problem, data can be loaded to shard memory with shuffling across shared memory banks.
* Note that it’s expected that when \p interleave is ::CU_TENSOR_MAP_INTERLEAVE_32B, \p swizzle should be ::CU_TENSOR_MAP_SWIZZLE_32B mode.
* Other interleave modes can have any swizzling patterns.
*
* - \p l2Promotion specifies L2 fetch size which indicates the byte granurality at which L2 requests is filled from DRAM. It must be of
* type ::CUtensorMapL2promotion, which is defined as:
* \code
    typedef enum CUtensorMapL2promotion_enum {
        CU_TENSOR_MAP_L2_PROMOTION_NONE = 0,
        CU_TENSOR_MAP_L2_PROMOTION_L2_64B,
        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,
        CU_TENSOR_MAP_L2_PROMOTION_L2_256B
    } CUtensorMapL2promotion;
* \endcode
*
* - \p oobFill, which indicates whether zero or a special NaN constant should be used to fill out-of-bound elements, must be of type
* ::CUtensorMapFloatOOBfill which is defined as:
* \code
    typedef enum CUtensorMapFloatOOBfill_enum {
        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE = 0,
        CU_TENSOR_MAP_FLOAT_OOB_FILL_NAN_REQUEST_ZERO_FMA
    } CUtensorMapFloatOOBfill;
* \endcode
* Note that ::CU_TENSOR_MAP_FLOAT_OOB_FILL_NAN_REQUEST_ZERO_FMA can only be used when \p tensorDataType represents a floating data type.
*
* \param tensorMap             - Tensor map object to create
* \param tensorDataType        - Tensor data type
* \param tensorRank            - Dimensionality of tensor, needs to be at least of dimension 3
* \param globalAddress         - Starting address of memory region described by tensor
* \param globalDim             - Array containing tensor size (number of elements) along each of the \p tensorRank dimensions
* \param globalStrides         - Array containing stride size (in bytes) along each of the \p tensorRank - 1 dimensions
* \param pixelBoxLowerCorner   - Array containing DHW dimentions of lower box corner
* \param pixelBoxUpperCorner   - Array containing DHW dimentions of upper box corner
* \param channelsPerPixel      - Number of channels per pixel
* \param pixelsPerColumn       - Number of pixels per column
* \param elementStrides        - Array containing traversal stride in each of the \p tensorRank dimensions
* \param interleave            - Type of interleaved layout the tensor addresses
* \param swizzle               - Bank swizzling pattern inside shared memory
* \param l2Promotion           - L2 promotion size
* \param oobFill               - Indicate whether zero or special NaN constant must be used to fill out-of-bound elements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTensorMapEncodeTiled,
* ::cuTensorMapReplaceAddress
*/
int handle_cuTensorMapEncodeIm2col(void *conn) {
    CUtensorMap tensorMap;
    CUtensorMapDataType tensorDataType;
    cuuint32_t tensorRank;
    void* globalAddress;
    cuuint64_t globalDim;
    cuuint64_t globalStrides;
    int pixelBoxLowerCorner;
    int pixelBoxUpperCorner;
    cuuint32_t channelsPerPixel;
    cuuint32_t pixelsPerColumn;
    cuuint32_t elementStrides;
    CUtensorMapInterleave interleave;
    CUtensorMapSwizzle swizzle;
    CUtensorMapL2promotion l2Promotion;
    CUtensorMapFloatOOBfill oobFill;

    if (rpc_read(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_read(conn, &tensorDataType, sizeof(CUtensorMapDataType)) < 0 ||
        rpc_read(conn, &tensorRank, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &globalAddress, sizeof(void*)) < 0 ||
        rpc_read(conn, &globalDim, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &globalStrides, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &pixelBoxLowerCorner, sizeof(int)) < 0 ||
        rpc_read(conn, &pixelBoxUpperCorner, sizeof(int)) < 0 ||
        rpc_read(conn, &channelsPerPixel, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &pixelsPerColumn, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &elementStrides, sizeof(cuuint32_t)) < 0 ||
        rpc_read(conn, &interleave, sizeof(CUtensorMapInterleave)) < 0 ||
        rpc_read(conn, &swizzle, sizeof(CUtensorMapSwizzle)) < 0 ||
        rpc_read(conn, &l2Promotion, sizeof(CUtensorMapL2promotion)) < 0 ||
        rpc_read(conn, &oobFill, sizeof(CUtensorMapFloatOOBfill)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTensorMapEncodeIm2col(&tensorMap, tensorDataType, tensorRank, &globalAddress, &globalDim, &globalStrides, &pixelBoxLowerCorner, &pixelBoxUpperCorner, channelsPerPixel, pixelsPerColumn, &elementStrides, interleave, swizzle, l2Promotion, oobFill);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_write(conn, &globalAddress, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Modify an existing tensor map descriptor with an updated global address
*
* Modifies the descriptor for Tensor Memory Access (TMA) object passed in \p tensorMap with
* an updated \p globalAddress.
*
* Tensor map objects are only supported on devices of compute capability 9.0 or higher.
* Additionally, a tensor map object is an opaque value, and, as such, should only be
* accessed through CUDA API calls.
*
* \param tensorMap             - Tensor map object to modify
* \param globalAddress         - Starting address of memory region described by tensor, must follow previous alignment requirements
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE
*
* \sa
* ::cuTensorMapEncodeTiled,
* ::cuTensorMapEncodeIm2col
*/
int handle_cuTensorMapReplaceAddress(void *conn) {
    CUtensorMap tensorMap;
    void* globalAddress;

    if (rpc_read(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_read(conn, &globalAddress, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuTensorMapReplaceAddress(&tensorMap, &globalAddress);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &tensorMap, sizeof(CUtensorMap)) < 0 ||
        rpc_write(conn, &globalAddress, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Queries if a device may directly access a peer device's memory.
*
* Returns in \p *canAccessPeer a value of 1 if contexts on \p dev are capable of
* directly accessing memory from contexts on \p peerDev and 0 otherwise.
* If direct access of \p peerDev from \p dev is possible, then access may be
* enabled on two specific contexts by calling ::cuCtxEnablePeerAccess().
*
* \param canAccessPeer - Returned access capability
* \param dev           - Device from which allocations on \p peerDev are to
*                        be directly accessed.
* \param peerDev       - Device on which the allocations to be directly accessed
*                        by \p dev reside.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE
* \notefnerr
*
* \sa
* ::cuCtxEnablePeerAccess,
* ::cuCtxDisablePeerAccess,
* ::cudaDeviceCanAccessPeer
*/
int handle_cuDeviceCanAccessPeer(void *conn) {
    int canAccessPeer;
    CUdevice dev;
    CUdevice peerDev;

    if (rpc_read(conn, &canAccessPeer, sizeof(int)) < 0 ||
        rpc_read(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &peerDev, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceCanAccessPeer(&canAccessPeer, dev, peerDev);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &canAccessPeer, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Enables direct access to memory allocations in a peer context.
*
* If both the current context and \p peerContext are on devices which support unified
* addressing (as may be queried using ::CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING) and same
* major compute capability, then on success all allocations from \p peerContext will
* immediately be accessible by the current context.  See \ref CUDA_UNIFIED for additional
* details.
*
* Note that access granted by this call is unidirectional and that in order to access
* memory from the current context in \p peerContext, a separate symmetric call
* to ::cuCtxEnablePeerAccess() is required.
*
* Note that there are both device-wide and system-wide limitations per system
* configuration, as noted in the CUDA Programming Guide under the section
* "Peer-to-Peer Memory Access".
*
* Returns ::CUDA_ERROR_PEER_ACCESS_UNSUPPORTED if ::cuDeviceCanAccessPeer() indicates
* that the ::CUdevice of the current context cannot directly access memory
* from the ::CUdevice of \p peerContext.
*
* Returns ::CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED if direct access of
* \p peerContext from the current context has already been enabled.
*
* Returns ::CUDA_ERROR_TOO_MANY_PEERS if direct peer access is not possible
* because hardware resources required for peer access have been exhausted.
*
* Returns ::CUDA_ERROR_INVALID_CONTEXT if there is no current context, \p peerContext
* is not a valid context, or if the current context is \p peerContext.
*
* Returns ::CUDA_ERROR_INVALID_VALUE if \p Flags is not 0.
*
* \param peerContext - Peer context to enable direct access to from the current context
* \param Flags       - Reserved for future use and must be set to 0
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED,
* ::CUDA_ERROR_TOO_MANY_PEERS,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_PEER_ACCESS_UNSUPPORTED,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuDeviceCanAccessPeer,
* ::cuCtxDisablePeerAccess,
* ::cudaDeviceEnablePeerAccess
*/
int handle_cuCtxEnablePeerAccess(void *conn) {
    CUcontext peerContext;
    unsigned int Flags;

    if (rpc_read(conn, &peerContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &Flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxEnablePeerAccess(peerContext, Flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Disables direct access to memory allocations in a peer context and
* unregisters any registered allocations.
*
  Returns ::CUDA_ERROR_PEER_ACCESS_NOT_ENABLED if direct peer access has
* not yet been enabled from \p peerContext to the current context.
*
* Returns ::CUDA_ERROR_INVALID_CONTEXT if there is no current context, or if
* \p peerContext is not a valid context.
*
* \param peerContext - Peer context to disable direct access to
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_PEER_ACCESS_NOT_ENABLED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* \notefnerr
*
* \sa
* ::cuDeviceCanAccessPeer,
* ::cuCtxEnablePeerAccess,
* ::cudaDeviceDisablePeerAccess
*/
int handle_cuCtxDisablePeerAccess(void *conn) {
    CUcontext peerContext;

    if (rpc_read(conn, &peerContext, sizeof(CUcontext)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuCtxDisablePeerAccess(peerContext);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries attributes of the link between two devices.
*
* Returns in \p *value the value of the requested attribute \p attrib of the
* link between \p srcDevice and \p dstDevice. The supported attributes are:
* - ::CU_DEVICE_P2P_ATTRIBUTE_PERFORMANCE_RANK: A relative value indicating the
*   performance of the link between two devices.
* - ::CU_DEVICE_P2P_ATTRIBUTE_ACCESS_SUPPORTED P2P: 1 if P2P Access is enable.
* - ::CU_DEVICE_P2P_ATTRIBUTE_NATIVE_ATOMIC_SUPPORTED: 1 if Atomic operations over
*   the link are supported.
* - ::CU_DEVICE_P2P_ATTRIBUTE_CUDA_ARRAY_ACCESS_SUPPORTED: 1 if cudaArray can
*   be accessed over the link.
*
* Returns ::CUDA_ERROR_INVALID_DEVICE if \p srcDevice or \p dstDevice are not valid
* or if they represent the same device.
*
* Returns ::CUDA_ERROR_INVALID_VALUE if \p attrib is not valid or if \p value is
* a null pointer.
*
* \param value         - Returned value of the requested attribute
* \param attrib        - The requested attribute of the link between \p srcDevice and \p dstDevice.
* \param srcDevice     - The source device of the target link.
* \param dstDevice     - The destination device of the target link.
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_DEVICE,
* ::CUDA_ERROR_INVALID_VALUE
* \notefnerr
*
* \sa
* ::cuCtxEnablePeerAccess,
* ::cuCtxDisablePeerAccess,
* ::cuDeviceCanAccessPeer,
* ::cudaDeviceGetP2PAttribute
*/
int handle_cuDeviceGetP2PAttribute(void *conn) {
    int value;
    CUdevice_P2PAttribute attrib;
    CUdevice srcDevice;
    CUdevice dstDevice;

    if (rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &attrib, sizeof(CUdevice_P2PAttribute)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(CUdevice)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuDeviceGetP2PAttribute(&value, attrib, srcDevice, dstDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Unregisters a graphics resource for access by CUDA
*
* Unregisters the graphics resource \p resource so it is not accessible by
* CUDA unless registered again.
*
* If \p resource is invalid then ::CUDA_ERROR_INVALID_HANDLE is
* returned.
*
* \param resource - Resource to unregister
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_UNKNOWN
* \notefnerr
*
* \sa
* ::cuGraphicsD3D9RegisterResource,
* ::cuGraphicsD3D10RegisterResource,
* ::cuGraphicsD3D11RegisterResource,
* ::cuGraphicsGLRegisterBuffer,
* ::cuGraphicsGLRegisterImage,
* ::cudaGraphicsUnregisterResource
*/
int handle_cuGraphicsUnregisterResource(void *conn) {
    CUgraphicsResource resource;

    if (rpc_read(conn, &resource, sizeof(CUgraphicsResource)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsUnregisterResource(resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Get an array through which to access a subresource of a mapped graphics resource.
*
* Returns in \p *pArray an array through which the subresource of the mapped
* graphics resource \p resource which corresponds to array index \p arrayIndex
* and mipmap level \p mipLevel may be accessed.  The value set in \p *pArray may
* change every time that \p resource is mapped.
*
* If \p resource is not a texture then it cannot be accessed via an array and
* ::CUDA_ERROR_NOT_MAPPED_AS_ARRAY is returned.
* If \p arrayIndex is not a valid array index for \p resource then
* ::CUDA_ERROR_INVALID_VALUE is returned.
* If \p mipLevel is not a valid mipmap level for \p resource then
* ::CUDA_ERROR_INVALID_VALUE is returned.
* If \p resource is not mapped then ::CUDA_ERROR_NOT_MAPPED is returned.
*
* \param pArray      - Returned array through which a subresource of \p resource may be accessed
* \param resource    - Mapped resource to access
* \param arrayIndex  - Array index for array textures or cubemap face
*                      index as defined by ::CUarray_cubemap_face for
*                      cubemap textures for the subresource to access
* \param mipLevel    - Mipmap level for the subresource to access
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_MAPPED,
* ::CUDA_ERROR_NOT_MAPPED_AS_ARRAY
* \notefnerr
*
* \sa
* ::cuGraphicsResourceGetMappedPointer,
* ::cudaGraphicsSubResourceGetMappedArray
*/
int handle_cuGraphicsSubResourceGetMappedArray(void *conn) {
    CUarray pArray;
    CUgraphicsResource resource;
    unsigned int arrayIndex;
    unsigned int mipLevel;

    if (rpc_read(conn, &pArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &arrayIndex, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &mipLevel, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsSubResourceGetMappedArray(&pArray, resource, arrayIndex, mipLevel);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pArray, sizeof(CUarray)) < 0)
        return -1;

    return result;
}

/**
* \brief Get a mipmapped array through which to access a mapped graphics resource.
*
* Returns in \p *pMipmappedArray a mipmapped array through which the mapped graphics
* resource \p resource. The value set in \p *pMipmappedArray may change every time
* that \p resource is mapped.
*
* If \p resource is not a texture then it cannot be accessed via a mipmapped array and
* ::CUDA_ERROR_NOT_MAPPED_AS_ARRAY is returned.
* If \p resource is not mapped then ::CUDA_ERROR_NOT_MAPPED is returned.
*
* \param pMipmappedArray - Returned mipmapped array through which \p resource may be accessed
* \param resource        - Mapped resource to access
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_MAPPED,
* ::CUDA_ERROR_NOT_MAPPED_AS_ARRAY
* \notefnerr
*
* \sa
* ::cuGraphicsResourceGetMappedPointer,
* ::cudaGraphicsResourceGetMappedMipmappedArray
*/
int handle_cuGraphicsResourceGetMappedMipmappedArray(void *conn) {
    CUmipmappedArray pMipmappedArray;
    CUgraphicsResource resource;

    if (rpc_read(conn, &pMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &resource, sizeof(CUgraphicsResource)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsResourceGetMappedMipmappedArray(&pMipmappedArray, resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pMipmappedArray, sizeof(CUmipmappedArray)) < 0)
        return -1;

    return result;
}

/**
* \brief Get a device pointer through which to access a mapped graphics resource.
*
* Returns in \p *pDevPtr a pointer through which the mapped graphics resource
* \p resource may be accessed.
* Returns in \p pSize the size of the memory in bytes which may be accessed from that pointer.
* The value set in \p pPointer may change every time that \p resource is mapped.
*
* If \p resource is not a buffer then it cannot be accessed via a pointer and
* ::CUDA_ERROR_NOT_MAPPED_AS_POINTER is returned.
* If \p resource is not mapped then ::CUDA_ERROR_NOT_MAPPED is returned.
* *
* \param pDevPtr    - Returned pointer through which \p resource may be accessed
* \param pSize      - Returned size of the buffer accessible starting at \p *pPointer
* \param resource   - Mapped resource to access
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_MAPPED,
* ::CUDA_ERROR_NOT_MAPPED_AS_POINTER
* \notefnerr
*
* \sa
* ::cuGraphicsMapResources,
* ::cuGraphicsSubResourceGetMappedArray,
* ::cudaGraphicsResourceGetMappedPointer
*/
int handle_cuGraphicsResourceGetMappedPointer_v2(void *conn) {
    CUdeviceptr pDevPtr;
    size_t pSize;
    CUgraphicsResource resource;

    if (rpc_read(conn, &pDevPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &pSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &resource, sizeof(CUgraphicsResource)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsResourceGetMappedPointer_v2(&pDevPtr, &pSize, resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pDevPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &pSize, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Set usage flags for mapping a graphics resource
*
* Set \p flags for mapping the graphics resource \p resource.
*
* Changes to \p flags will take effect the next time \p resource is mapped.
* The \p flags argument may be any of the following:
* - ::CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE: Specifies no hints about how this
*   resource will be used. It is therefore assumed that this resource will be
*   read from and written to by CUDA kernels.  This is the default value.
* - ::CU_GRAPHICS_MAP_RESOURCE_FLAGS_READONLY: Specifies that CUDA kernels which
*   access this resource will not write to this resource.
* - ::CU_GRAPHICS_MAP_RESOURCE_FLAGS_WRITEDISCARD: Specifies that CUDA kernels
*   which access this resource will not read from this resource and will
*   write over the entire contents of the resource, so none of the data
*   previously stored in the resource will be preserved.
*
* If \p resource is presently mapped for access by CUDA then
* ::CUDA_ERROR_ALREADY_MAPPED is returned.
* If \p flags is not one of the above values then ::CUDA_ERROR_INVALID_VALUE is returned.
*
* \param resource - Registered resource to set flags for
* \param flags    - Parameters for resource mapping
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_ALREADY_MAPPED
* \notefnerr
*
* \sa
* ::cuGraphicsMapResources,
* ::cudaGraphicsResourceSetMapFlags
*/
int handle_cuGraphicsResourceSetMapFlags_v2(void *conn) {
    CUgraphicsResource resource;
    unsigned int flags;

    if (rpc_read(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsResourceSetMapFlags_v2(resource, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Map graphics resources for access by CUDA
*
* Maps the \p count graphics resources in \p resources for access by CUDA.
*
* The resources in \p resources may be accessed by CUDA until they
* are unmapped. The graphics API from which \p resources were registered
* should not access any resources while they are mapped by CUDA. If an
* application does so, the results are undefined.
*
* This function provides the synchronization guarantee that any graphics calls
* issued before ::cuGraphicsMapResources() will complete before any subsequent CUDA
* work issued in \p stream begins.
*
* If \p resources includes any duplicate entries then ::CUDA_ERROR_INVALID_HANDLE is returned.
* If any of \p resources are presently mapped for access by CUDA then ::CUDA_ERROR_ALREADY_MAPPED is returned.
*
* \param count      - Number of resources to map
* \param resources  - Resources to map for CUDA usage
* \param hStream    - Stream with which to synchronize
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_ALREADY_MAPPED,
* ::CUDA_ERROR_UNKNOWN
* \note_null_stream
* \notefnerr
*
* \sa
* ::cuGraphicsResourceGetMappedPointer,
* ::cuGraphicsSubResourceGetMappedArray,
* ::cuGraphicsUnmapResources,
* ::cudaGraphicsMapResources
*/
int handle_cuGraphicsMapResources(void *conn) {
    unsigned int count;
    CUgraphicsResource resources;
    CUstream hStream;

    if (rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsMapResources(count, &resources, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resources, sizeof(CUgraphicsResource)) < 0)
        return -1;

    return result;
}

/**
* \brief Unmap graphics resources.
*
* Unmaps the \p count graphics resources in \p resources.
*
* Once unmapped, the resources in \p resources may not be accessed by CUDA
* until they are mapped again.
*
* This function provides the synchronization guarantee that any CUDA work issued
* in \p stream before ::cuGraphicsUnmapResources() will complete before any
* subsequently issued graphics work begins.
*
*
* If \p resources includes any duplicate entries then ::CUDA_ERROR_INVALID_HANDLE is returned.
* If any of \p resources are not presently mapped for access by CUDA then ::CUDA_ERROR_NOT_MAPPED is returned.
*
* \param count      - Number of resources to unmap
* \param resources  - Resources to unmap
* \param hStream    - Stream with which to synchronize
*
* \return
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_DEINITIALIZED,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_INVALID_CONTEXT,
* ::CUDA_ERROR_INVALID_HANDLE,
* ::CUDA_ERROR_NOT_MAPPED,
* ::CUDA_ERROR_UNKNOWN
* \note_null_stream
* \notefnerr
*
* \sa
* ::cuGraphicsMapResources,
* ::cudaGraphicsUnmapResources
*/
int handle_cuGraphicsUnmapResources(void *conn) {
    unsigned int count;
    CUgraphicsResource resources;
    CUstream hStream;

    if (rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &hStream, sizeof(CUstream)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGraphicsUnmapResources(count, &resources, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resources, sizeof(CUgraphicsResource)) < 0)
        return -1;

    return result;
}

int handle_cuGetExportTable(void *conn) {
    const void* ppExportTable;
    CUuuid pExportTableId;

    if (rpc_read(conn, &ppExportTable, sizeof(const void*)) < 0 ||
        rpc_read(conn, &pExportTableId, sizeof(CUuuid)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    CUresult result = cuGetExportTable(&ppExportTable, &pExportTableId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ppExportTable, sizeof(const void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroy all allocations and reset all state on the current device
* in the current process.
*
* Explicitly destroys and cleans up all resources associated with the current
* device in the current process. It is the caller's responsibility to ensure
* that the resources are not accessed or passed in subsequent API calls and
* doing so will result in undefined behavior. These resources include CUDA types
* such as ::cudaStream_t, ::cudaEvent_t, ::cudaArray_t, ::cudaMipmappedArray_t,
* ::cudaTextureObject_t, ::cudaSurfaceObject_t, ::textureReference, ::surfaceReference,
* ::cudaExternalMemory_t, ::cudaExternalSemaphore_t and ::cudaGraphicsResource_t.
* Any subsequent API call to this device will reinitialize the device.
*
* Note that this function will reset the device immediately.  It is the caller's
* responsibility to ensure that the device is not being accessed by any 
* other host threads from the process when this function is called.
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSynchronize
*/
int handle_cudaDeviceReset(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceReset();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Wait for compute device to finish
*
* Blocks until the device has completed all preceding requested tasks.
* ::cudaDeviceSynchronize() returns an error if one of the preceding tasks
* has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for 
* this device, the host thread will block until the device has finished 
* its work.
*
* \return
* ::cudaSuccess
* \note_device_sync_deprecated
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDeviceReset,
* ::cuCtxSynchronize
*/
int handle_cudaDeviceSynchronize(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSynchronize();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set resource limits
*
* Setting \p limit to \p value is a request by the application to update
* the current limit maintained by the device.  The driver is free to
* modify the requested value to meet h/w requirements (this could be
* clamping to minimum or maximum values, rounding up to nearest element
* size, etc).  The application can use ::cudaDeviceGetLimit() to find out
* exactly what the limit has been set to.
*
* Setting each ::cudaLimit has its own specific restrictions, so each is
* discussed here.
*
* - ::cudaLimitStackSize controls the stack size in bytes of each GPU thread.
*
* - ::cudaLimitPrintfFifoSize controls the size in bytes of the shared FIFO
*   used by the ::printf() device system call. Setting
*   ::cudaLimitPrintfFifoSize must not be performed after launching any kernel
*   that uses the ::printf() device system call - in such case
*   ::cudaErrorInvalidValue will be returned.
*
* - ::cudaLimitMallocHeapSize controls the size in bytes of the heap used by
*   the ::malloc() and ::free() device system calls. Setting
*   ::cudaLimitMallocHeapSize must not be performed after launching any kernel
*   that uses the ::malloc() or ::free() device system calls - in such case
*   ::cudaErrorInvalidValue will be returned.
*
* - ::cudaLimitDevRuntimeSyncDepth controls the maximum nesting depth of a
*   grid at which a thread can safely call ::cudaDeviceSynchronize(). Setting
*   this limit must be performed before any launch of a kernel that uses the
*   device runtime and calls ::cudaDeviceSynchronize() above the default sync
*   depth, two levels of grids. Calls to ::cudaDeviceSynchronize() will fail
*   with error code ::cudaErrorSyncDepthExceeded if the limitation is
*   violated. This limit can be set smaller than the default or up the maximum
*   launch depth of 24. When setting this limit, keep in mind that additional
*   levels of sync depth require the runtime to reserve large amounts of
*   device memory which can no longer be used for user allocations. If these
*   reservations of device memory fail, ::cudaDeviceSetLimit will return
*   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
*   This limit is only applicable to devices of compute capability < 9.0.
*   Attempting to set this limit on devices of other compute capability will
*   results in error ::cudaErrorUnsupportedLimit being returned.
*
* - ::cudaLimitDevRuntimePendingLaunchCount controls the maximum number of
*   outstanding device runtime launches that can be made from the current
*   device. A grid is outstanding from the point of launch up until the grid
*   is known to have been completed. Device runtime launches which violate
*   this limitation fail and return ::cudaErrorLaunchPendingCountExceeded when
*   ::cudaGetLastError() is called after launch. If more pending launches than
*   the default (2048 launches) are needed for a module using the device
*   runtime, this limit can be increased. Keep in mind that being able to
*   sustain additional pending launches will require the runtime to reserve
*   larger amounts of device memory upfront which can no longer be used for
*   allocations. If these reservations fail, ::cudaDeviceSetLimit will return
*   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
*   This limit is only applicable to devices of compute capability 3.5 and
*   higher. Attempting to set this limit on devices of compute capability less
*   than 3.5 will result in the error ::cudaErrorUnsupportedLimit being
*   returned.
*
* - ::cudaLimitMaxL2FetchGranularity controls the L2 cache fetch granularity.
*   Values can range from 0B to 128B. This is purely a performance hint and
*   it can be ignored or clamped depending on the platform.
*
* - ::cudaLimitPersistingL2CacheSize controls size in bytes available
*   for persisting L2 cache. This is purely a performance hint and it
*   can be ignored or clamped depending on the platform.
*
* \param limit - Limit to set
* \param value - Size of limit
*
* \return
* ::cudaSuccess,
* ::cudaErrorUnsupportedLimit,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDeviceGetLimit,
* ::cuCtxSetLimit
*/
int handle_cudaDeviceSetLimit(void *conn) {
    enum cudaLimit limit;
    size_t value;

    if (rpc_read(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_read(conn, &value, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSetLimit(limit, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Return resource limits
*
* Returns in \p *pValue the current size of \p limit. The following ::cudaLimit values are supported.
* - ::cudaLimitStackSize is the stack size in bytes of each GPU thread.
* - ::cudaLimitPrintfFifoSize is the size in bytes of the shared FIFO used by the
*   ::printf() device system call.
* - ::cudaLimitMallocHeapSize is the size in bytes of the heap used by the
*   ::malloc() and ::free() device system calls.
* - ::cudaLimitDevRuntimeSyncDepth is the maximum grid depth at which a
*   thread can issue the device runtime call ::cudaDeviceSynchronize()
*   to wait on child grid launches to complete. This functionality is removed
*   for devices of compute capability >= 9.0, and hence will return error
*   ::cudaErrorUnsupportedLimit on such devices.
* - ::cudaLimitDevRuntimePendingLaunchCount is the maximum number of outstanding
*   device runtime launches.
* - ::cudaLimitMaxL2FetchGranularity is the L2 cache fetch granularity.
* - ::cudaLimitPersistingL2CacheSize is the persisting L2 cache size in bytes.
*
* \param limit  - Limit to query
* \param pValue - Returned size of the limit
*
* \return
* ::cudaSuccess,
* ::cudaErrorUnsupportedLimit,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDeviceSetLimit,
* ::cuCtxGetLimit
*/
int handle_cudaDeviceGetLimit(void *conn) {
    size_t pValue;
    enum cudaLimit limit;

    if (rpc_read(conn, &pValue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(enum cudaLimit)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetLimit(&pValue, limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pValue, sizeof(size_t)) < 0)
        return -1;

    return result;
}

int handle_cudaDeviceGetTexture1DLinearMaxWidth(void *conn) {
    size_t maxWidthInElements;
    struct cudaChannelFormatDesc fmtDesc;
    int device;

    if (rpc_read(conn, &maxWidthInElements, sizeof(size_t)) < 0 ||
        rpc_read(conn, &fmtDesc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetTexture1DLinearMaxWidth(&maxWidthInElements, &fmtDesc, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &maxWidthInElements, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the preferred cache configuration for the current device.
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this returns through \p pCacheConfig the preferred cache
* configuration for the current device. This is only a preference. The
* runtime will use the requested configuration if possible, but it is free to
* choose a different configuration if required to execute functions.
*
* This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
* where the size of the L1 cache and shared memory are fixed.
*
* The supported cache configurations are:
* - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
* - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
* - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
* - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
*
* \param pCacheConfig - Returned cache configuration
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetCacheConfig,
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
* ::cuCtxGetCacheConfig
*/
int handle_cudaDeviceGetCacheConfig(void *conn) {
    enum cudaFuncCache pCacheConfig;

    if (rpc_read(conn, &pCacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetCacheConfig(&pCacheConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pCacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns numerical values that correspond to the least and
* greatest stream priorities.
*
* Returns in \p *leastPriority and \p *greatestPriority the numerical values that correspond
* to the least and greatest stream priorities respectively. Stream priorities
* follow a convention where lower numbers imply greater priorities. The range of
* meaningful stream priorities is given by [\p *greatestPriority, \p *leastPriority].
* If the user attempts to create a stream with a priority value that is
* outside the the meaningful range as specified by this API, the priority is
* automatically clamped down or up to either \p *leastPriority or \p *greatestPriority
* respectively. See ::cudaStreamCreateWithPriority for details on creating a
* priority stream.
* A NULL may be passed in for \p *leastPriority or \p *greatestPriority if the value
* is not desired.
*
* This function will return '0' in both \p *leastPriority and \p *greatestPriority if
* the current context's device does not support stream priorities
* (see ::cudaDeviceGetAttribute).
*
* \param leastPriority    - Pointer to an int in which the numerical value for least
*                           stream priority is returned
* \param greatestPriority - Pointer to an int in which the numerical value for greatest
*                           stream priority is returned
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreateWithPriority,
* ::cudaStreamGetPriority,
* ::cuCtxGetStreamPriorityRange
*/
int handle_cudaDeviceGetStreamPriorityRange(void *conn) {
    int leastPriority;
    int greatestPriority;

    if (rpc_read(conn, &leastPriority, sizeof(int)) < 0 ||
        rpc_read(conn, &greatestPriority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &leastPriority, sizeof(int)) < 0 ||
        rpc_write(conn, &greatestPriority, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for the current device.
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p cacheConfig the preferred cache
* configuration for the current device. This is only a preference. The
* runtime will use the requested configuration if possible, but it is free to
* choose a different configuration if required to execute the function. Any
* function preference set via
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
* or
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
* will be preferred over this device-wide setting. Setting the device-wide
* cache configuration to ::cudaFuncCachePreferNone will cause subsequent
* kernel launches to prefer to not change the cache configuration unless
* required to launch the kernel.
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
* The supported cache configurations are:
* - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
* - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
* - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
* - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
*
* \param cacheConfig - Requested cache configuration
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceGetCacheConfig,
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
* ::cuCtxSetCacheConfig
*/
int handle_cudaDeviceSetCacheConfig(void *conn) {
    enum cudaFuncCache cacheConfig;

    if (rpc_read(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSetCacheConfig(cacheConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the shared memory configuration for the current device.
*
* This function will return in \p pConfig the current size of shared memory banks
* on the current device. On devices with configurable shared memory banks, 
* ::cudaDeviceSetSharedMemConfig can be used to change this setting, so that all 
* subsequent kernel launches will by default use the new bank size. When 
* ::cudaDeviceGetSharedMemConfig is called on devices without configurable shared 
* memory, it will return the fixed bank size of the hardware.
*
* The returned bank configurations can be either:
* - ::cudaSharedMemBankSizeFourByte - shared memory bank width is four bytes.
* - ::cudaSharedMemBankSizeEightByte - shared memory bank width is eight bytes.
*
* \param pConfig - Returned cache configuration
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetCacheConfig,
* ::cudaDeviceGetCacheConfig,
* ::cudaDeviceSetSharedMemConfig,
* ::cudaFuncSetCacheConfig,
* ::cuCtxGetSharedMemConfig
*/
int handle_cudaDeviceGetSharedMemConfig(void *conn) {
    enum cudaSharedMemConfig pConfig;

    if (rpc_read(conn, &pConfig, sizeof(enum cudaSharedMemConfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetSharedMemConfig(&pConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pConfig, sizeof(enum cudaSharedMemConfig)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the shared memory configuration for the current device.
*
* On devices with configurable shared memory banks, this function will set
* the shared memory bank size which is used for all subsequent kernel launches.
* Any per-function setting of shared memory set via ::cudaFuncSetSharedMemConfig
* will override the device wide setting.
*
* Changing the shared memory configuration between launches may introduce
* a device side synchronization point.
*
* Changing the shared memory bank size will not increase shared memory usage
* or affect occupancy of kernels, but may have major effects on performance. 
* Larger bank sizes will allow for greater potential bandwidth to shared memory,
* but will change what kinds of accesses to shared memory will result in bank 
* conflicts.
*
* This function will do nothing on devices with fixed shared memory bank size.
*
* The supported bank configurations are:
* - ::cudaSharedMemBankSizeDefault: set bank width the device default (currently,
*   four bytes)
* - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be four bytes
*   natively.
* - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight 
*   bytes natively.
*
* \param config - Requested cache configuration
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetCacheConfig,
* ::cudaDeviceGetCacheConfig,
* ::cudaDeviceGetSharedMemConfig,
* ::cudaFuncSetCacheConfig,
* ::cuCtxSetSharedMemConfig
*/
int handle_cudaDeviceSetSharedMemConfig(void *conn) {
    enum cudaSharedMemConfig config;

    if (rpc_read(conn, &config, sizeof(enum cudaSharedMemConfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSetSharedMemConfig(config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a handle to a compute device
*
* Returns in \p *device a device ordinal given a PCI bus ID string.
*
* \param device   - Returned device ordinal
*
* \param pciBusId - String in one of the following forms: 
* [domain]:[bus]:[device].[function]
* [domain]:[bus]:[device]
* [bus]:[device].[function]
* where \p domain, \p bus, \p device, and \p function are all hexadecimal values
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDeviceGetPCIBusId,
* ::cuDeviceGetByPCIBusId
*/
int handle_cudaDeviceGetByPCIBusId(void *conn) {
    int device;
    char pciBusId;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &pciBusId, sizeof(char)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetByPCIBusId(&device, &pciBusId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a PCI Bus Id string for the device
*
* Returns an ASCII string identifying the device \p dev in the NULL-terminated
* string pointed to by \p pciBusId. \p len specifies the maximum length of the
* string that may be returned.
*
* \param pciBusId - Returned identifier string for the device in the following format
* [domain]:[bus]:[device].[function]
* where \p domain, \p bus, \p device, and \p function are all hexadecimal values.
* pciBusId should be large enough to store 13 characters including the NULL-terminator.
*
* \param len      - Maximum length of string to store in \p name
*
* \param device   - Device to get identifier string for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDeviceGetByPCIBusId,
* ::cuDeviceGetPCIBusId
*/
int handle_cudaDeviceGetPCIBusId(void *conn) {
    char pciBusId;
    int len;
    int device;

    if (rpc_read(conn, &pciBusId, sizeof(char)) < 0 ||
        rpc_read(conn, &len, sizeof(int)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetPCIBusId(&pciBusId, len, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pciBusId, sizeof(char)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets an interprocess handle for a previously allocated event
*
* Takes as input a previously allocated event. This event must have been 
* created with the ::cudaEventInterprocess and ::cudaEventDisableTiming
* flags set. This opaque handle may be copied into other processes and
* opened with ::cudaIpcOpenEventHandle to allow efficient hardware
* synchronization between GPU work in different processes.
*
* After the event has been been opened in the importing process, 
* ::cudaEventRecord, ::cudaEventSynchronize, ::cudaStreamWaitEvent and 
* ::cudaEventQuery may be used in either process. Performing operations 
* on the imported event after the exported event has been freed 
* with ::cudaEventDestroy will result in undefined behavior.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode.
* Users can test their device for IPC functionality by calling
* ::cudaDeviceGetAttribute with ::cudaDevAttrIpcEventSupport
*
* \param handle - Pointer to a user allocated cudaIpcEventHandle
*                    in which to return the opaque event handle
* \param event   - Event allocated with ::cudaEventInterprocess and 
*                    ::cudaEventDisableTiming flags.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorMemoryAllocation,
* ::cudaErrorMapBufferObjectFailed,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaEventCreate,
* ::cudaEventDestroy,
* ::cudaEventSynchronize,
* ::cudaEventQuery,
* ::cudaStreamWaitEvent,
* ::cudaIpcOpenEventHandle,
* ::cudaIpcGetMemHandle,
* ::cudaIpcOpenMemHandle,
* ::cudaIpcCloseMemHandle,
* ::cuIpcGetEventHandle
*/
int handle_cudaIpcGetEventHandle(void *conn) {
    cudaIpcEventHandle_t handle;
    cudaEvent_t event;

    if (rpc_read(conn, &handle, sizeof(cudaIpcEventHandle_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaIpcGetEventHandle(&handle, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(cudaIpcEventHandle_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Opens an interprocess event handle for use in the current process
*
* Opens an interprocess event handle exported from another process with 
* ::cudaIpcGetEventHandle. This function returns a ::cudaEvent_t that behaves like 
* a locally created event with the ::cudaEventDisableTiming flag specified. 
* This event must be freed with ::cudaEventDestroy.
*
* Performing operations on the imported event after the exported event has 
* been freed with ::cudaEventDestroy will result in undefined behavior.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode.
* Users can test their device for IPC functionality by calling
* ::cudaDeviceGetAttribute with ::cudaDevAttrIpcEventSupport
*
* \param event - Returns the imported event
* \param handle  - Interprocess handle to open
*
* \returns
* ::cudaSuccess,
* ::cudaErrorMapBufferObjectFailed,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue,
* ::cudaErrorDeviceUninitialized
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaEventCreate,
* ::cudaEventDestroy,
* ::cudaEventSynchronize,
* ::cudaEventQuery,
* ::cudaStreamWaitEvent,
* ::cudaIpcGetEventHandle,
* ::cudaIpcGetMemHandle,
* ::cudaIpcOpenMemHandle,
* ::cudaIpcCloseMemHandle,
* ::cuIpcOpenEventHandle
*/
int handle_cudaIpcOpenEventHandle(void *conn) {
    cudaEvent_t event;
    cudaIpcEventHandle_t handle;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &handle, sizeof(cudaIpcEventHandle_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaIpcOpenEventHandle(&event, handle);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets an interprocess memory handle for an existing device memory
*          allocation
*
* Takes a pointer to the base of an existing device memory allocation created 
* with ::cudaMalloc and exports it for use in another process. This is a 
* lightweight operation and may be called multiple times on an allocation
* without adverse effects. 
*
* If a region of memory is freed with ::cudaFree and a subsequent call
* to ::cudaMalloc returns memory with the same device address,
* ::cudaIpcGetMemHandle will return a unique handle for the
* new memory. 
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode.
* Users can test their device for IPC functionality by calling
* ::cudaDeviceGetAttribute with ::cudaDevAttrIpcEventSupport
*
* \param handle - Pointer to user allocated ::cudaIpcMemHandle to return
*                    the handle in.
* \param devPtr - Base pointer to previously allocated device memory 
*
* \returns
* ::cudaSuccess,
* ::cudaErrorMemoryAllocation,
* ::cudaErrorMapBufferObjectFailed,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMalloc,
* ::cudaFree,
* ::cudaIpcGetEventHandle,
* ::cudaIpcOpenEventHandle,
* ::cudaIpcOpenMemHandle,
* ::cudaIpcCloseMemHandle,
* ::cuIpcGetMemHandle
*/
int handle_cudaIpcGetMemHandle(void *conn) {
    cudaIpcMemHandle_t handle;
    void* devPtr;

    if (rpc_read(conn, &handle, sizeof(cudaIpcMemHandle_t)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaIpcGetMemHandle(&handle, &devPtr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &handle, sizeof(cudaIpcMemHandle_t)) < 0 ||
        rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Opens an interprocess memory handle exported from another process
*          and returns a device pointer usable in the local process.
*
* Maps memory exported from another process with ::cudaIpcGetMemHandle into
* the current device address space. For contexts on different devices 
* ::cudaIpcOpenMemHandle can attempt to enable peer access between the
* devices as if the user called ::cudaDeviceEnablePeerAccess. This behavior is 
* controlled by the ::cudaIpcMemLazyEnablePeerAccess flag. 
* ::cudaDeviceCanAccessPeer can determine if a mapping is possible.
*
* ::cudaIpcOpenMemHandle can open handles to devices that may not be visible
* in the process calling the API.
*
* Contexts that may open ::cudaIpcMemHandles are restricted in the following way.
* ::cudaIpcMemHandles from each device in a given process may only be opened 
* by one context per device per other process.
*
* If the memory handle has already been opened by the current context, the
* reference count on the handle is incremented by 1 and the existing device pointer
* is returned.
*
* Memory returned from ::cudaIpcOpenMemHandle must be freed with
* ::cudaIpcCloseMemHandle.
*
* Calling ::cudaFree on an exported memory region before calling
* ::cudaIpcCloseMemHandle in the importing context will result in undefined
* behavior.
* 
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode.
* Users can test their device for IPC functionality by calling
* ::cudaDeviceGetAttribute with ::cudaDevAttrIpcEventSupport
*
* \param devPtr - Returned device pointer
* \param handle - ::cudaIpcMemHandle to open
* \param flags  - Flags for this operation. Must be specified as ::cudaIpcMemLazyEnablePeerAccess
*
* \returns
* ::cudaSuccess,
* ::cudaErrorMapBufferObjectFailed,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorDeviceUninitialized,
* ::cudaErrorTooManyPeers,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \note No guarantees are made about the address returned in \p *devPtr.  
* In particular, multiple processes may not receive the same address for the same \p handle.
*
* \sa
* ::cudaMalloc,
* ::cudaFree,
* ::cudaIpcGetEventHandle,
* ::cudaIpcOpenEventHandle,
* ::cudaIpcGetMemHandle,
* ::cudaIpcCloseMemHandle,
* ::cudaDeviceEnablePeerAccess,
* ::cudaDeviceCanAccessPeer,
* ::cuIpcOpenMemHandle
*/
int handle_cudaIpcOpenMemHandle(void *conn) {
    void* devPtr;
    cudaIpcMemHandle_t handle;
    unsigned int flags;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &handle, sizeof(cudaIpcMemHandle_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaIpcOpenMemHandle(&devPtr, handle, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Attempts to close memory mapped with cudaIpcOpenMemHandle
* 
* Decrements the reference count of the memory returned by ::cudaIpcOpenMemHandle by 1.
* When the reference count reaches 0, this API unmaps the memory. The original allocation
* in the exporting process as well as imported mappings in other processes
* will be unaffected.
*
* Any resources used to enable peer access will be freed if this is the
* last mapping using them.
*
* IPC functionality is restricted to devices with support for unified
* addressing on Linux and Windows operating systems.
* IPC functionality on Windows is restricted to GPUs in TCC mode.
* Users can test their device for IPC functionality by calling
* ::cudaDeviceGetAttribute with ::cudaDevAttrIpcEventSupport
*
* \param devPtr - Device pointer returned by ::cudaIpcOpenMemHandle
* 
* \returns
* ::cudaSuccess,
* ::cudaErrorMapBufferObjectFailed,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMalloc,
* ::cudaFree,
* ::cudaIpcGetEventHandle,
* ::cudaIpcOpenEventHandle,
* ::cudaIpcGetMemHandle,
* ::cudaIpcOpenMemHandle,
* ::cuIpcCloseMemHandle
*/
int handle_cudaIpcCloseMemHandle(void *conn) {
    void* devPtr;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaIpcCloseMemHandle(&devPtr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaDeviceFlushGPUDirectRDMAWrites(void *conn) {
    enum cudaFlushGPUDirectRDMAWritesTarget target;
    enum cudaFlushGPUDirectRDMAWritesScope scope;

    if (rpc_read(conn, &target, sizeof(enum cudaFlushGPUDirectRDMAWritesTarget)) < 0 ||
        rpc_read(conn, &scope, sizeof(enum cudaFlushGPUDirectRDMAWritesScope)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceFlushGPUDirectRDMAWrites(target, scope);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Exit and clean up from CUDA launches
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is identical to the 
* non-deprecated function ::cudaDeviceReset(), which should be used
* instead.
*
* Explicitly destroys all cleans up all resources associated with the current
* device in the current process.  Any subsequent API call to this device will 
* reinitialize the device.  
*
* Note that this function will reset the device immediately.  It is the caller's
* responsibility to ensure that the device is not being accessed by any 
* other host threads from the process when this function is called.
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceReset
*/
int handle_cudaThreadExit(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadExit();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Wait for compute device to finish
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is similar to the 
* non-deprecated function ::cudaDeviceSynchronize(), which should be used
* instead.
*
* Blocks until the device has completed all preceding requested tasks.
* ::cudaThreadSynchronize() returns an error if one of the preceding tasks
* has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for 
* this device, the host thread will block until the device has finished 
* its work.
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSynchronize
*/
int handle_cudaThreadSynchronize(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadSynchronize();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set resource limits
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is identical to the 
* non-deprecated function ::cudaDeviceSetLimit(), which should be used
* instead.
*
* Setting \p limit to \p value is a request by the application to update
* the current limit maintained by the device.  The driver is free to
* modify the requested value to meet h/w requirements (this could be
* clamping to minimum or maximum values, rounding up to nearest element
* size, etc).  The application can use ::cudaThreadGetLimit() to find out
* exactly what the limit has been set to.
*
* Setting each ::cudaLimit has its own specific restrictions, so each is
* discussed here.
*
* - ::cudaLimitStackSize controls the stack size of each GPU thread.
*
* - ::cudaLimitPrintfFifoSize controls the size of the shared FIFO
*   used by the ::printf() device system call.
*   Setting ::cudaLimitPrintfFifoSize must be performed before
*   launching any kernel that uses the ::printf() device
*   system call, otherwise ::cudaErrorInvalidValue will be returned.
*
* - ::cudaLimitMallocHeapSize controls the size of the heap used
*   by the ::malloc() and ::free() device system calls.  Setting
*   ::cudaLimitMallocHeapSize must be performed before launching
*   any kernel that uses the ::malloc() or ::free() device system calls,
*   otherwise ::cudaErrorInvalidValue will be returned.
*
* \param limit - Limit to set
* \param value - Size in bytes of limit
*
* \return
* ::cudaSuccess,
* ::cudaErrorUnsupportedLimit,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetLimit
*/
int handle_cudaThreadSetLimit(void *conn) {
    enum cudaLimit limit;
    size_t value;

    if (rpc_read(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_read(conn, &value, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadSetLimit(limit, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns resource limits
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is identical to the 
* non-deprecated function ::cudaDeviceGetLimit(), which should be used
* instead.
*
* Returns in \p *pValue the current size of \p limit.  The supported
* ::cudaLimit values are:
* - ::cudaLimitStackSize: stack size of each GPU thread;
* - ::cudaLimitPrintfFifoSize: size of the shared FIFO used by the
*   ::printf() device system call.
* - ::cudaLimitMallocHeapSize: size of the heap used by the
*   ::malloc() and ::free() device system calls;
*
* \param limit  - Limit to query
* \param pValue - Returned size in bytes of limit
*
* \return
* ::cudaSuccess,
* ::cudaErrorUnsupportedLimit,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceGetLimit
*/
int handle_cudaThreadGetLimit(void *conn) {
    size_t pValue;
    enum cudaLimit limit;

    if (rpc_read(conn, &pValue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &limit, sizeof(enum cudaLimit)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadGetLimit(&pValue, limit);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pValue, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the preferred cache configuration for the current device.
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is identical to the 
* non-deprecated function ::cudaDeviceGetCacheConfig(), which should be 
* used instead.
* 
* On devices where the L1 cache and shared memory use the same hardware
* resources, this returns through \p pCacheConfig the preferred cache
* configuration for the current device. This is only a preference. The
* runtime will use the requested configuration if possible, but it is free to
* choose a different configuration if required to execute functions.
*
* This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
* where the size of the L1 cache and shared memory are fixed.
*
* The supported cache configurations are:
* - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
* - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
* - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
*
* \param pCacheConfig - Returned cache configuration
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceGetCacheConfig
*/
int handle_cudaThreadGetCacheConfig(void *conn) {
    enum cudaFuncCache pCacheConfig;

    if (rpc_read(conn, &pCacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadGetCacheConfig(&pCacheConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pCacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for the current device.
*
* \deprecated
*
* Note that this function is deprecated because its name does not 
* reflect its behavior.  Its functionality is identical to the 
* non-deprecated function ::cudaDeviceSetCacheConfig(), which should be 
* used instead.
* 
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p cacheConfig the preferred cache
* configuration for the current device. This is only a preference. The
* runtime will use the requested configuration if possible, but it is free to
* choose a different configuration if required to execute the function. Any
* function preference set via
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
* or
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
* will be preferred over this device-wide setting. Setting the device-wide
* cache configuration to ::cudaFuncCachePreferNone will cause subsequent
* kernel launches to prefer to not change the cache configuration unless
* required to launch the kernel.
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
* The supported cache configurations are:
* - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
* - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
* - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
*
* \param cacheConfig - Requested cache configuration
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetCacheConfig
*/
int handle_cudaThreadSetCacheConfig(void *conn) {
    enum cudaFuncCache cacheConfig;

    if (rpc_read(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadSetCacheConfig(cacheConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the last error from a runtime call
*
* Returns the last error that has been produced by any of the runtime calls
* in the same instance of the CUDA Runtime library in the host thread and
* resets it to ::cudaSuccess.
*
* Note: Multiple instances of the CUDA Runtime library can be present in an
* application when using a library that statically links the CUDA Runtime.
*
* \return
* ::cudaSuccess,
* ::cudaErrorMissingConfiguration,
* ::cudaErrorMemoryAllocation,
* ::cudaErrorInitializationError,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorUnmapBufferObjectFailed,
* ::cudaErrorInvalidDevicePointer,
* ::cudaErrorInvalidTexture,
* ::cudaErrorInvalidTextureBinding,
* ::cudaErrorInvalidChannelDescriptor,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorInvalidFilterSetting,
* ::cudaErrorInvalidNormSetting,
* ::cudaErrorUnknown,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorInsufficientDriver,
* ::cudaErrorNoDevice,
* ::cudaErrorSetOnActiveProcess,
* ::cudaErrorStartupFailure,
* ::cudaErrorInvalidPtx,
* ::cudaErrorUnsupportedPtxVersion,
* ::cudaErrorNoKernelImageForDevice,
* ::cudaErrorJitCompilerNotFound,
* ::cudaErrorJitCompilationDisabled
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaPeekAtLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
*/
int handle_cudaGetLastError(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetLastError();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the last error from a runtime call
*
* Returns the last error that has been produced by any of the runtime calls
* in the same instance of the CUDA Runtime library in the host thread. This
* call does not reset the error to ::cudaSuccess like ::cudaGetLastError().
*
* Note: Multiple instances of the CUDA Runtime library can be present in an
* application when using a library that statically links the CUDA Runtime.
*
* \return
* ::cudaSuccess,
* ::cudaErrorMissingConfiguration,
* ::cudaErrorMemoryAllocation,
* ::cudaErrorInitializationError,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorUnmapBufferObjectFailed,
* ::cudaErrorInvalidDevicePointer,
* ::cudaErrorInvalidTexture,
* ::cudaErrorInvalidTextureBinding,
* ::cudaErrorInvalidChannelDescriptor,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorInvalidFilterSetting,
* ::cudaErrorInvalidNormSetting,
* ::cudaErrorUnknown,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorInsufficientDriver,
* ::cudaErrorNoDevice,
* ::cudaErrorSetOnActiveProcess,
* ::cudaErrorStartupFailure,
* ::cudaErrorInvalidPtx,
* ::cudaErrorUnsupportedPtxVersion,
* ::cudaErrorNoKernelImageForDevice,
* ::cudaErrorJitCompilerNotFound,
* ::cudaErrorJitCompilationDisabled
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
*/
int handle_cudaPeekAtLastError(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaPeekAtLastError();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the number of compute-capable devices
*
* Returns in \p *count the number of devices with compute capability greater
* or equal to 2.0 that are available for execution.
*
* \param count - Returns the number of devices with compute capability
* greater or equal to 2.0
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDevice, ::cudaSetDevice, ::cudaGetDeviceProperties,
* ::cudaChooseDevice, 
* ::cudaInitDevice,
* ::cuDeviceGetCount
*/
int handle_cudaGetDeviceCount(void *conn) {
    int count;

    if (rpc_read(conn, &count, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetDeviceCount(&count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &count, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about the compute-device
*
* Returns in \p *prop the properties of device \p dev. The ::cudaDeviceProp
* structure is defined as:
* \code
    struct cudaDeviceProp {
        char name[256];
        cudaUUID_t uuid;
        size_t totalGlobalMem;
        size_t sharedMemPerBlock;
        int regsPerBlock;
        int warpSize;
        size_t memPitch;
        int maxThreadsPerBlock;
        int maxThreadsDim[3];
        int maxGridSize[3];
        int clockRate;
        size_t totalConstMem;
        int major;
        int minor;
        size_t textureAlignment;
        size_t texturePitchAlignment;
        int deviceOverlap;
        int multiProcessorCount;
        int kernelExecTimeoutEnabled;
        int integrated;
        int canMapHostMemory;
        int computeMode;
        int maxTexture1D;
        int maxTexture1DMipmap;
        int maxTexture1DLinear;
        int maxTexture2D[2];
        int maxTexture2DMipmap[2];
        int maxTexture2DLinear[3];
        int maxTexture2DGather[2];
        int maxTexture3D[3];
        int maxTexture3DAlt[3];
        int maxTextureCubemap;
        int maxTexture1DLayered[2];
        int maxTexture2DLayered[3];
        int maxTextureCubemapLayered[2];
        int maxSurface1D;
        int maxSurface2D[2];
        int maxSurface3D[3];
        int maxSurface1DLayered[2];
        int maxSurface2DLayered[3];
        int maxSurfaceCubemap;
        int maxSurfaceCubemapLayered[2];
        size_t surfaceAlignment;
        int concurrentKernels;
        int ECCEnabled;
        int pciBusID;
        int pciDeviceID;
        int pciDomainID;
        int tccDriver;
        int asyncEngineCount;
        int unifiedAddressing;
        int memoryClockRate;
        int memoryBusWidth;
        int l2CacheSize;
        int persistingL2CacheMaxSize;
        int maxThreadsPerMultiProcessor;
        int streamPrioritiesSupported;
        int globalL1CacheSupported;
        int localL1CacheSupported;
        size_t sharedMemPerMultiprocessor;
        int regsPerMultiprocessor;
        int managedMemory;
        int isMultiGpuBoard;
        int multiGpuBoardGroupID;
        int singleToDoublePrecisionPerfRatio;
        int pageableMemoryAccess;
        int concurrentManagedAccess;
        int computePreemptionSupported;
        int canUseHostPointerForRegisteredMem;
        int cooperativeLaunch;
        int cooperativeMultiDeviceLaunch;
        int pageableMemoryAccessUsesHostPageTables;
        int directManagedMemAccessFromHost;
        int accessPolicyMaxWindowSize;
    }
 \endcode
* where:
* - \ref ::cudaDeviceProp::name "name[256]" is an ASCII string identifying
*   the device.
* - \ref ::cudaDeviceProp::uuid "uuid" is a 16-byte unique identifier.
* - \ref ::cudaDeviceProp::totalGlobalMem "totalGlobalMem" is the total
*   amount of global memory available on the device in bytes.
* - \ref ::cudaDeviceProp::sharedMemPerBlock "sharedMemPerBlock" is the
*   maximum amount of shared memory available to a thread block in bytes.
* - \ref ::cudaDeviceProp::regsPerBlock "regsPerBlock" is the maximum number
*   of 32-bit registers available to a thread block.
* - \ref ::cudaDeviceProp::warpSize "warpSize" is the warp size in threads.
* - \ref ::cudaDeviceProp::memPitch "memPitch" is the maximum pitch in
*   bytes allowed by the memory copy functions that involve memory regions
*   allocated through ::cudaMallocPitch().
* - \ref ::cudaDeviceProp::maxThreadsPerBlock "maxThreadsPerBlock" is the
*   maximum number of threads per block.
* - \ref ::cudaDeviceProp::maxThreadsDim "maxThreadsDim[3]" contains the
*   maximum size of each dimension of a block.
* - \ref ::cudaDeviceProp::maxGridSize "maxGridSize[3]" contains the
*   maximum size of each dimension of a grid.
* - \ref ::cudaDeviceProp::clockRate "clockRate" is the clock frequency in
*   kilohertz.
* - \ref ::cudaDeviceProp::totalConstMem "totalConstMem" is the total amount
*   of constant memory available on the device in bytes.
* - \ref ::cudaDeviceProp::major "major",
*   \ref ::cudaDeviceProp::minor "minor" are the major and minor revision
*   numbers defining the device's compute capability.
* - \ref ::cudaDeviceProp::textureAlignment "textureAlignment" is the
*   alignment requirement; texture base addresses that are aligned to
*   \ref ::cudaDeviceProp::textureAlignment "textureAlignment" bytes do not
*   need an offset applied to texture fetches.
* - \ref ::cudaDeviceProp::texturePitchAlignment "texturePitchAlignment" is the
*   pitch alignment requirement for 2D texture references that are bound to 
*   pitched memory.
* - \ref ::cudaDeviceProp::deviceOverlap "deviceOverlap" is 1 if the device
*   can concurrently copy memory between host and device while executing a
*   kernel, or 0 if not.  Deprecated, use instead asyncEngineCount.
* - \ref ::cudaDeviceProp::multiProcessorCount "multiProcessorCount" is the
*   number of multiprocessors on the device.
* - \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
*   is 1 if there is a run time limit for kernels executed on the device, or
*   0 if not.
* - \ref ::cudaDeviceProp::integrated "integrated" is 1 if the device is an
*   integrated (motherboard) GPU and 0 if it is a discrete (card) component.
* - \ref ::cudaDeviceProp::canMapHostMemory "canMapHostMemory" is 1 if the
*   device can map host memory into the CUDA address space for use with
*   ::cudaHostAlloc()/::cudaHostGetDevicePointer(), or 0 if not.
* - \ref ::cudaDeviceProp::computeMode "computeMode" is the compute mode
*   that the device is currently in. Available modes are as follows:
*   - cudaComputeModeDefault: Default mode - Device is not restricted and
*     multiple threads can use ::cudaSetDevice() with this device.
*   - cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
*     ::cudaSetDevice() with this device.
*   - cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many 
*     threads in one process will be able to use ::cudaSetDevice() with this device.
*   <br> When an occupied exclusive mode device is chosen with ::cudaSetDevice,
*   all subsequent non-device management runtime functions will return
*   ::cudaErrorDevicesUnavailable.
* - \ref ::cudaDeviceProp::maxTexture1D "maxTexture1D" is the maximum 1D
*   texture size.
* - \ref ::cudaDeviceProp::maxTexture1DMipmap "maxTexture1DMipmap" is the maximum
*   1D mipmapped texture texture size.
* - \ref ::cudaDeviceProp::maxTexture1DLinear "maxTexture1DLinear" is the maximum
*   1D texture size for textures bound to linear memory.
* - \ref ::cudaDeviceProp::maxTexture2D "maxTexture2D[2]" contains the maximum
*   2D texture dimensions.
* - \ref ::cudaDeviceProp::maxTexture2DMipmap "maxTexture2DMipmap[2]" contains the
*   maximum 2D mipmapped texture dimensions.
* - \ref ::cudaDeviceProp::maxTexture2DLinear "maxTexture2DLinear[3]" contains the 
*   maximum 2D texture dimensions for 2D textures bound to pitch linear memory.
* - \ref ::cudaDeviceProp::maxTexture2DGather "maxTexture2DGather[2]" contains the 
*   maximum 2D texture dimensions if texture gather operations have to be performed.
* - \ref ::cudaDeviceProp::maxTexture3D "maxTexture3D[3]" contains the maximum
*   3D texture dimensions.
* - \ref ::cudaDeviceProp::maxTexture3DAlt "maxTexture3DAlt[3]"
*   contains the maximum alternate 3D texture dimensions.
* - \ref ::cudaDeviceProp::maxTextureCubemap "maxTextureCubemap" is the 
*   maximum cubemap texture width or height.
* - \ref ::cudaDeviceProp::maxTexture1DLayered "maxTexture1DLayered[2]" contains
*   the maximum 1D layered texture dimensions.
* - \ref ::cudaDeviceProp::maxTexture2DLayered "maxTexture2DLayered[3]" contains
*   the maximum 2D layered texture dimensions.
* - \ref ::cudaDeviceProp::maxTextureCubemapLayered "maxTextureCubemapLayered[2]"
*   contains the maximum cubemap layered texture dimensions.
* - \ref ::cudaDeviceProp::maxSurface1D "maxSurface1D" is the maximum 1D
*   surface size.
* - \ref ::cudaDeviceProp::maxSurface2D "maxSurface2D[2]" contains the maximum
*   2D surface dimensions.
* - \ref ::cudaDeviceProp::maxSurface3D "maxSurface3D[3]" contains the maximum
*   3D surface dimensions.
* - \ref ::cudaDeviceProp::maxSurface1DLayered "maxSurface1DLayered[2]" contains
*   the maximum 1D layered surface dimensions.
* - \ref ::cudaDeviceProp::maxSurface2DLayered "maxSurface2DLayered[3]" contains
*   the maximum 2D layered surface dimensions.
* - \ref ::cudaDeviceProp::maxSurfaceCubemap "maxSurfaceCubemap" is the maximum 
*   cubemap surface width or height.
* - \ref ::cudaDeviceProp::maxSurfaceCubemapLayered "maxSurfaceCubemapLayered[2]"
*   contains the maximum cubemap layered surface dimensions.
* - \ref ::cudaDeviceProp::surfaceAlignment "surfaceAlignment" specifies the
*   alignment requirements for surfaces.
* - \ref ::cudaDeviceProp::concurrentKernels "concurrentKernels" is 1 if the
*   device supports executing multiple kernels within the same context
*   simultaneously, or 0 if not. It is not guaranteed that multiple kernels
*   will be resident on the device concurrently so this feature should not be
*   relied upon for correctness.
* - \ref ::cudaDeviceProp::ECCEnabled "ECCEnabled" is 1 if the device has ECC
*   support turned on, or 0 if not.
* - \ref ::cudaDeviceProp::pciBusID "pciBusID" is the PCI bus identifier of
*   the device.
* - \ref ::cudaDeviceProp::pciDeviceID "pciDeviceID" is the PCI device
*   (sometimes called slot) identifier of the device.
* - \ref ::cudaDeviceProp::pciDomainID "pciDomainID" is the PCI domain identifier
*   of the device.
* - \ref ::cudaDeviceProp::tccDriver "tccDriver" is 1 if the device is using a
*   TCC driver or 0 if not.
* - \ref ::cudaDeviceProp::asyncEngineCount "asyncEngineCount" is 1 when the
*   device can concurrently copy memory between host and device while executing
*   a kernel. It is 2 when the device can concurrently copy memory between host
*   and device in both directions and execute a kernel at the same time. It is
*   0 if neither of these is supported.
* - \ref ::cudaDeviceProp::unifiedAddressing "unifiedAddressing" is 1 if the device 
*   shares a unified address space with the host and 0 otherwise.
* - \ref ::cudaDeviceProp::memoryClockRate "memoryClockRate" is the peak memory 
*   clock frequency in kilohertz.
* - \ref ::cudaDeviceProp::memoryBusWidth "memoryBusWidth" is the memory bus width  
*   in bits.
* - \ref ::cudaDeviceProp::l2CacheSize "l2CacheSize" is L2 cache size in bytes. 
* - \ref ::cudaDeviceProp::persistingL2CacheMaxSize "persistingL2CacheMaxSize" is L2 cache's maximum persisting lines size in bytes.
* - \ref ::cudaDeviceProp::maxThreadsPerMultiProcessor "maxThreadsPerMultiProcessor"  
*   is the number of maximum resident threads per multiprocessor.
* - \ref ::cudaDeviceProp::streamPrioritiesSupported "streamPrioritiesSupported"
*   is 1 if the device supports stream priorities, or 0 if it is not supported.
* - \ref ::cudaDeviceProp::globalL1CacheSupported "globalL1CacheSupported"
*   is 1 if the device supports caching of globals in L1 cache, or 0 if it is not supported.
* - \ref ::cudaDeviceProp::localL1CacheSupported "localL1CacheSupported"
*   is 1 if the device supports caching of locals in L1 cache, or 0 if it is not supported.
* - \ref ::cudaDeviceProp::sharedMemPerMultiprocessor "sharedMemPerMultiprocessor" is the
*   maximum amount of shared memory available to a multiprocessor in bytes; this amount is
*   shared by all thread blocks simultaneously resident on a multiprocessor.
* - \ref ::cudaDeviceProp::regsPerMultiprocessor "regsPerMultiprocessor" is the maximum number
*   of 32-bit registers available to a multiprocessor; this number is shared
*   by all thread blocks simultaneously resident on a multiprocessor.
* - \ref ::cudaDeviceProp::managedMemory "managedMemory"
*   is 1 if the device supports allocating managed memory on this system, or 0 if it is not supported.
* - \ref ::cudaDeviceProp::isMultiGpuBoard "isMultiGpuBoard"
*   is 1 if the device is on a multi-GPU board (e.g. Gemini cards), and 0 if not;
* - \ref ::cudaDeviceProp::multiGpuBoardGroupID "multiGpuBoardGroupID" is a unique identifier
*   for a group of devices associated with the same board.
*   Devices on the same multi-GPU board will share the same identifier.
* - \ref ::cudaDeviceProp::hostNativeAtomicSupported "hostNativeAtomicSupported"
*   is 1 if the link between the device and the host supports native atomic operations, or 0 if it is not supported.
* - \ref ::cudaDeviceProp::singleToDoublePrecisionPerfRatio "singleToDoublePrecisionPerfRatio"  
*   is the ratio of single precision performance (in floating-point operations per second)
*   to double precision performance.
* - \ref ::cudaDeviceProp::pageableMemoryAccess "pageableMemoryAccess" is 1 if the device supports
*   coherently accessing pageable memory without calling cudaHostRegister on it, and 0 otherwise.
* - \ref ::cudaDeviceProp::concurrentManagedAccess "concurrentManagedAccess" is 1 if the device can
*   coherently access managed memory concurrently with the CPU, and 0 otherwise.
* - \ref ::cudaDeviceProp::computePreemptionSupported "computePreemptionSupported" is 1 if the device
*   supports Compute Preemption, and 0 otherwise.
* - \ref ::cudaDeviceProp::canUseHostPointerForRegisteredMem "canUseHostPointerForRegisteredMem" is 1 if
*   the device can access host registered memory at the same virtual address as the CPU, and 0 otherwise.
* - \ref ::cudaDeviceProp::cooperativeLaunch "cooperativeLaunch" is 1 if the device supports launching
*   cooperative kernels via ::cudaLaunchCooperativeKernel, and 0 otherwise.
* - \ref ::cudaDeviceProp::cooperativeMultiDeviceLaunch "cooperativeMultiDeviceLaunch" is 1 if the device
*   supports launching cooperative kernels via ::cudaLaunchCooperativeKernelMultiDevice, and 0 otherwise.
* - \ref ::cudaDeviceProp::sharedMemPerBlockOptin "sharedMemPerBlockOptin"
*   is the per device maximum shared memory per block usable by special opt in
* - \ref ::cudaDeviceProp::pageableMemoryAccessUsesHostPageTables "pageableMemoryAccessUsesHostPageTables" is 1 if the device accesses
*   pageable memory via the host's page tables, and 0 otherwise.
* - \ref ::cudaDeviceProp::directManagedMemAccessFromHost "directManagedMemAccessFromHost" is 1 if the host can directly access managed
*   memory on the device without migration, and 0 otherwise.
* - \ref ::cudaDeviceProp::maxBlocksPerMultiProcessor "maxBlocksPerMultiProcessor" is the maximum number of thread blocks
*   that can reside on a multiprocessor.
* - \ref ::cudaDeviceProp::accessPolicyMaxWindowSize "accessPolicyMaxWindowSize" is
*   the maximum value of ::cudaAccessPolicyWindow::num_bytes.
* - \ref ::cudaDeviceProp::reservedSharedMemPerBlock "reservedSharedMemPerBlock"
*   is the shared memory reserved by CUDA driver per block in bytes
* - \ref ::cudaDeviceProp::hostRegisterSupported "hostRegisterSupported"
*  is 1 if the device supports host memory registration via ::cudaHostRegister, and 0 otherwise.
* - \ref ::cudaDeviceProp::sparseCudaArraySupported "sparseCudaArraySupported"
*  is 1 if the device supports sparse CUDA arrays and sparse CUDA mipmapped arrays, 0 otherwise
* - \ref ::cudaDeviceProp::hostRegisterReadOnlySupported "hostRegisterReadOnlySupported"
*  is 1 if the device supports using the ::cudaHostRegister flag cudaHostRegisterReadOnly to register memory that must be mapped as
*  read-only to the GPU
* - \ref ::cudaDeviceProp::timelineSemaphoreInteropSupported "timelineSemaphoreInteropSupported"
*  is 1 if external timeline semaphore interop is supported on the device, 0 otherwise
* - \ref ::cudaDeviceProp::memoryPoolsSupported "memoryPoolsSupported"
*  is 1 if the device supports using the cudaMallocAsync and cudaMemPool family of APIs, 0 otherwise
* - \ref ::cudaDeviceProp::gpuDirectRDMASupported "gpuDirectRDMASupported"
*  is 1 if the device supports GPUDirect RDMA APIs, 0 otherwise
* - \ref ::cudaDeviceProp::gpuDirectRDMAFlushWritesOptions "gpuDirectRDMAFlushWritesOptions"
*  is a bitmask to be interpreted according to the ::cudaFlushGPUDirectRDMAWritesOptions enum
* - \ref ::cudaDeviceProp::gpuDirectRDMAWritesOrdering "gpuDirectRDMAWritesOrdering"
*  See the ::cudaGPUDirectRDMAWritesOrdering enum for numerical values
* - \ref ::cudaDeviceProp::memoryPoolSupportedHandleTypes "memoryPoolSupportedHandleTypes"
*  is a bitmask of handle types supported with mempool-based IPC
* - \ref ::cudaDeviceProp::deferredMappingCudaArraySupported "deferredMappingCudaArraySupported"
*  is 1 if the device supports deferred mapping CUDA arrays and CUDA mipmapped arrays
* - \ref ::cudaDeviceProp::ipcEventSupported "ipcEventSupported"
*  is 1 if the device supports IPC Events, and 0 otherwise
* - \ref ::cudaDeviceProp::unifiedFunctionPointers "unifiedFunctionPointers"
*  is 1 if the device support unified pointers, and 0 otherwise
*
* \param prop   - Properties for the specified device
* \param device - Device number to get properties for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
* ::cudaDeviceGetAttribute, 
* ::cudaInitDevice,
* ::cuDeviceGetAttribute,
* ::cuDeviceGetName
*/
int handle_cudaGetDeviceProperties_v2(void *conn) {
    struct cudaDeviceProp prop;
    int device;

    if (rpc_read(conn, &prop, sizeof(struct cudaDeviceProp)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetDeviceProperties_v2(&prop, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &prop, sizeof(struct cudaDeviceProp)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns information about the device
*
* Returns in \p *value the integer value of the attribute \p attr on device
* \p device. The supported attributes are:
* - ::cudaDevAttrMaxThreadsPerBlock: Maximum number of threads per block
* - ::cudaDevAttrMaxBlockDimX: Maximum x-dimension of a block
* - ::cudaDevAttrMaxBlockDimY: Maximum y-dimension of a block
* - ::cudaDevAttrMaxBlockDimZ: Maximum z-dimension of a block
* - ::cudaDevAttrMaxGridDimX: Maximum x-dimension of a grid
* - ::cudaDevAttrMaxGridDimY: Maximum y-dimension of a grid
* - ::cudaDevAttrMaxGridDimZ: Maximum z-dimension of a grid
* - ::cudaDevAttrMaxSharedMemoryPerBlock: Maximum amount of shared memory
*   available to a thread block in bytes
* - ::cudaDevAttrTotalConstantMemory: Memory available on device for
*   __constant__ variables in a CUDA C kernel in bytes
* - ::cudaDevAttrWarpSize: Warp size in threads
* - ::cudaDevAttrMaxPitch: Maximum pitch in bytes allowed by the memory copy
*   functions that involve memory regions allocated through ::cudaMallocPitch()
* - ::cudaDevAttrMaxTexture1DWidth: Maximum 1D texture width
* - ::cudaDevAttrMaxTexture1DLinearWidth: Maximum width for a 1D texture bound
*   to linear memory
* - ::cudaDevAttrMaxTexture1DMipmappedWidth: Maximum mipmapped 1D texture width
* - ::cudaDevAttrMaxTexture2DWidth: Maximum 2D texture width
* - ::cudaDevAttrMaxTexture2DHeight: Maximum 2D texture height
* - ::cudaDevAttrMaxTexture2DLinearWidth: Maximum width for a 2D texture
*   bound to linear memory
* - ::cudaDevAttrMaxTexture2DLinearHeight: Maximum height for a 2D texture
*   bound to linear memory
* - ::cudaDevAttrMaxTexture2DLinearPitch: Maximum pitch in bytes for a 2D
*   texture bound to linear memory
* - ::cudaDevAttrMaxTexture2DMipmappedWidth: Maximum mipmapped 2D texture
*   width
* - ::cudaDevAttrMaxTexture2DMipmappedHeight: Maximum mipmapped 2D texture
*   height
* - ::cudaDevAttrMaxTexture3DWidth: Maximum 3D texture width
* - ::cudaDevAttrMaxTexture3DHeight: Maximum 3D texture height
* - ::cudaDevAttrMaxTexture3DDepth: Maximum 3D texture depth
* - ::cudaDevAttrMaxTexture3DWidthAlt: Alternate maximum 3D texture width,
*   0 if no alternate maximum 3D texture size is supported
* - ::cudaDevAttrMaxTexture3DHeightAlt: Alternate maximum 3D texture height,
*   0 if no alternate maximum 3D texture size is supported
* - ::cudaDevAttrMaxTexture3DDepthAlt: Alternate maximum 3D texture depth,
*   0 if no alternate maximum 3D texture size is supported
* - ::cudaDevAttrMaxTextureCubemapWidth: Maximum cubemap texture width or
*   height
* - ::cudaDevAttrMaxTexture1DLayeredWidth: Maximum 1D layered texture width
* - ::cudaDevAttrMaxTexture1DLayeredLayers: Maximum layers in a 1D layered
*   texture
* - ::cudaDevAttrMaxTexture2DLayeredWidth: Maximum 2D layered texture width
* - ::cudaDevAttrMaxTexture2DLayeredHeight: Maximum 2D layered texture height
* - ::cudaDevAttrMaxTexture2DLayeredLayers: Maximum layers in a 2D layered
*   texture
* - ::cudaDevAttrMaxTextureCubemapLayeredWidth: Maximum cubemap layered
*   texture width or height
* - ::cudaDevAttrMaxTextureCubemapLayeredLayers: Maximum layers in a cubemap
*   layered texture
* - ::cudaDevAttrMaxSurface1DWidth: Maximum 1D surface width
* - ::cudaDevAttrMaxSurface2DWidth: Maximum 2D surface width
* - ::cudaDevAttrMaxSurface2DHeight: Maximum 2D surface height
* - ::cudaDevAttrMaxSurface3DWidth: Maximum 3D surface width
* - ::cudaDevAttrMaxSurface3DHeight: Maximum 3D surface height
* - ::cudaDevAttrMaxSurface3DDepth: Maximum 3D surface depth
* - ::cudaDevAttrMaxSurface1DLayeredWidth: Maximum 1D layered surface width
* - ::cudaDevAttrMaxSurface1DLayeredLayers: Maximum layers in a 1D layered
*   surface
* - ::cudaDevAttrMaxSurface2DLayeredWidth: Maximum 2D layered surface width
* - ::cudaDevAttrMaxSurface2DLayeredHeight: Maximum 2D layered surface height
* - ::cudaDevAttrMaxSurface2DLayeredLayers: Maximum layers in a 2D layered
*   surface
* - ::cudaDevAttrMaxSurfaceCubemapWidth: Maximum cubemap surface width
* - ::cudaDevAttrMaxSurfaceCubemapLayeredWidth: Maximum cubemap layered
*   surface width
* - ::cudaDevAttrMaxSurfaceCubemapLayeredLayers: Maximum layers in a cubemap
*   layered surface
* - ::cudaDevAttrMaxRegistersPerBlock: Maximum number of 32-bit registers 
*   available to a thread block
* - ::cudaDevAttrClockRate: Peak clock frequency in kilohertz
* - ::cudaDevAttrTextureAlignment: Alignment requirement; texture base
*   addresses aligned to ::textureAlign bytes do not need an offset applied
*   to texture fetches
* - ::cudaDevAttrTexturePitchAlignment: Pitch alignment requirement for 2D
*   texture references bound to pitched memory
* - ::cudaDevAttrGpuOverlap: 1 if the device can concurrently copy memory
*   between host and device while executing a kernel, or 0 if not
* - ::cudaDevAttrMultiProcessorCount: Number of multiprocessors on the device
* - ::cudaDevAttrKernelExecTimeout: 1 if there is a run time limit for kernels
*   executed on the device, or 0 if not
* - ::cudaDevAttrIntegrated: 1 if the device is integrated with the memory
*   subsystem, or 0 if not
* - ::cudaDevAttrCanMapHostMemory: 1 if the device can map host memory into
*   the CUDA address space, or 0 if not
* - ::cudaDevAttrComputeMode: Compute mode is the compute mode that the device
*   is currently in. Available modes are as follows:
*   - ::cudaComputeModeDefault: Default mode - Device is not restricted and
*     multiple threads can use ::cudaSetDevice() with this device.
*   - ::cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
*     ::cudaSetDevice() with this device.
*   - ::cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many 
*     threads in one process will be able to use ::cudaSetDevice() with this
*     device.
* - ::cudaDevAttrConcurrentKernels: 1 if the device supports executing
*   multiple kernels within the same context simultaneously, or 0 if
*   not. It is not guaranteed that multiple kernels will be resident on the
*   device concurrently so this feature should not be relied upon for
*   correctness.
* - ::cudaDevAttrEccEnabled: 1 if error correction is enabled on the device,
*   0 if error correction is disabled or not supported by the device
* - ::cudaDevAttrPciBusId: PCI bus identifier of the device
* - ::cudaDevAttrPciDeviceId: PCI device (also known as slot) identifier of
*   the device
* - ::cudaDevAttrTccDriver: 1 if the device is using a TCC driver. TCC is only
*   available on Tesla hardware running Windows Vista or later.
* - ::cudaDevAttrMemoryClockRate: Peak memory clock frequency in kilohertz
* - ::cudaDevAttrGlobalMemoryBusWidth: Global memory bus width in bits
* - ::cudaDevAttrL2CacheSize: Size of L2 cache in bytes. 0 if the device
*   doesn't have L2 cache.
* - ::cudaDevAttrMaxThreadsPerMultiProcessor: Maximum resident threads per 
*   multiprocessor
* - ::cudaDevAttrUnifiedAddressing: 1 if the device shares a unified address
*   space with the host, or 0 if not
* - ::cudaDevAttrComputeCapabilityMajor: Major compute capability version
*   number
* - ::cudaDevAttrComputeCapabilityMinor: Minor compute capability version
*   number
* - ::cudaDevAttrStreamPrioritiesSupported: 1 if the device supports stream
*   priorities, or 0 if not
* - ::cudaDevAttrGlobalL1CacheSupported: 1 if device supports caching globals 
*    in L1 cache, 0 if not
* - ::cudaDevAttrLocalL1CacheSupported: 1 if device supports caching locals 
*    in L1 cache, 0 if not
* - ::cudaDevAttrMaxSharedMemoryPerMultiprocessor: Maximum amount of shared memory
*   available to a multiprocessor in bytes; this amount is shared by all 
*   thread blocks simultaneously resident on a multiprocessor
* - ::cudaDevAttrMaxRegistersPerMultiprocessor: Maximum number of 32-bit registers 
*   available to a multiprocessor; this number is shared by all thread blocks
*   simultaneously resident on a multiprocessor
* - ::cudaDevAttrManagedMemory: 1 if device supports allocating
*   managed memory, 0 if not
* - ::cudaDevAttrIsMultiGpuBoard: 1 if device is on a multi-GPU board, 0 if not
* - ::cudaDevAttrMultiGpuBoardGroupID: Unique identifier for a group of devices on the
*   same multi-GPU board
* - ::cudaDevAttrHostNativeAtomicSupported: 1 if the link between the device and the
*   host supports native atomic operations
* - ::cudaDevAttrSingleToDoublePrecisionPerfRatio: Ratio of single precision performance
*   (in floating-point operations per second) to double precision performance
* - ::cudaDevAttrPageableMemoryAccess: 1 if the device supports coherently accessing
*   pageable memory without calling cudaHostRegister on it, and 0 otherwise
* - ::cudaDevAttrConcurrentManagedAccess: 1 if the device can coherently access managed
*   memory concurrently with the CPU, and 0 otherwise
* - ::cudaDevAttrComputePreemptionSupported: 1 if the device supports
*   Compute Preemption, 0 if not
* - ::cudaDevAttrCanUseHostPointerForRegisteredMem: 1 if the device can access host
*   registered memory at the same virtual address as the CPU, and 0 otherwise
* - ::cudaDevAttrCooperativeLaunch: 1 if the device supports launching cooperative kernels
*   via ::cudaLaunchCooperativeKernel, and 0 otherwise
* - ::cudaDevAttrCooperativeMultiDeviceLaunch: 1 if the device supports launching cooperative
*   kernels via ::cudaLaunchCooperativeKernelMultiDevice, and 0 otherwise
* - ::cudaDevAttrCanFlushRemoteWrites: 1 if the device supports flushing of outstanding 
*   remote writes, and 0 otherwise
* - ::cudaDevAttrHostRegisterSupported: 1 if the device supports host memory registration
*   via ::cudaHostRegister, and 0 otherwise
* - ::cudaDevAttrPageableMemoryAccessUsesHostPageTables: 1 if the device accesses pageable memory via the
*   host's page tables, and 0 otherwise
* - ::cudaDevAttrDirectManagedMemAccessFromHost: 1 if the host can directly access managed memory on the device
*   without migration, and 0 otherwise
* - ::cudaDevAttrMaxSharedMemoryPerBlockOptin: Maximum per block shared memory size on the device. This value can
*   be opted into when using ::cudaFuncSetAttribute
* - ::cudaDevAttrMaxBlocksPerMultiprocessor: Maximum number of thread blocks that can reside on a multiprocessor
* - ::cudaDevAttrMaxPersistingL2CacheSize: Maximum L2 persisting lines capacity setting in bytes
* - ::cudaDevAttrMaxAccessPolicyWindowSize: Maximum value of cudaAccessPolicyWindow::num_bytes
* - ::cudaDevAttrReservedSharedMemoryPerBlock: Shared memory reserved by CUDA driver per block in bytes
* - ::cudaDevAttrSparseCudaArraySupported: 1 if the device supports sparse CUDA arrays and sparse CUDA mipmapped arrays.
* - ::cudaDevAttrHostRegisterReadOnlySupported: Device supports using the ::cudaHostRegister flag cudaHostRegisterReadOnly
*   to register memory that must be mapped as read-only to the GPU
* - ::cudaDevAttrMemoryPoolsSupported: 1 if the device supports using the cudaMallocAsync and cudaMemPool family of APIs, and 0 otherwise
* - ::cudaDevAttrGPUDirectRDMASupported: 1 if the device supports GPUDirect RDMA APIs, and 0 otherwise
* - ::cudaDevAttrGPUDirectRDMAFlushWritesOptions: bitmask to be interpreted according to the ::cudaFlushGPUDirectRDMAWritesOptions enum 
* - ::cudaDevAttrGPUDirectRDMAWritesOrdering: see the ::cudaGPUDirectRDMAWritesOrdering enum for numerical values
* - ::cudaDevAttrMemoryPoolSupportedHandleTypes: Bitmask of handle types supported with mempool based IPC
* - ::cudaDevAttrDeferredMappingCudaArraySupported : 1 if the device supports deferred mapping CUDA arrays and CUDA mipmapped arrays.
* - ::cudaDevAttrIpcEventSupport: 1 if the device supports IPC Events.
*
* \param value  - Returned device attribute value
* \param attr   - Device attribute to query
* \param device - Device number to query 
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
* ::cudaGetDeviceProperties, 
* ::cudaInitDevice,
* ::cuDeviceGetAttribute
*/
int handle_cudaDeviceGetAttribute(void *conn) {
    int value;
    enum cudaDeviceAttr attr;
    int device;

    if (rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaDeviceAttr)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetAttribute(&value, attr, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the default mempool of a device
*
* The default mempool of a device contains device memory from that device.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue
* ::cudaErrorNotSupported
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cuDeviceGetDefaultMemPool, ::cudaMallocAsync, ::cudaMemPoolTrimTo, ::cudaMemPoolGetAttribute, ::cudaDeviceSetMemPool, ::cudaMemPoolSetAttribute, ::cudaMemPoolSetAccess
*/
int handle_cudaDeviceGetDefaultMemPool(void *conn) {
    cudaMemPool_t memPool;
    int device;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetDefaultMemPool(&memPool, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the current memory pool of a device
*
* The memory pool must be local to the specified device.
* Unless a mempool is specified in the ::cudaMallocAsync call,
* ::cudaMallocAsync allocates from the current mempool of the provided stream's device.
* By default, a device's current memory pool is its default memory pool.
*
* \note Use ::cudaMallocFromPoolAsync to specify asynchronous allocations from a device different
* than the one the stream runs on.
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* ::cudaErrorInvalidDevice
* ::cudaErrorNotSupported
* \notefnerr
* \note_callback
*
* \sa ::cuDeviceSetMemPool, ::cudaDeviceGetMemPool, ::cudaDeviceGetDefaultMemPool, ::cudaMemPoolCreate, ::cudaMemPoolDestroy, ::cudaMallocFromPoolAsync
*/
int handle_cudaDeviceSetMemPool(void *conn) {
    int device;
    cudaMemPool_t memPool;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSetMemPool(device, memPool);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the current mempool for a device
*
* Returns the last pool provided to ::cudaDeviceSetMemPool for this device
* or the device's default memory pool if ::cudaDeviceSetMemPool has never been called.
* By default the current mempool is the default mempool for a device,
* otherwise the returned pool must have been set with ::cuDeviceSetMemPool or ::cudaDeviceSetMemPool.
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* ::cudaErrorNotSupported
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cuDeviceGetMemPool, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceSetMemPool
*/
int handle_cudaDeviceGetMemPool(void *conn) {
    cudaMemPool_t memPool;
    int device;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetMemPool(&memPool, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Return NvSciSync attributes that this device can support.
*
* Returns in \p nvSciSyncAttrList, the properties of NvSciSync that
* this CUDA device, \p dev can support. The returned \p nvSciSyncAttrList
* can be used to create an NvSciSync that matches this device's capabilities.
* 
* If NvSciSyncAttrKey_RequiredPerm field in \p nvSciSyncAttrList is
* already set this API will return ::cudaErrorInvalidValue.
* 
* The applications should set \p nvSciSyncAttrList to a valid 
* NvSciSyncAttrList failing which this API will return
* ::cudaErrorInvalidHandle.
* 
* The \p flags controls how applications intends to use
* the NvSciSync created from the \p nvSciSyncAttrList. The valid flags are:
* - ::cudaNvSciSyncAttrSignal, specifies that the applications intends to 
* signal an NvSciSync on this CUDA device.
* - ::cudaNvSciSyncAttrWait, specifies that the applications intends to 
* wait on an NvSciSync on this CUDA device.
*
* At least one of these flags must be set, failing which the API
* returns ::cudaErrorInvalidValue. Both the flags are orthogonal
* to one another: a developer may set both these flags that allows one to
* set both wait and signal specific attributes in the same \p nvSciSyncAttrList.
*
* \param nvSciSyncAttrList     - Return NvSciSync attributes supported.
* \param device                - Valid Cuda Device to get NvSciSync attributes for.
* \param flags                 - flags describing NvSciSync usage.
*
* \return
*
* ::cudaSuccess,
* ::cudaErrorDeviceUninitialized,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidHandle,
* ::cudaErrorInvalidDevice,
* ::cudaErrorNotSupported,
* ::cudaErrorMemoryAllocation
*
* \sa
* ::cudaImportExternalSemaphore,
* ::cudaDestroyExternalSemaphore,
* ::cudaSignalExternalSemaphoresAsync,
* ::cudaWaitExternalSemaphoresAsync
*/
int handle_cudaDeviceGetNvSciSyncAttributes(void *conn) {
    void* nvSciSyncAttrList;
    int device;
    int flags;

    if (rpc_read(conn, &nvSciSyncAttrList, sizeof(void*)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &flags, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetNvSciSyncAttributes(&nvSciSyncAttrList, device, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nvSciSyncAttrList, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Queries attributes of the link between two devices.
*
* Returns in \p *value the value of the requested attribute \p attrib of the
* link between \p srcDevice and \p dstDevice. The supported attributes are:
* - ::cudaDevP2PAttrPerformanceRank: A relative value indicating the
*   performance of the link between two devices. Lower value means better
*   performance (0 being the value used for most performant link).
* - ::cudaDevP2PAttrAccessSupported: 1 if peer access is enabled.
* - ::cudaDevP2PAttrNativeAtomicSupported: 1 if native atomic operations over
*   the link are supported.
* - ::cudaDevP2PAttrCudaArrayAccessSupported: 1 if accessing CUDA arrays over
*   the link is supported.
*
* Returns ::cudaErrorInvalidDevice if \p srcDevice or \p dstDevice are not valid
* or if they represent the same device.
*
* Returns ::cudaErrorInvalidValue if \p attrib is not valid or if \p value is
* a null pointer.
*
* \param value         - Returned value of the requested attribute
* \param attrib        - The requested attribute of the link between \p srcDevice and \p dstDevice.
* \param srcDevice     - The source device of the target link.
* \param dstDevice     - The destination device of the target link.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceEnablePeerAccess,
* ::cudaDeviceDisablePeerAccess,
* ::cudaDeviceCanAccessPeer,
* ::cuDeviceGetP2PAttribute
*/
int handle_cudaDeviceGetP2PAttribute(void *conn) {
    int value;
    enum cudaDeviceP2PAttr attr;
    int srcDevice;
    int dstDevice;

    if (rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaDeviceP2PAttr)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetP2PAttribute(&value, attr, srcDevice, dstDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Select compute-device which best matches criteria
*
* Returns in \p *device the device which has properties that best match
* \p *prop.
*
* \param device - Device with best match
* \param prop   - Desired device properties
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
* ::cudaGetDeviceProperties, 
* ::cudaInitDevice
*/
int handle_cudaChooseDevice(void *conn) {
    int device;
    struct cudaDeviceProp prop;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &prop, sizeof(struct cudaDeviceProp)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaChooseDevice(&device, &prop);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Initialize device to be used for GPU executions
*
* This function will initialize the CUDA Runtime structures and primary context on \p device when called,
* but the context will not be made current to \p device.
*
* When ::cudaInitDeviceFlagsAreValid is set in \p flags, deviceFlags are applied to the requested device.
* The values of deviceFlags match those of the flags parameters in ::cudaSetDeviceFlags. 
* The effect may be verified by ::cudaGetDeviceFlags.
*
* This function will return an error if the device is in ::cudaComputeModeExclusiveProcess
* and is occupied by another process or if the device is in ::cudaComputeModeProhibited.
*
* \param device - Device on which the runtime will initialize itself.
* \param deviceFlags - Parameters for device operation.
* \param flags - Flags for controlling the device initialization.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
* ::cudaChooseDevice, ::cudaSetDevice
* ::cuCtxSetCurrent
*/
int handle_cudaInitDevice(void *conn) {
    int device;
    unsigned int deviceFlags;
    unsigned int flags;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &deviceFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaInitDevice(device, deviceFlags, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set device to be used for GPU executions
*
* Sets \p device as the current device for the calling host thread.
* Valid device id's are 0 to (::cudaGetDeviceCount() - 1).
*
* Any device memory subsequently allocated from this host thread
* using ::cudaMalloc(), ::cudaMallocPitch() or ::cudaMallocArray()
* will be physically resident on \p device.  Any host memory allocated
* from this host thread using ::cudaMallocHost() or ::cudaHostAlloc() 
* or ::cudaHostRegister() will have its lifetime associated  with
* \p device.  Any streams or events created from this host thread will 
* be associated with \p device.  Any kernels launched from this host
* thread using the <<<>>> operator or ::cudaLaunchKernel() will be executed
* on \p device.
*
* This call may be made from any host thread, to any device, and at 
* any time.  This function will do no synchronization with the previous 
* or new device, 
* and should only take significant time when it initializes the runtime's context state.
* This call will bind the primary context of the specified device to the calling thread and all the
* subsequent memory allocations, stream and event creations, and kernel launches
* will be associated with the primary context. 
* This function will also immediately initialize the runtime state on the primary context, 
* and the context will be current on \p device immediately. This function will return an 
* error if the device is in ::cudaComputeModeExclusiveProcess and is occupied by another 
* process or if the device is in ::cudaComputeModeProhibited.
* 
* It is not required to call ::cudaInitDevice before using this function.
* \param device - Device on which the active host thread should execute the
* device code.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorDeviceUnavailable,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
* ::cudaChooseDevice,
* ::cudaInitDevice,
* ::cuCtxSetCurrent
*/
int handle_cudaSetDevice(void *conn) {
    int device;

    if (rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSetDevice(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns which device is currently being used
*
* Returns in \p *device the current device for the calling host thread.
*
* \param device - Returns the device on which the active host thread
* executes the device code.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorDeviceUnavailable,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
* ::cudaChooseDevice,
* ::cuCtxGetCurrent
*/
int handle_cudaGetDevice(void *conn) {
    int device;

    if (rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetDevice(&device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Set a list of devices that can be used for CUDA
*
* Sets a list of devices for CUDA execution in priority order using
* \p device_arr. The parameter \p len specifies the number of elements in the
* list.  CUDA will try devices from the list sequentially until it finds one
* that works.  If this function is not called, or if it is called with a \p len
* of 0, then CUDA will go back to its default behavior of trying devices
* sequentially from a default list containing all of the available CUDA
* devices in the system. If a specified device ID in the list does not exist,
* this function will return ::cudaErrorInvalidDevice. If \p len is not 0 and
* \p device_arr is NULL or if \p len exceeds the number of devices in
* the system, then ::cudaErrorInvalidValue is returned.
*
* \param device_arr - List of devices to try
* \param len        - Number of devices in specified list
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
* ::cudaSetDeviceFlags,
* ::cudaChooseDevice
*/
int handle_cudaSetValidDevices(void *conn) {
    int device_arr;
    int len;

    if (rpc_read(conn, &device_arr, sizeof(int)) < 0 ||
        rpc_read(conn, &len, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSetValidDevices(&device_arr, len);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &device_arr, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets flags to be used for device executions
* 
* Records \p flags as the flags for the current device. If the current device
* has been set and that device has already been initialized, the previous flags
* are overwritten. If the current device has not been initialized, it is
* initialized with the provided flags. If no device has been made current to
* the calling thread, a default device is selected and initialized with the
* provided flags.
* 
* The two LSBs of the \p flags parameter can be used to control how the CPU
* thread interacts with the OS scheduler when waiting for results from the
* device.
*
* - ::cudaDeviceScheduleAuto: The default value if the \p flags parameter is
* zero, uses a heuristic based on the number of active CUDA contexts in the
* process \p C and the number of logical processors in the system \p P. If
* \p C \> \p P, then CUDA will yield to other OS threads when waiting for the
* device, otherwise CUDA will not yield while waiting for results and
* actively spin on the processor. Additionally, on Tegra devices,
* ::cudaDeviceScheduleAuto uses a heuristic based on the power profile of
* the platform and may choose ::cudaDeviceScheduleBlockingSync for low-powered
* devices.
* - ::cudaDeviceScheduleSpin: Instruct CUDA to actively spin when waiting for
* results from the device. This can decrease latency when waiting for the
* device, but may lower the performance of CPU threads if they are performing
* work in parallel with the CUDA thread.
* - ::cudaDeviceScheduleYield: Instruct CUDA to yield its thread when waiting
* for results from the device. This can increase latency when waiting for the
* device, but can increase the performance of CPU threads performing work in
* parallel with the device.
* - ::cudaDeviceScheduleBlockingSync: Instruct CUDA to block the CPU thread 
* on a synchronization primitive when waiting for the device to finish work.
* - ::cudaDeviceBlockingSync: Instruct CUDA to block the CPU thread on a 
* synchronization primitive when waiting for the device to finish work. <br>
* \ref deprecated "Deprecated:" This flag was deprecated as of CUDA 4.0 and
* replaced with ::cudaDeviceScheduleBlockingSync.
* - ::cudaDeviceMapHost: This flag enables allocating pinned
* host memory that is accessible to the device. It is implicit for the
* runtime but may be absent if a context is created using the driver API.
* If this flag is not set, ::cudaHostGetDevicePointer() will always return
* a failure code.
* - ::cudaDeviceLmemResizeToMax: Instruct CUDA to not reduce local memory
* after resizing local memory for a kernel. This can prevent thrashing by
* local memory allocations when launching many kernels with high local
* memory usage at the cost of potentially increased memory usage. <br>
* \ref deprecated "Deprecated:" This flag is deprecated and the behavior enabled          
* by this flag is now the default and cannot be disabled.
*
* \param flags - Parameters for device operation
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceFlags, ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
* ::cudaSetDevice, ::cudaSetValidDevices,
* ::cudaInitDevice,
* ::cudaChooseDevice,
* ::cuDevicePrimaryCtxSetFlags
*/
int handle_cudaSetDeviceFlags(void *conn) {
    unsigned int flags;

    if (rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSetDeviceFlags(flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets the flags for the current device
*
* 
* Returns in \p flags the flags for the current device. If there is a current
* device for the calling thread, the flags for the device are returned. If
* there is no current device, the flags for the first device are returned,
* which may be the default flags.  Compare to the behavior of
* ::cudaSetDeviceFlags.
*
* Typically, the flags returned should match the behavior that will be seen
* if the calling thread uses a device after this call, without any change to
* the flags or current device inbetween by this or another thread.  Note that
* if the device is not initialized, it is possible for another thread to
* change the flags for the current device before it is initialized.
* Additionally, when using exclusive mode, if this thread has not requested a
* specific device, it may use a device other than the first device, contrary
* to the assumption made by this function.
*
* If a context has been created via the driver API and is current to the
* calling thread, the flags for that context are always returned.
*
* Flags returned by this function may specifically include ::cudaDeviceMapHost
* even though it is not accepted by ::cudaSetDeviceFlags because it is
* implicit in runtime API flags.  The reason for this is that the current
* context may have been created via the driver API in which case the flag is
* not implicit and may be unset.
*
* \param flags - Pointer to store the device flags
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDevice, ::cudaGetDeviceProperties,
* ::cudaSetDevice, ::cudaSetDeviceFlags,
* ::cudaInitDevice,
* ::cuCtxGetFlags,
* ::cuDevicePrimaryCtxGetState
*/
int handle_cudaGetDeviceFlags(void *conn) {
    unsigned int flags;

    if (rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetDeviceFlags(&flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Create an asynchronous stream
*
* Creates a new asynchronous stream.
*
* \param pStream - Pointer to new stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreateWithPriority,
* ::cudaStreamCreateWithFlags,
* ::cudaStreamGetPriority,
* ::cudaStreamGetFlags,
* ::cudaStreamQuery,
* ::cudaStreamSynchronize,
* ::cudaStreamWaitEvent,
* ::cudaStreamAddCallback,
* ::cudaStreamDestroy,
* ::cuStreamCreate
*/
int handle_cudaStreamCreate(void *conn) {
    cudaStream_t pStream;

    if (rpc_read(conn, &pStream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamCreate(&pStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pStream, sizeof(cudaStream_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Create an asynchronous stream
*
* Creates a new asynchronous stream.  The \p flags argument determines the 
* behaviors of the stream.  Valid values for \p flags are
* - ::cudaStreamDefault: Default stream creation flag.
* - ::cudaStreamNonBlocking: Specifies that work running in the created 
*   stream may run concurrently with work in stream 0 (the NULL stream), and that
*   the created stream should perform no implicit synchronization with stream 0.
*
* \param pStream - Pointer to new stream identifier
* \param flags   - Parameters for stream creation
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate,
* ::cudaStreamCreateWithPriority,
* ::cudaStreamGetFlags,
* ::cudaStreamQuery,
* ::cudaStreamSynchronize,
* ::cudaStreamWaitEvent,
* ::cudaStreamAddCallback,
* ::cudaStreamDestroy,
* ::cuStreamCreate
*/
int handle_cudaStreamCreateWithFlags(void *conn) {
    cudaStream_t pStream;
    unsigned int flags;

    if (rpc_read(conn, &pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamCreateWithFlags(&pStream, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pStream, sizeof(cudaStream_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Create an asynchronous stream with the specified priority
*
* Creates a stream with the specified priority and returns a handle in \p pStream.
* This API alters the scheduler priority of work in the stream. Work in a higher
* priority stream may preempt work already executing in a low priority stream.
*
* \p priority follows a convention where lower numbers represent higher priorities.
* '0' represents default priority. The range of meaningful numerical priorities can
* be queried using ::cudaDeviceGetStreamPriorityRange. If the specified priority is
* outside the numerical range returned by ::cudaDeviceGetStreamPriorityRange,
* it will automatically be clamped to the lowest or the highest number in the range.
*
* \param pStream  - Pointer to new stream identifier
* \param flags    - Flags for stream creation. See ::cudaStreamCreateWithFlags for a list of valid flags that can be passed
* \param priority - Priority of the stream. Lower numbers represent higher priorities.
*                   See ::cudaDeviceGetStreamPriorityRange for more information about
*                   the meaningful stream priorities that can be passed.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \note Stream priorities are supported only on GPUs
* with compute capability 3.5 or higher.
*
* \note In the current implementation, only compute kernels launched in
* priority streams are affected by the stream's priority. Stream priorities have
* no effect on host-to-device and device-to-host memory operations.
*
* \sa ::cudaStreamCreate,
* ::cudaStreamCreateWithFlags,
* ::cudaDeviceGetStreamPriorityRange,
* ::cudaStreamGetPriority,
* ::cudaStreamQuery,
* ::cudaStreamWaitEvent,
* ::cudaStreamAddCallback,
* ::cudaStreamSynchronize,
* ::cudaStreamDestroy,
* ::cuStreamCreateWithPriority
*/
int handle_cudaStreamCreateWithPriority(void *conn) {
    cudaStream_t pStream;
    unsigned int flags;
    int priority;

    if (rpc_read(conn, &pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &priority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamCreateWithPriority(&pStream, flags, priority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pStream, sizeof(cudaStream_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the priority of a stream
*
* Query the priority of a stream. The priority is returned in in \p priority.
* Note that if the stream was created with a priority outside the meaningful
* numerical range returned by ::cudaDeviceGetStreamPriorityRange,
* this function returns the clamped priority.
* See ::cudaStreamCreateWithPriority for details about priority clamping.
*
* \param hStream    - Handle to the stream to be queried
* \param priority   - Pointer to a signed integer in which the stream's priority is returned
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreateWithPriority,
* ::cudaDeviceGetStreamPriorityRange,
* ::cudaStreamGetFlags,
* ::cuStreamGetPriority
*/
int handle_cudaStreamGetPriority(void *conn) {
    cudaStream_t hStream;
    int priority;

    if (rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &priority, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamGetPriority(hStream, &priority);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &priority, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the flags of a stream
*
* Query the flags of a stream. The flags are returned in \p flags.
* See ::cudaStreamCreateWithFlags for a list of valid flags.
*
* \param hStream - Handle to the stream to be queried
* \param flags   - Pointer to an unsigned integer in which the stream's flags are returned
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreateWithPriority,
* ::cudaStreamCreateWithFlags,
* ::cudaStreamGetPriority,
* ::cuStreamGetFlags
*/
int handle_cudaStreamGetFlags(void *conn) {
    cudaStream_t hStream;
    unsigned int flags;

    if (rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamGetFlags(hStream, &flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the Id of a stream
*
* Query the Id of a stream. The Id is returned in \p streamId.
* The stream handle \p hStream can refer to any of the following:
* <ul>
*   <li>a stream created via any of the CUDA runtime APIs such as ::cudaStreamCreate, 
*   ::cudaStreamCreateWithFlags and ::cudaStreamCreateWithPriority, or their driver 
*   API equivalents such as ::cuStreamCreate or ::cuStreamCreateWithPriority.
*   Passing an invalid handle will result in undefined behavior.</li>
*   <li>any of the special streams such as the NULL stream, ::cudaStreamLegacy 
*   and ::cudaStreamPerThread respectively.  The driver API equivalents of these 
*   are also accepted which are NULL, ::CU_STREAM_LEGACY and ::CU_STREAM_PER_THREAD.</li>
* </ul>
* 
* \param hStream    - Handle to the stream to be queried
* \param streamId   - Pointer to an unsigned long long in which the stream Id is returned
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreateWithPriority,
* ::cudaStreamCreateWithFlags,
* ::cudaStreamGetPriority,
* ::cudaStreamGetFlags,
* ::cuStreamGetId
*/
int handle_cudaStreamGetId(void *conn) {
    cudaStream_t hStream;
    unsigned long long streamId;

    if (rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &streamId, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamGetId(hStream, &streamId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &streamId, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* \brief Resets all persisting lines in cache to normal status.
*
* Resets all persisting lines in cache to normal status.
* Takes effect on function return.
*
* \return
* ::cudaSuccess,
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaCtxResetPersistingL2Cache(void *conn) {
    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaCtxResetPersistingL2Cache();

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies attributes from source stream to destination stream.
*
* Copies attributes from source stream \p src to destination stream \p dst.
* Both streams must have the same context.
*
* \param[out] dst Destination stream
* \param[in] src Source stream
* For attributes see ::cudaStreamAttrID
*
* \return
* ::cudaSuccess,
* ::cudaErrorNotSupported
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaStreamCopyAttributes(void *conn) {
    cudaStream_t dst;
    cudaStream_t src;

    if (rpc_read(conn, &dst, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamCopyAttributes(dst, src);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries stream attribute.
*
* Queries attribute \p attr from \p hStream and stores it in corresponding
* member of \p value_out.
*
* \param[in] hStream
* \param[in] attr
* \param[out] value_out
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaStreamGetAttribute(void *conn) {
    cudaStream_t hStream;
    cudaLaunchAttributeID attr;
    cudaLaunchAttributeValue value_out;

    if (rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_read(conn, &value_out, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamGetAttribute(hStream, attr, &value_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value_out, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets stream attribute.
*
* Sets attribute \p attr on \p hStream from corresponding attribute of
* \p value. The updated attribute will be applied to subsequent work
* submitted to the stream. It will not affect previously submitted work.
*
* \param[out] hStream
* \param[in] attr
* \param[in] value
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaStreamSetAttribute(void *conn) {
    cudaStream_t hStream;
    cudaLaunchAttributeID attr;
    cudaLaunchAttributeValue value;

    if (rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_read(conn, &value, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamSetAttribute(hStream, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys and cleans up an asynchronous stream
*
* Destroys and cleans up the asynchronous stream specified by \p stream.
*
* In case the device is still doing work in the stream \p stream
* when ::cudaStreamDestroy() is called, the function will return immediately 
* and the resources associated with \p stream will be released automatically 
* once the device has completed all work in \p stream.
*
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa ::cudaStreamCreate,
* ::cudaStreamCreateWithFlags,
* ::cudaStreamQuery,
* ::cudaStreamWaitEvent,
* ::cudaStreamSynchronize,
* ::cudaStreamAddCallback,
* ::cuStreamDestroy
*/
int handle_cudaStreamDestroy(void *conn) {
    cudaStream_t stream;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamDestroy(stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Make a compute stream wait on an event
*
* Makes all future work submitted to \p stream wait for all work captured in
* \p event.  See ::cudaEventRecord() for details on what is captured by an event.
* The synchronization will be performed efficiently on the device when applicable.
* \p event may be from a different device than \p stream.
*
* flags include:
* - ::cudaEventWaitDefault: Default event creation flag.
* - ::cudaEventWaitExternal: Event is captured in the graph as an external
*   event node when performing stream capture.
*
* \param stream - Stream to wait
* \param event  - Event to wait on
* \param flags  - Parameters for the operation(See above)
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy,
* ::cuStreamWaitEvent
*/
int handle_cudaStreamWaitEvent(void *conn) {
    cudaStream_t stream;
    cudaEvent_t event;
    unsigned int flags = 0;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamWaitEvent(stream, event, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Add a callback to a compute stream
*
* \note This function is slated for eventual deprecation and removal. If
* you do not require the callback to execute in case of a device error,
* consider using ::cudaLaunchHostFunc. Additionally, this function is not
* supported with ::cudaStreamBeginCapture and ::cudaStreamEndCapture, unlike
* ::cudaLaunchHostFunc.
*
* Adds a callback to be called on the host after all currently enqueued
* items in the stream have completed.  For each 
* cudaStreamAddCallback call, a callback will be executed exactly once.
* The callback will block later work in the stream until it is finished.
*
* The callback may be passed ::cudaSuccess or an error code.  In the event
* of a device error, all subsequently executed callbacks will receive an
* appropriate ::cudaError_t.
*
* Callbacks must not make any CUDA API calls.  Attempting to use CUDA APIs
* may result in ::cudaErrorNotPermitted.  Callbacks must not perform any
* synchronization that may depend on outstanding device work or other callbacks
* that are not mandated to run earlier.  Callbacks without a mandated order
* (in independent streams) execute in undefined order and may be serialized.
*
* For the purposes of Unified Memory, callback execution makes a number of
* guarantees:
* <ul>
*   <li>The callback stream is considered idle for the duration of the
*   callback.  Thus, for example, a callback may always use memory attached
*   to the callback stream.</li>
*   <li>The start of execution of a callback has the same effect as
*   synchronizing an event recorded in the same stream immediately prior to
*   the callback.  It thus synchronizes streams which have been "joined"
*   prior to the callback.</li>
*   <li>Adding device work to any stream does not have the effect of making
*   the stream active until all preceding callbacks have executed.  Thus, for
*   example, a callback might use global attached memory even if work has
*   been added to another stream, if it has been properly ordered with an
*   event.</li>
*   <li>Completion of a callback does not cause a stream to become
*   active except as described above.  The callback stream will remain idle
*   if no device work follows the callback, and will remain idle across
*   consecutive callbacks without device work in between.  Thus, for example,
*   stream synchronization can be done by signaling from a callback at the
*   end of the stream.</li>
* </ul>
*
* \param stream   - Stream to add callback to
* \param callback - The function to call once preceding stream operations are complete
* \param userData - User specified data to be passed to the callback function
* \param flags    - Reserved for future use, must be 0
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamWaitEvent, ::cudaStreamDestroy, ::cudaMallocManaged, ::cudaStreamAttachMemAsync,
* ::cudaLaunchHostFunc, ::cuStreamAddCallback
*/
int handle_cudaStreamAddCallback(void *conn) {
    cudaStream_t stream;
    cudaStreamCallback_t callback;
    void* userData;
    unsigned int flags;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &callback, sizeof(cudaStreamCallback_t)) < 0 ||
        rpc_read(conn, &userData, sizeof(void*)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamAddCallback(stream, callback, &userData, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &userData, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Waits for stream tasks to complete
*
* Blocks until \p stream has completed all operations. If the
* ::cudaDeviceScheduleBlockingSync flag was set for this device, 
* the host thread will block until the stream is finished with 
* all of its tasks.
*
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamWaitEvent, ::cudaStreamAddCallback, ::cudaStreamDestroy,
* ::cuStreamSynchronize
*/
int handle_cudaStreamSynchronize(void *conn) {
    cudaStream_t stream;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamSynchronize(stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries an asynchronous stream for completion status
*
* Returns ::cudaSuccess if all operations in \p stream have
* completed, or ::cudaErrorNotReady if not.
*
* For the purposes of Unified Memory, a return value of ::cudaSuccess
* is equivalent to having called ::cudaStreamSynchronize().
*
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorNotReady,
* ::cudaErrorInvalidResourceHandle
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy,
* ::cuStreamQuery
*/
int handle_cudaStreamQuery(void *conn) {
    cudaStream_t stream;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamQuery(stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaStreamAttachMemAsync(void *conn) {
    cudaStream_t stream;
    void* devPtr;
    size_t length = 0;
    unsigned int flags = 0x04;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &length, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamAttachMemAsync(stream, &devPtr, length, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Begins graph capture on a stream
*
* Begin graph capture on \p stream. When a stream is in capture mode, all operations
* pushed into the stream will not be executed, but will instead be captured into
* a graph, which will be returned via ::cudaStreamEndCapture. Capture may not be initiated
* if \p stream is ::cudaStreamLegacy. Capture must be ended on the same stream in which
* it was initiated, and it may only be initiated if the stream is not already in capture
* mode. The capture mode may be queried via ::cudaStreamIsCapturing. A unique id
* representing the capture sequence may be queried via ::cudaStreamGetCaptureInfo.
*
* If \p mode is not ::cudaStreamCaptureModeRelaxed, ::cudaStreamEndCapture must be
* called on this stream from the same thread.
*
* \note Kernels captured using this API must not use texture and surface references.
*       Reading or writing through any texture or surface reference is undefined
*       behavior. This restriction does not apply to texture and surface objects.
*
* \param stream - Stream in which to initiate capture
* \param mode    - Controls the interaction of this capture sequence with other API
*                  calls that are potentially unsafe. For more details see
*                  ::cudaThreadExchangeStreamCaptureMode.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
*
* \sa
* ::cudaStreamCreate,
* ::cudaStreamIsCapturing,
* ::cudaStreamEndCapture,
* ::cudaThreadExchangeStreamCaptureMode
*/
int handle_cudaStreamBeginCapture(void *conn) {
    cudaStream_t stream;
    enum cudaStreamCaptureMode mode;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &mode, sizeof(enum cudaStreamCaptureMode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamBeginCapture(stream, mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Swaps the stream capture interaction mode for a thread
*
* Sets the calling thread's stream capture interaction mode to the value contained
* in \p *mode, and overwrites \p *mode with the previous mode for the thread. To
* facilitate deterministic behavior across function or module boundaries, callers
* are encouraged to use this API in a push-pop fashion: \code
     cudaStreamCaptureMode mode = desiredMode;
     cudaThreadExchangeStreamCaptureMode(&mode);
     ...
     cudaThreadExchangeStreamCaptureMode(&mode); // restore previous mode
* \endcode
*
* During stream capture (see ::cudaStreamBeginCapture), some actions, such as a call
* to ::cudaMalloc, may be unsafe. In the case of ::cudaMalloc, the operation is
* not enqueued asynchronously to a stream, and is not observed by stream capture.
* Therefore, if the sequence of operations captured via ::cudaStreamBeginCapture
* depended on the allocation being replayed whenever the graph is launched, the
* captured graph would be invalid.
*
* Therefore, stream capture places restrictions on API calls that can be made within
* or concurrently to a ::cudaStreamBeginCapture-::cudaStreamEndCapture sequence. This
* behavior can be controlled via this API and flags to ::cudaStreamBeginCapture.
*
* A thread's mode is one of the following:
* - \p cudaStreamCaptureModeGlobal: This is the default mode. If the local thread has
*   an ongoing capture sequence that was not initiated with
*   \p cudaStreamCaptureModeRelaxed at \p cuStreamBeginCapture, or if any other thread
*   has a concurrent capture sequence initiated with \p cudaStreamCaptureModeGlobal,
*   this thread is prohibited from potentially unsafe API calls.
* - \p cudaStreamCaptureModeThreadLocal: If the local thread has an ongoing capture
*   sequence not initiated with \p cudaStreamCaptureModeRelaxed, it is prohibited
*   from potentially unsafe API calls. Concurrent capture sequences in other threads
*   are ignored.
* - \p cudaStreamCaptureModeRelaxed: The local thread is not prohibited from potentially
*   unsafe API calls. Note that the thread is still prohibited from API calls which
*   necessarily conflict with stream capture, for example, attempting ::cudaEventQuery
*   on an event that was last recorded inside a capture sequence.
*
* \param mode - Pointer to mode value to swap with the current mode
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
*
* \sa
* ::cudaStreamBeginCapture
*/
int handle_cudaThreadExchangeStreamCaptureMode(void *conn) {
    enum cudaStreamCaptureMode mode;

    if (rpc_read(conn, &mode, sizeof(enum cudaStreamCaptureMode)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaThreadExchangeStreamCaptureMode(&mode);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mode, sizeof(enum cudaStreamCaptureMode)) < 0)
        return -1;

    return result;
}

/**
* \brief Ends capture on a stream, returning the captured graph
*
* End capture on \p stream, returning the captured graph via \p pGraph.
* Capture must have been initiated on \p stream via a call to ::cudaStreamBeginCapture.
* If capture was invalidated, due to a violation of the rules of stream capture, then
* a NULL graph will be returned.
*
* If the \p mode argument to ::cudaStreamBeginCapture was not
* ::cudaStreamCaptureModeRelaxed, this call must be from the same thread as
* ::cudaStreamBeginCapture.
*
* \param stream - Stream to query
* \param pGraph - The captured graph
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorStreamCaptureWrongThread
* \notefnerr
*
* \sa
* ::cudaStreamCreate,
* ::cudaStreamBeginCapture,
* ::cudaStreamIsCapturing
*/
int handle_cudaStreamEndCapture(void *conn) {
    cudaStream_t stream;
    cudaGraph_t pGraph;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &pGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamEndCapture(stream, &pGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a stream's capture status
*
* Return the capture status of \p stream via \p pCaptureStatus. After a successful
* call, \p *pCaptureStatus will contain one of the following:
* - ::cudaStreamCaptureStatusNone: The stream is not capturing.
* - ::cudaStreamCaptureStatusActive: The stream is capturing.
* - ::cudaStreamCaptureStatusInvalidated: The stream was capturing but an error
*   has invalidated the capture sequence. The capture sequence must be terminated
*   with ::cudaStreamEndCapture on the stream where it was initiated in order to
*   continue using \p stream.
*
* Note that, if this is called on ::cudaStreamLegacy (the "null stream") while
* a blocking stream on the same device is capturing, it will return
* ::cudaErrorStreamCaptureImplicit and \p *pCaptureStatus is unspecified
* after the call. The blocking stream capture is not invalidated.
*
* When a blocking stream is capturing, the legacy stream is in an
* unusable state until the blocking stream capture is terminated. The legacy
* stream is not supported for stream capture, but attempted use would have an
* implicit dependency on the capturing stream(s).
*
* \param stream         - Stream to query
* \param pCaptureStatus - Returns the stream's capture status
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorStreamCaptureImplicit
* \notefnerr
*
* \sa
* ::cudaStreamCreate,
* ::cudaStreamBeginCapture,
* ::cudaStreamEndCapture
*/
int handle_cudaStreamIsCapturing(void *conn) {
    cudaStream_t stream;
    enum cudaStreamCaptureStatus pCaptureStatus;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &pCaptureStatus, sizeof(enum cudaStreamCaptureStatus)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamIsCapturing(stream, &pCaptureStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pCaptureStatus, sizeof(enum cudaStreamCaptureStatus)) < 0)
        return -1;

    return result;
}

/**
* \brief Query a stream's capture state
*
* Query stream state related to stream capture.
*
* If called on ::cudaStreamLegacy (the "null stream") while a stream not created 
* with ::cudaStreamNonBlocking is capturing, returns ::cudaErrorStreamCaptureImplicit.
*
* Valid data (other than capture status) is returned only if both of the following are true:
* - the call returns cudaSuccess
* - the returned capture status is ::cudaStreamCaptureStatusActive
*
* \param stream - The stream to query
* \param captureStatus_out - Location to return the capture status of the stream; required
* \param id_out - Optional location to return an id for the capture sequence, which is
*           unique over the lifetime of the process
* \param graph_out - Optional location to return the graph being captured into. All
*           operations other than destroy and node removal are permitted on the graph
*           while the capture sequence is in progress. This API does not transfer
*           ownership of the graph, which is transferred or destroyed at
*           ::cudaStreamEndCapture. Note that the graph handle may be invalidated before
*           end of capture for certain errors. Nodes that are or become
*           unreachable from the original stream at ::cudaStreamEndCapture due to direct
*           actions on the graph do not trigger ::cudaErrorStreamCaptureUnjoined.
* \param dependencies_out - Optional location to store a pointer to an array of nodes.
*           The next node to be captured in the stream will depend on this set of nodes,
*           absent operations such as event wait which modify this set. The array pointer
*           is valid until the next API call which operates on the stream or until end of
*           capture. The node handles may be copied out and are valid until they or the
*           graph is destroyed. The driver-owned array may also be passed directly to
*           APIs that operate on the graph (not the stream) without copying.
* \param numDependencies_out - Optional location to store the size of the array
*           returned in dependencies_out.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorStreamCaptureImplicit
* \note_graph_thread_safety
* \notefnerr
*
* \sa
* ::cudaStreamBeginCapture,
* ::cudaStreamIsCapturing,
* ::cudaStreamUpdateCaptureDependencies
*/
int handle_cudaStreamGetCaptureInfo_v2(void *conn) {
    cudaStream_t stream;
    enum cudaStreamCaptureStatus captureStatus_out;
    unsigned long long id_out;
    cudaGraph_t graph_out;
    const cudaGraphNode_t* dependencies_out;
    size_t numDependencies_out;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &captureStatus_out, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_read(conn, &id_out, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &graph_out, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &dependencies_out, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_read(conn, &numDependencies_out, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamGetCaptureInfo_v2(stream, &captureStatus_out, &id_out, &graph_out, &dependencies_out, &numDependencies_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &captureStatus_out, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_write(conn, &id_out, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &graph_out, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &dependencies_out, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &numDependencies_out, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Update the set of dependencies in a capturing stream (11.3+)
*
* Modifies the dependency set of a capturing stream. The dependency set is the set
* of nodes that the next captured node in the stream will depend on.
*
* Valid flags are ::cudaStreamAddCaptureDependencies and
* ::cudaStreamSetCaptureDependencies. These control whether the set passed to
* the API is added to the existing set or replaces it. A flags value of 0 defaults
* to ::cudaStreamAddCaptureDependencies.
*
* Nodes that are removed from the dependency set via this API do not result in
* ::cudaErrorStreamCaptureUnjoined if they are unreachable from the stream at
* ::cudaStreamEndCapture.
*
* Returns ::cudaErrorIllegalState if the stream is not capturing.
*
* This API is new in CUDA 11.3. Developers requiring compatibility across minor
* versions of the CUDA driver to 11.0 should not use this API or provide a fallback.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorIllegalState
* \notefnerr
*
* \sa
* ::cudaStreamBeginCapture,
* ::cudaStreamGetCaptureInfo,
*/
int handle_cudaStreamUpdateCaptureDependencies(void *conn) {
    cudaStream_t stream;
    cudaGraphNode_t dependencies;
    size_t numDependencies;
    unsigned int flags = 0;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &dependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaStreamUpdateCaptureDependencies(stream, &dependencies, numDependencies, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dependencies, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an event object
*
* Creates an event object for the current device using ::cudaEventDefault.
*
* \param event - Newly created event
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorLaunchFailure,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*, unsigned int) "cudaEventCreate (C++ API)",
* ::cudaEventCreateWithFlags, ::cudaEventRecord, ::cudaEventQuery,
* ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
* ::cudaStreamWaitEvent,
* ::cuEventCreate
*/
int handle_cudaEventCreate(void *conn) {
    cudaEvent_t event;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventCreate(&event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an event object with the specified flags
*
* Creates an event object for the current device with the specified flags. Valid
* flags include:
* - ::cudaEventDefault: Default event creation flag.
* - ::cudaEventBlockingSync: Specifies that event should use blocking
*   synchronization. A host thread that uses ::cudaEventSynchronize() to wait
*   on an event created with this flag will block until the event actually
*   completes.
* - ::cudaEventDisableTiming: Specifies that the created event does not need
*   to record timing data.  Events created with this flag specified and
*   the ::cudaEventBlockingSync flag not specified will provide the best
*   performance when used with ::cudaStreamWaitEvent() and ::cudaEventQuery().
* - ::cudaEventInterprocess: Specifies that the created event may be used as an
*   interprocess event by ::cudaIpcGetEventHandle(). ::cudaEventInterprocess must
*   be specified along with ::cudaEventDisableTiming.
*
* \param event - Newly created event
* \param flags - Flags for new event
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorLaunchFailure,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
* ::cudaStreamWaitEvent,
* ::cuEventCreate
*/
int handle_cudaEventCreateWithFlags(void *conn) {
    cudaEvent_t event;
    unsigned int flags;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventCreateWithFlags(&event, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Records an event
*
* Captures in \p event the contents of \p stream at the time of this call.
* \p event and \p stream must be on the same CUDA context.
* Calls such as ::cudaEventQuery() or ::cudaStreamWaitEvent() will then
* examine or wait for completion of the work that was captured. Uses of
* \p stream after this call do not modify \p event. See note on default
* stream behavior for what is captured in the default case.
*
* ::cudaEventRecord() can be called multiple times on the same event and
* will overwrite the previously captured state. Other APIs such as
* ::cudaStreamWaitEvent() use the most recently captured state at the time
* of the API call, and are not affected by later calls to
* ::cudaEventRecord(). Before the first call to ::cudaEventRecord(), an
* event represents an empty set of work, so for example ::cudaEventQuery()
* would return ::cudaSuccess.
*
* \param event  - Event to record
* \param stream - Stream in which to record event
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorLaunchFailure
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventCreateWithFlags, ::cudaEventQuery,
* ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
* ::cudaStreamWaitEvent,
* ::cudaEventRecordWithFlags,
* ::cuEventRecord
*/
int handle_cudaEventRecord(void *conn) {
    cudaEvent_t event;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventRecord(event, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaEventRecordWithFlags(void *conn) {
    cudaEvent_t event;
    cudaStream_t stream = 0;
    unsigned int flags = 0;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventRecordWithFlags(event, stream, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries an event's status
*
* Queries the status of all work currently captured by \p event. See
* ::cudaEventRecord() for details on what is captured by an event.
*
* Returns ::cudaSuccess if all captured work has been completed, or
* ::cudaErrorNotReady if any captured work is incomplete.
*
* For the purposes of Unified Memory, a return value of ::cudaSuccess
* is equivalent to having called ::cudaEventSynchronize().
*
* \param event - Event to query
*
* \return
* ::cudaSuccess,
* ::cudaErrorNotReady,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorLaunchFailure
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventCreateWithFlags, ::cudaEventRecord,
* ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
* ::cuEventQuery
*/
int handle_cudaEventQuery(void *conn) {
    cudaEvent_t event;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventQuery(event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Waits for an event to complete
*
* Waits until the completion of all work currently captured in \p event.
* See ::cudaEventRecord() for details on what is captured by an event.
*
* Waiting for an event that was created with the ::cudaEventBlockingSync
* flag will cause the calling CPU thread to block until the event has
* been completed by the device.  If the ::cudaEventBlockingSync flag has
* not been set, then the CPU thread will busy-wait until the event has
* been completed by the device.
*
* \param event - Event to wait for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorLaunchFailure
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventCreateWithFlags, ::cudaEventRecord,
* ::cudaEventQuery, ::cudaEventDestroy, ::cudaEventElapsedTime,
* ::cuEventSynchronize
*/
int handle_cudaEventSynchronize(void *conn) {
    cudaEvent_t event;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventSynchronize(event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an event object
*
* Destroys the event specified by \p event.
*
* An event may be destroyed before it is complete (i.e., while
* ::cudaEventQuery() would return ::cudaErrorNotReady). In this case, the
* call does not block on completion of the event, and any associated
* resources will automatically be released asynchronously at completion.
*
* \param event - Event to destroy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorLaunchFailure
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventCreateWithFlags, ::cudaEventQuery,
* ::cudaEventSynchronize, ::cudaEventRecord, ::cudaEventElapsedTime,
* ::cuEventDestroy
*/
int handle_cudaEventDestroy(void *conn) {
    cudaEvent_t event;

    if (rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventDestroy(event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Computes the elapsed time between events
*
* Computes the elapsed time between two events (in milliseconds with a
* resolution of around 0.5 microseconds).
*
* If either event was last recorded in a non-NULL stream, the resulting time
* may be greater than expected (even if both used the same stream handle). This
* happens because the ::cudaEventRecord() operation takes place asynchronously
* and there is no guarantee that the measured latency is actually just between
* the two events. Any number of other different stream operations could execute
* in between the two measured events, thus altering the timing in a significant
* way.
*
* If ::cudaEventRecord() has not been called on either event, then
* ::cudaErrorInvalidResourceHandle is returned. If ::cudaEventRecord() has been
* called on both events but one or both of them has not yet been completed
* (that is, ::cudaEventQuery() would return ::cudaErrorNotReady on at least one
* of the events), ::cudaErrorNotReady is returned. If either event was created
* with the ::cudaEventDisableTiming flag, then this function will return
* ::cudaErrorInvalidResourceHandle.
*
* \param ms    - Time between \p start and \p end in ms
* \param start - Starting event
* \param end   - Ending event
*
* \return
* ::cudaSuccess,
* ::cudaErrorNotReady,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorLaunchFailure,
* ::cudaErrorUnknown
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
* ::cudaEventCreateWithFlags, ::cudaEventQuery,
* ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventRecord,
* ::cuEventElapsedTime
*/
int handle_cudaEventElapsedTime(void *conn) {
    float ms;
    cudaEvent_t start;
    cudaEvent_t end;

    if (rpc_read(conn, &ms, sizeof(float)) < 0 ||
        rpc_read(conn, &start, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &end, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaEventElapsedTime(&ms, start, end);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ms, sizeof(float)) < 0)
        return -1;

    return result;
}

/**
* \brief Imports an external memory object
*
* Imports an externally allocated memory object and returns
* a handle to that in \p extMem_out.
*
* The properties of the handle being imported must be described in
* \p memHandleDesc. The ::cudaExternalMemoryHandleDesc structure
* is defined as follows:
*
* \code
        typedef struct cudaExternalMemoryHandleDesc_st {
            cudaExternalMemoryHandleType type;
            union {
                int fd;
                struct {
                    void *handle;
                    const void *name;
                } win32;
                const void *nvSciBufObject;
            } handle;
            unsigned long long size;
            unsigned int flags;
        } cudaExternalMemoryHandleDesc;
* \endcode
*
* where ::cudaExternalMemoryHandleDesc::type specifies the type
* of handle being imported. ::cudaExternalMemoryHandleType is
* defined as:
*
* \code
        typedef enum cudaExternalMemoryHandleType_enum {
            cudaExternalMemoryHandleTypeOpaqueFd         = 1,
            cudaExternalMemoryHandleTypeOpaqueWin32      = 2,
            cudaExternalMemoryHandleTypeOpaqueWin32Kmt   = 3,
            cudaExternalMemoryHandleTypeD3D12Heap        = 4,
            cudaExternalMemoryHandleTypeD3D12Resource    = 5,
	        cudaExternalMemoryHandleTypeD3D11Resource    = 6,
		    cudaExternalMemoryHandleTypeD3D11ResourceKmt = 7,
            cudaExternalMemoryHandleTypeNvSciBuf         = 8
        } cudaExternalMemoryHandleType;
* \endcode
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeOpaqueFd, then
* ::cudaExternalMemoryHandleDesc::handle::fd must be a valid
* file descriptor referencing a memory object. Ownership of
* the file descriptor is transferred to the CUDA driver when the
* handle is imported successfully. Performing any operations on the
* file descriptor after it is imported results in undefined behavior.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeOpaqueWin32, then exactly one
* of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
* ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a memory object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::cudaExternalMemoryHandleDesc::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a memory object.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeOpaqueWin32Kmt, then
* ::cudaExternalMemoryHandleDesc::handle::win32::handle must
* be non-NULL and
* ::cudaExternalMemoryHandleDesc::handle::win32::name
* must be NULL. The handle specified must be a globally shared KMT
* handle. This handle does not hold a reference to the underlying
* object, and thus will be invalid when all references to the
* memory object are destroyed.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeD3D12Heap, then exactly one
* of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
* ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Heap object. This handle holds a reference to the underlying
* object. If ::cudaExternalMemoryHandleDesc::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D12Heap object.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeD3D12Resource, then exactly one
* of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
* ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Resource object. This handle holds a reference to the
* underlying object. If
* ::cudaExternalMemoryHandleDesc::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D12Resource object.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeD3D11Resource,then exactly one
* of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
* ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle is    
* not NULL, then it must represent a valid shared NT handle that is  
* returned by  IDXGIResource1::CreateSharedHandle when referring to a 
* ID3D11Resource object. If
* ::cudaExternalMemoryHandleDesc::handle::win32::name
* is not NULL, then it must point to a NULL-terminated array of
* UTF-16 characters that refers to a ID3D11Resource object.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeD3D11ResourceKmt, then
* ::cudaExternalMemoryHandleDesc::handle::win32::handle must
* be non-NULL and ::cudaExternalMemoryHandleDesc::handle::win32::name
* must be NULL. The handle specified must be a valid shared KMT
* handle that is returned by IDXGIResource::GetSharedHandle when
* referring to a ID3D11Resource object.
*
* If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeNvSciBuf, then
* ::cudaExternalMemoryHandleDesc::handle::nvSciBufObject must be NON-NULL
* and reference a valid NvSciBuf object.
* If the NvSciBuf object imported into CUDA is also mapped by other drivers, then the
* application must use ::cudaWaitExternalSemaphoresAsync or ::cudaSignalExternalSemaphoresAsync
* as approprriate barriers to maintain coherence between CUDA and the other drivers.
* See ::cudaExternalSemaphoreWaitSkipNvSciBufMemSync and ::cudaExternalSemaphoreSignalSkipNvSciBufMemSync 
* for memory synchronization.
*
* The size of the memory object must be specified in
* ::cudaExternalMemoryHandleDesc::size.
*
* Specifying the flag ::cudaExternalMemoryDedicated in
* ::cudaExternalMemoryHandleDesc::flags indicates that the
* resource is a dedicated resource. The definition of what a
* dedicated resource is outside the scope of this extension.
* This flag must be set if ::cudaExternalMemoryHandleDesc::type
* is one of the following:
* ::cudaExternalMemoryHandleTypeD3D12Resource
* ::cudaExternalMemoryHandleTypeD3D11Resource
* ::cudaExternalMemoryHandleTypeD3D11ResourceKmt
*
* \param extMem_out    - Returned handle to an external memory object
* \param memHandleDesc - Memory import handle descriptor
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorOperatingSystem
* \notefnerr
* \note_init_rt
* \note_callback
*
* \note If the Vulkan memory imported into CUDA is mapped on the CPU then the
* application must use vkInvalidateMappedMemoryRanges/vkFlushMappedMemoryRanges
* as well as appropriate Vulkan pipeline barriers to maintain coherence between
* CPU and GPU. For more information on these APIs, please refer to "Synchronization
* and Cache Control" chapter from Vulkan specification.
*
*
* \sa ::cudaDestroyExternalMemory,
* ::cudaExternalMemoryGetMappedBuffer,
* ::cudaExternalMemoryGetMappedMipmappedArray
*/
int handle_cudaImportExternalMemory(void *conn) {
    cudaExternalMemory_t extMem_out;
    struct cudaExternalMemoryHandleDesc memHandleDesc;

    if (rpc_read(conn, &extMem_out, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_read(conn, &memHandleDesc, sizeof(struct cudaExternalMemoryHandleDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaImportExternalMemory(&extMem_out, &memHandleDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &extMem_out, sizeof(cudaExternalMemory_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Maps a buffer onto an imported memory object
*
* Maps a buffer onto an imported memory object and returns a device
* pointer in \p devPtr.
*
* The properties of the buffer being mapped must be described in
* \p bufferDesc. The ::cudaExternalMemoryBufferDesc structure is
* defined as follows:
*
* \code
        typedef struct cudaExternalMemoryBufferDesc_st {
            unsigned long long offset;
            unsigned long long size;
            unsigned int flags;
        } cudaExternalMemoryBufferDesc;
* \endcode
*
* where ::cudaExternalMemoryBufferDesc::offset is the offset in
* the memory object where the buffer's base address is.
* ::cudaExternalMemoryBufferDesc::size is the size of the buffer.
* ::cudaExternalMemoryBufferDesc::flags must be zero.
*
* The offset and size have to be suitably aligned to match the
* requirements of the external API. Mapping two buffers whose ranges
* overlap may or may not result in the same virtual address being
* returned for the overlapped portion. In such cases, the application
* must ensure that all accesses to that region from the GPU are
* volatile. Otherwise writes made via one address are not guaranteed
* to be visible via the other address, even if they're issued by the
* same thread. It is recommended that applications map the combined
* range instead of mapping separate buffers and then apply the
* appropriate offsets to the returned pointer to derive the
* individual buffers.
*
* The returned pointer \p devPtr must be freed using ::cudaFree.
*
* \param devPtr     - Returned device pointer to buffer
* \param extMem     - Handle to external memory object
* \param bufferDesc - Buffer descriptor
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaImportExternalMemory,
* ::cudaDestroyExternalMemory,
* ::cudaExternalMemoryGetMappedMipmappedArray
*/
int handle_cudaExternalMemoryGetMappedBuffer(void *conn) {
    void* devPtr;
    cudaExternalMemory_t extMem;
    struct cudaExternalMemoryBufferDesc bufferDesc;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_read(conn, &bufferDesc, sizeof(struct cudaExternalMemoryBufferDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaExternalMemoryGetMappedBuffer(&devPtr, extMem, &bufferDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Maps a CUDA mipmapped array onto an external memory object
*
* Maps a CUDA mipmapped array onto an external object and returns a
* handle to it in \p mipmap.
*
* The properties of the CUDA mipmapped array being mapped must be
* described in \p mipmapDesc. The structure
* ::cudaExternalMemoryMipmappedArrayDesc is defined as follows:
*
* \code
        typedef struct cudaExternalMemoryMipmappedArrayDesc_st {
            unsigned long long offset;
            cudaChannelFormatDesc formatDesc;
            cudaExtent extent;
            unsigned int flags;
            unsigned int numLevels;
        } cudaExternalMemoryMipmappedArrayDesc;
* \endcode
*
* where ::cudaExternalMemoryMipmappedArrayDesc::offset is the
* offset in the memory object where the base level of the mipmap
* chain is.
* ::cudaExternalMemoryMipmappedArrayDesc::formatDesc describes the
* format of the data.
* ::cudaExternalMemoryMipmappedArrayDesc::extent specifies the
* dimensions of the base level of the mipmap chain.
* ::cudaExternalMemoryMipmappedArrayDesc::flags are flags associated
* with CUDA mipmapped arrays. For further details, please refer to
* the documentation for ::cudaMalloc3DArray. Note that if the mipmapped
* array is bound as a color target in the graphics API, then the flag
* ::cudaArrayColorAttachment must be specified in 
* ::cudaExternalMemoryMipmappedArrayDesc::flags.
* ::cudaExternalMemoryMipmappedArrayDesc::numLevels specifies
* the total number of levels in the mipmap chain.
*
* The returned CUDA mipmapped array must be freed using ::cudaFreeMipmappedArray.
*
* \param mipmap     - Returned CUDA mipmapped array
* \param extMem     - Handle to external memory object
* \param mipmapDesc - CUDA array descriptor
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaImportExternalMemory,
* ::cudaDestroyExternalMemory,
* ::cudaExternalMemoryGetMappedBuffer
*
* \note If ::cudaExternalMemoryHandleDesc::type is
* ::cudaExternalMemoryHandleTypeNvSciBuf, then
* ::cudaExternalMemoryMipmappedArrayDesc::numLevels must not be greater than 1.
*/
int handle_cudaExternalMemoryGetMappedMipmappedArray(void *conn) {
    cudaMipmappedArray_t mipmap;
    cudaExternalMemory_t extMem;
    struct cudaExternalMemoryMipmappedArrayDesc mipmapDesc;

    if (rpc_read(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_read(conn, &mipmapDesc, sizeof(struct cudaExternalMemoryMipmappedArrayDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &mipmapDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an external memory object.
*
* Destroys the specified external memory object. Any existing buffers
* and CUDA mipmapped arrays mapped onto this object must no longer be
* used and must be explicitly freed using ::cudaFree and
* ::cudaFreeMipmappedArray respectively.
*
* \param extMem - External memory object to be destroyed
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa ::cudaImportExternalMemory,
* ::cudaExternalMemoryGetMappedBuffer,
* ::cudaExternalMemoryGetMappedMipmappedArray
*/
int handle_cudaDestroyExternalMemory(void *conn) {
    cudaExternalMemory_t extMem;

    if (rpc_read(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDestroyExternalMemory(extMem);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Imports an external semaphore
*
* Imports an externally allocated synchronization object and returns
* a handle to that in \p extSem_out.
*
* The properties of the handle being imported must be described in
* \p semHandleDesc. The ::cudaExternalSemaphoreHandleDesc is defined
* as follows:
*
* \code
        typedef struct cudaExternalSemaphoreHandleDesc_st {
            cudaExternalSemaphoreHandleType type;
            union {
                int fd;
                struct {
                    void *handle;
                    const void *name;
                } win32;
                const void* NvSciSyncObj;
            } handle;
            unsigned int flags;
        } cudaExternalSemaphoreHandleDesc;
* \endcode
*
* where ::cudaExternalSemaphoreHandleDesc::type specifies the type of
* handle being imported. ::cudaExternalSemaphoreHandleType is defined
* as:
*
* \code
        typedef enum cudaExternalSemaphoreHandleType_enum {
            cudaExternalSemaphoreHandleTypeOpaqueFd                = 1,
            cudaExternalSemaphoreHandleTypeOpaqueWin32             = 2,
            cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt          = 3,
            cudaExternalSemaphoreHandleTypeD3D12Fence              = 4,
            cudaExternalSemaphoreHandleTypeD3D11Fence              = 5,
            cudaExternalSemaphoreHandleTypeNvSciSync               = 6,
            cudaExternalSemaphoreHandleTypeKeyedMutex              = 7,
            cudaExternalSemaphoreHandleTypeKeyedMutexKmt           = 8,
            cudaExternalSemaphoreHandleTypeTimelineSemaphoreFd     = 9,
            cudaExternalSemaphoreHandleTypeTimelineSemaphoreWin32  = 10
        } cudaExternalSemaphoreHandleType;
* \endcode
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeOpaqueFd, then
* ::cudaExternalSemaphoreHandleDesc::handle::fd must be a valid file
* descriptor referencing a synchronization object. Ownership of the
* file descriptor is transferred to the CUDA driver when the handle
* is imported successfully. Performing any operations on the file
* descriptor after it is imported results in undefined behavior.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32, then exactly one of
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a synchronization object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::cudaExternalSemaphoreHandleDesc::handle::win32::name is
* not NULL, then it must name a valid synchronization object.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt, then
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle must be
* non-NULL and ::cudaExternalSemaphoreHandleDesc::handle::win32::name
* must be NULL. The handle specified must be a globally shared KMT
* handle. This handle does not hold a reference to the underlying
* object, and thus will be invalid when all references to the
* synchronization object are destroyed.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeD3D12Fence, then exactly one of
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D12Device::CreateSharedHandle when referring to a
* ID3D12Fence object. This handle holds a reference to the underlying
* object. If ::cudaExternalSemaphoreHandleDesc::handle::win32::name
* is not NULL, then it must name a valid synchronization object that
* refers to a valid ID3D12Fence object.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeD3D11Fence, then exactly one of
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* is returned by ID3D11Fence::CreateSharedHandle. If 
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name
* is not NULL, then it must name a valid synchronization object that
* refers to a valid ID3D11Fence object.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeNvSciSync, then
* ::cudaExternalSemaphoreHandleDesc::handle::nvSciSyncObj
* represents a valid NvSciSyncObj.
*
* ::cudaExternalSemaphoreHandleTypeKeyedMutex, then exactly one of
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
* is not NULL, then it represent a valid shared NT handle that
* is returned by IDXGIResource1::CreateSharedHandle when referring to
* a IDXGIKeyedMutex object.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeKeyedMutexKmt, then
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle must be
* non-NULL and ::cudaExternalSemaphoreHandleDesc::handle::win32::name
* must be NULL. The handle specified must represent a valid KMT
* handle that is returned by IDXGIResource::GetSharedHandle when
* referring to a IDXGIKeyedMutex object.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreFd, then
* ::cudaExternalSemaphoreHandleDesc::handle::fd must be a valid file
* descriptor referencing a synchronization object. Ownership of the
* file descriptor is transferred to the CUDA driver when the handle
* is imported successfully. Performing any operations on the file
* descriptor after it is imported results in undefined behavior.
*
* If ::cudaExternalSemaphoreHandleDesc::type is
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreWin32, then exactly one of
* ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
* ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
* NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
* is not NULL, then it must represent a valid shared NT handle that
* references a synchronization object. Ownership of this handle is
* not transferred to CUDA after the import operation, so the
* application must release the handle using the appropriate system
* call. If ::cudaExternalSemaphoreHandleDesc::handle::win32::name is
* not NULL, then it must name a valid synchronization object.
*
* \param extSem_out    - Returned handle to an external semaphore
* \param semHandleDesc - Semaphore import handle descriptor
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorOperatingSystem
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDestroyExternalSemaphore,
* ::cudaSignalExternalSemaphoresAsync,
* ::cudaWaitExternalSemaphoresAsync
*/
int handle_cudaImportExternalSemaphore(void *conn) {
    cudaExternalSemaphore_t extSem_out;
    struct cudaExternalSemaphoreHandleDesc semHandleDesc;

    if (rpc_read(conn, &extSem_out, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_read(conn, &semHandleDesc, sizeof(struct cudaExternalSemaphoreHandleDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaImportExternalSemaphore(&extSem_out, &semHandleDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &extSem_out, sizeof(cudaExternalSemaphore_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Signals a set of external semaphore objects
*
* Enqueues a signal operation on a set of externally allocated
* semaphore object in the specified stream. The operations will be
* executed when all prior operations in the stream complete.
*
* The exact semantics of signaling a semaphore depends on the type of
* the object.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeOpaqueFd,
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32,
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
* then signaling the semaphore will set it to the signaled state.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeD3D12Fence,
* ::cudaExternalSemaphoreHandleTypeD3D11Fence,
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreFd,
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreWin32
* then the semaphore will be set to the value specified in
* ::cudaExternalSemaphoreSignalParams::params::fence::value.
*
* If the semaphore object is of the type ::cudaExternalSemaphoreHandleTypeNvSciSync
* this API sets ::cudaExternalSemaphoreSignalParams::params::nvSciSync::fence to a
* value that can be used by subsequent waiters of the same NvSciSync object to
* order operations with those currently submitted in \p stream. Such an update
* will overwrite previous contents of
* ::cudaExternalSemaphoreSignalParams::params::nvSciSync::fence. By default,
* signaling such an external semaphore object causes appropriate memory synchronization
* operations to be performed over all the external memory objects that are imported as
* ::cudaExternalMemoryHandleTypeNvSciBuf. This ensures that any subsequent accesses
* made by other importers of the same set of NvSciBuf memory object(s) are coherent.
* These operations can be skipped by specifying the flag
* ::cudaExternalSemaphoreSignalSkipNvSciBufMemSync, which can be used as a
* performance optimization when data coherency is not required. But specifying this
* flag in scenarios where data coherency is required results in undefined behavior.
* Also, for semaphore object of the type ::cudaExternalSemaphoreHandleTypeNvSciSync,
* if the NvSciSyncAttrList used to create the NvSciSyncObj had not set the flags in
* ::cudaDeviceGetNvSciSyncAttributes to cudaNvSciSyncAttrSignal, this API will return
* cudaErrorNotSupported.
* 
* ::cudaExternalSemaphoreSignalParams::params::nvSciSync::fence associated with 
* semaphore object of the type ::cudaExternalSemaphoreHandleTypeNvSciSync can be 
* deterministic. For this the NvSciSyncAttrList used to create the semaphore object 
* must have value of NvSciSyncAttrKey_RequireDeterministicFences key set to true. 
* Deterministic fences allow users to enqueue a wait over the semaphore object even 
* before corresponding signal is enqueued. For such a semaphore object, CUDA guarantees 
* that each signal operation will increment the fence value by '1'. Users are expected 
* to track count of signals enqueued on the semaphore object and insert waits accordingly. 
* When such a semaphore object is signaled from multiple streams, due to concurrent 
* stream execution, it is possible that the order in which the semaphore gets signaled 
* is indeterministic. This could lead to waiters of the semaphore getting unblocked 
* incorrectly. Users are expected to handle such situations, either by not using the 
* same semaphore object with deterministic fence support enabled in different streams 
* or by adding explicit dependency amongst such streams so that the semaphore is 
* signaled in order.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeKeyedMutex,
* ::cudaExternalSemaphoreHandleTypeKeyedMutexKmt,
* then the keyed mutex will be released with the key specified in
* ::cudaExternalSemaphoreSignalParams::params::keyedmutex::key.
*
* \param extSemArray - Set of external semaphores to be signaled
* \param paramsArray - Array of semaphore parameters
* \param numExtSems  - Number of semaphores to signal
* \param stream     - Stream to enqueue the signal operations in
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaImportExternalSemaphore,
* ::cudaDestroyExternalSemaphore,
* ::cudaWaitExternalSemaphoresAsync
*/
int handle_cudaSignalExternalSemaphoresAsync_v2(void *conn) {
    cudaExternalSemaphore_t extSemArray;
    struct cudaExternalSemaphoreSignalParams paramsArray;
    unsigned int numExtSems;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &extSemArray, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_read(conn, &paramsArray, sizeof(struct cudaExternalSemaphoreSignalParams)) < 0 ||
        rpc_read(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSignalExternalSemaphoresAsync_v2(&extSemArray, &paramsArray, numExtSems, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Waits on a set of external semaphore objects
*
* Enqueues a wait operation on a set of externally allocated
* semaphore object in the specified stream. The operations will be
* executed when all prior operations in the stream complete.
*
* The exact semantics of waiting on a semaphore depends on the type
* of the object.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeOpaqueFd,
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32,
* ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
* then waiting on the semaphore will wait until the semaphore reaches
* the signaled state. The semaphore will then be reset to the
* unsignaled state. Therefore for every signal operation, there can
* only be one wait operation.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeD3D12Fence,
* ::cudaExternalSemaphoreHandleTypeD3D11Fence,
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreFd,
* ::cudaExternalSemaphoreHandleTypeTimelineSemaphoreWin32
* then waiting on the semaphore will wait until the value of the
* semaphore is greater than or equal to
* ::cudaExternalSemaphoreWaitParams::params::fence::value.
*
* If the semaphore object is of the type ::cudaExternalSemaphoreHandleTypeNvSciSync
* then, waiting on the semaphore will wait until the
* ::cudaExternalSemaphoreSignalParams::params::nvSciSync::fence is signaled by the
* signaler of the NvSciSyncObj that was associated with this semaphore object.
* By default, waiting on such an external semaphore object causes appropriate
* memory synchronization operations to be performed over all external memory objects
* that are imported as ::cudaExternalMemoryHandleTypeNvSciBuf. This ensures that
* any subsequent accesses made by other importers of the same set of NvSciBuf memory
* object(s) are coherent. These operations can be skipped by specifying the flag
* ::cudaExternalSemaphoreWaitSkipNvSciBufMemSync, which can be used as a
* performance optimization when data coherency is not required. But specifying this
* flag in scenarios where data coherency is required results in undefined behavior.
* Also, for semaphore object of the type ::cudaExternalSemaphoreHandleTypeNvSciSync,
* if the NvSciSyncAttrList used to create the NvSciSyncObj had not set the flags in
* ::cudaDeviceGetNvSciSyncAttributes to cudaNvSciSyncAttrWait, this API will return
* cudaErrorNotSupported.
*
* If the semaphore object is any one of the following types:
* ::cudaExternalSemaphoreHandleTypeKeyedMutex,
* ::cudaExternalSemaphoreHandleTypeKeyedMutexKmt,
* then the keyed mutex will be acquired when it is released with the key specified 
* in ::cudaExternalSemaphoreSignalParams::params::keyedmutex::key or
* until the timeout specified by
* ::cudaExternalSemaphoreSignalParams::params::keyedmutex::timeoutMs
* has lapsed. The timeout interval can either be a finite value
* specified in milliseconds or an infinite value. In case an infinite
* value is specified the timeout never elapses. The windows INFINITE
* macro must be used to specify infinite timeout
*
* \param extSemArray - External semaphores to be waited on
* \param paramsArray - Array of semaphore parameters
* \param numExtSems  - Number of semaphores to wait on
* \param stream      - Stream to enqueue the wait operations in
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle
* ::cudaErrorTimeout
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaImportExternalSemaphore,
* ::cudaDestroyExternalSemaphore,
* ::cudaSignalExternalSemaphoresAsync
*/
int handle_cudaWaitExternalSemaphoresAsync_v2(void *conn) {
    cudaExternalSemaphore_t extSemArray;
    struct cudaExternalSemaphoreWaitParams paramsArray;
    unsigned int numExtSems;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &extSemArray, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_read(conn, &paramsArray, sizeof(struct cudaExternalSemaphoreWaitParams)) < 0 ||
        rpc_read(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaWaitExternalSemaphoresAsync_v2(&extSemArray, &paramsArray, numExtSems, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an external semaphore
*
* Destroys an external semaphore object and releases any references
* to the underlying resource. Any outstanding signals or waits must
* have completed before the semaphore is destroyed.
*
* \param extSem - External semaphore to be destroyed
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa ::cudaImportExternalSemaphore,
* ::cudaSignalExternalSemaphoresAsync,
* ::cudaWaitExternalSemaphoresAsync
*/
int handle_cudaDestroyExternalSemaphore(void *conn) {
    cudaExternalSemaphore_t extSem;

    if (rpc_read(conn, &extSem, sizeof(cudaExternalSemaphore_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDestroyExternalSemaphore(extSem);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a device function
*
* The function invokes kernel \p func on \p gridDim (\p gridDim.x &times; \p gridDim.y
* &times; \p gridDim.z) grid of blocks. Each block contains \p blockDim (\p blockDim.x &times;
* \p blockDim.y &times; \p blockDim.z) threads.
*
* If the kernel has N parameters the \p args should point to array of N pointers.
* Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point to the region
* of memory from which the actual parameter will be copied.
*
* For templated functions, pass the function symbol as follows:
* func_name<template_arg_0,...,template_arg_N>
*
* \p sharedMem sets the amount of dynamic shared memory that will be available to
* each thread block.
*
* \p stream specifies a stream the invocation is associated to.
*
* \param func        - Device function symbol
* \param gridDim     - Grid dimensions
* \param blockDim    - Block dimensions
* \param args        - Arguments
* \param sharedMem   - Shared memory
* \param stream      - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorSharedObjectInitFailed,
* ::cudaErrorInvalidPtx,
* ::cudaErrorUnsupportedPtxVersion,
* ::cudaErrorNoKernelImageForDevice,
* ::cudaErrorJitCompilerNotFound,
* ::cudaErrorJitCompilationDisabled
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C++ API)",
* ::cuLaunchKernel
*/
int handle_cudaLaunchKernel(void *conn) {
    void* func;
    dim3 gridDim;
    dim3 blockDim;
    void* args;
    size_t sharedMem;
    cudaStream_t stream;

    if (rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &gridDim, sizeof(dim3)) < 0 ||
        rpc_read(conn, &blockDim, sizeof(dim3)) < 0 ||
        rpc_read(conn, &args, sizeof(void*)) < 0 ||
        rpc_read(conn, &sharedMem, sizeof(size_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaLaunchKernel(&func, gridDim, blockDim, &args, sharedMem, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &args, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a CUDA function with launch-time configuration
*
* Note that the functionally equivalent variadic template ::cudaLaunchKernelEx
* is available for C++11 and newer.
*
* Invokes the kernel \p func on \p config->gridDim (\p config->gridDim.x
* &times; \p config->gridDim.y &times; \p config->gridDim.z) grid of blocks.
* Each block contains \p config->blockDim (\p config->blockDim.x &times;
* \p config->blockDim.y &times; \p config->blockDim.z) threads.
*
* \p config->dynamicSmemBytes sets the amount of dynamic shared memory that
* will be available to each thread block.
*
* \p config->stream specifies a stream the invocation is associated to.
*
* Configuration beyond grid and block dimensions, dynamic shared memory size,
* and stream can be provided with the following two fields of \p config:
*
* \p config->attrs is an array of \p config->numAttrs contiguous
* ::cudaLaunchAttribute elements. The value of this pointer is not considered
* if \p config->numAttrs is zero. However, in that case, it is recommended to
* set the pointer to NULL.                                  
* \p config->numAttrs is the number of attributes populating the first
* \p config->numAttrs positions of the \p config->attrs array.
*
* If the kernel has N parameters the \p args should point to array of N
* pointers. Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point
* to the region of memory from which the actual parameter will be copied.
*
* N.B. This function is so named to avoid unintentionally invoking the
*      templated version, \p cudaLaunchKernelEx, for kernels taking a single
*      void** or void* parameter.
*
* \param config - Launch configuration
* \param func   - Kernel to launch
* \param args   - Array of pointers to kernel parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorSharedObjectInitFailed,
* ::cudaErrorInvalidPtx,
* ::cudaErrorUnsupportedPtxVersion,
* ::cudaErrorNoKernelImageForDevice,
* ::cudaErrorJitCompilerNotFound,
* ::cudaErrorJitCompilationDisabled
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaLaunchKernelEx(const cudaLaunchConfig_t *config, void (*kernel)(ExpTypes...), ActTypes &&... args) "cudaLaunchKernelEx (C++ API)",
* ::cuLaunchKernelEx
*/
int handle_cudaLaunchKernelExC(void *conn) {
    cudaLaunchConfig_t config;
    void* func;
    void* args;

    if (rpc_read(conn, &config, sizeof(cudaLaunchConfig_t)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &args, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaLaunchKernelExC(&config, &func, &args);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &args, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches a device function where thread blocks can cooperate and synchronize as they execute
*
* The function invokes kernel \p func on \p gridDim (\p gridDim.x &times; \p gridDim.y
* &times; \p gridDim.z) grid of blocks. Each block contains \p blockDim (\p blockDim.x &times;
* \p blockDim.y &times; \p blockDim.z) threads.
*
* The device on which this kernel is invoked must have a non-zero value for
* the device attribute ::cudaDevAttrCooperativeLaunch.
*
* The total number of blocks launched cannot exceed the maximum number of blocks per
* multiprocessor as returned by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor (or
* ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
* as specified by the device attribute ::cudaDevAttrMultiProcessorCount.
*
* The kernel cannot make use of CUDA dynamic parallelism.
*
* If the kernel has N parameters the \p args should point to array of N pointers.
* Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point to the region
* of memory from which the actual parameter will be copied.
*
* For templated functions, pass the function symbol as follows:
* func_name<template_arg_0,...,template_arg_N>
*
* \p sharedMem sets the amount of dynamic shared memory that will be available to
* each thread block.
*
* \p stream specifies a stream the invocation is associated to.
*
* \param func        - Device function symbol
* \param gridDim     - Grid dimensions
* \param blockDim    - Block dimensions
* \param args        - Arguments
* \param sharedMem   - Shared memory
* \param stream      - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorCooperativeLaunchTooLarge,
* ::cudaErrorSharedObjectInitFailed
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaLaunchCooperativeKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchCooperativeKernel (C++ API)",
* ::cudaLaunchCooperativeKernelMultiDevice,
* ::cuLaunchCooperativeKernel
*/
int handle_cudaLaunchCooperativeKernel(void *conn) {
    void* func;
    dim3 gridDim;
    dim3 blockDim;
    void* args;
    size_t sharedMem;
    cudaStream_t stream;

    if (rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &gridDim, sizeof(dim3)) < 0 ||
        rpc_read(conn, &blockDim, sizeof(dim3)) < 0 ||
        rpc_read(conn, &args, sizeof(void*)) < 0 ||
        rpc_read(conn, &sharedMem, sizeof(size_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaLaunchCooperativeKernel(&func, gridDim, blockDim, &args, sharedMem, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &args, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Launches device functions on multiple devices where thread blocks can cooperate and synchronize as they execute
*
* \deprecated This function is deprecated as of CUDA 11.3.
*
* Invokes kernels as specified in the \p launchParamsList array where each element
* of the array specifies all the parameters required to perform a single kernel launch.
* These kernels can cooperate and synchronize as they execute. The size of the array is
* specified by \p numDevices.
*
* No two kernels can be launched on the same device. All the devices targeted by this
* multi-device launch must be identical. All devices must have a non-zero value for the
* device attribute ::cudaDevAttrCooperativeMultiDeviceLaunch.
*
* The same kernel must be launched on all devices. Note that any __device__ or __constant__
* variables are independently instantiated on every device. It is the application's
* responsibility to ensure these variables are initialized and used appropriately.
*
* The size of the grids as specified in blocks, the size of the blocks themselves and the
* amount of shared memory used by each thread block must also match across all launched kernels.
*
* The streams used to launch these kernels must have been created via either ::cudaStreamCreate
* or ::cudaStreamCreateWithPriority or ::cudaStreamCreateWithPriority. The NULL stream or
* ::cudaStreamLegacy or ::cudaStreamPerThread cannot be used.
*
* The total number of blocks launched per kernel cannot exceed the maximum number of blocks
* per multiprocessor as returned by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor (or
* ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
* as specified by the device attribute ::cudaDevAttrMultiProcessorCount. Since the
* total number of blocks launched per device has to match across all devices, the maximum
* number of blocks that can be launched per device will be limited by the device with the
* least number of multiprocessors.
*
* The kernel cannot make use of CUDA dynamic parallelism.
*
* The ::cudaLaunchParams structure is defined as:
* \code
        struct cudaLaunchParams
        {
            void *func;
            dim3 gridDim;
            dim3 blockDim;
            void **args;
            size_t sharedMem;
            cudaStream_t stream;
        };
* \endcode
* where:
* - ::cudaLaunchParams::func specifies the kernel to be launched. This same functions must
*   be launched on all devices. For templated functions, pass the function symbol as follows:
*   func_name<template_arg_0,...,template_arg_N>
* - ::cudaLaunchParams::gridDim specifies the width, height and depth of the grid in blocks.
*   This must match across all kernels launched.
* - ::cudaLaunchParams::blockDim is the width, height and depth of each thread block. This
*   must match across all kernels launched.
* - ::cudaLaunchParams::args specifies the arguments to the kernel. If the kernel has
*   N parameters then ::cudaLaunchParams::args should point to array of N pointers. Each
*   pointer, from <tt>::cudaLaunchParams::args[0]</tt> to <tt>::cudaLaunchParams::args[N - 1]</tt>,
*   point to the region of memory from which the actual parameter will be copied.
* - ::cudaLaunchParams::sharedMem is the dynamic shared-memory size per thread block in bytes.
*   This must match across all kernels launched.
* - ::cudaLaunchParams::stream is the handle to the stream to perform the launch in. This cannot
*   be the NULL stream or ::cudaStreamLegacy or ::cudaStreamPerThread.
*
* By default, the kernel won't begin execution on any GPU until all prior work in all the specified
* streams has completed. This behavior can be overridden by specifying the flag
* ::cudaCooperativeLaunchMultiDeviceNoPreSync. When this flag is specified, each kernel
* will only wait for prior work in the stream corresponding to that GPU to complete before it begins
* execution.
*
* Similarly, by default, any subsequent work pushed in any of the specified streams will not begin
* execution until the kernels on all GPUs have completed. This behavior can be overridden by specifying
* the flag ::cudaCooperativeLaunchMultiDeviceNoPostSync. When this flag is specified,
* any subsequent work pushed in any of the specified streams will only wait for the kernel launched
* on the GPU corresponding to that stream to complete before it begins execution.
*
* \param launchParamsList - List of launch parameters, one per device
* \param numDevices       - Size of the \p launchParamsList array
* \param flags            - Flags to control launch behavior
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidConfiguration,
* ::cudaErrorLaunchFailure,
* ::cudaErrorLaunchTimeout,
* ::cudaErrorLaunchOutOfResources,
* ::cudaErrorCooperativeLaunchTooLarge,
* ::cudaErrorSharedObjectInitFailed
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaLaunchCooperativeKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchCooperativeKernel (C++ API)",
* ::cudaLaunchCooperativeKernel,
* ::cuLaunchCooperativeKernelMultiDevice
*/
int handle_cudaLaunchCooperativeKernelMultiDevice(void *conn) {
    struct cudaLaunchParams launchParamsList;
    unsigned int numDevices;
    unsigned int flags = 0;

    if (rpc_read(conn, &launchParamsList, sizeof(struct cudaLaunchParams)) < 0 ||
        rpc_read(conn, &numDevices, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaLaunchCooperativeKernelMultiDevice(&launchParamsList, numDevices, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &launchParamsList, sizeof(struct cudaLaunchParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the preferred cache configuration for a device function
*
* On devices where the L1 cache and shared memory use the same hardware
* resources, this sets through \p cacheConfig the preferred cache configuration
* for the function specified via \p func. This is only a preference. The
* runtime will use the requested configuration if possible, but it is free to
* choose a different configuration if required to execute \p func.
*
* \p func is a device function symbol and must be declared as a
* \c __global__ function. If the specified function does not exist,
* then ::cudaErrorInvalidDeviceFunction is returned. For templated functions,
* pass the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
*
* This setting does nothing on devices where the size of the L1 cache and
* shared memory are fixed.
*
* Launching a kernel with a different preference than the most recent
* preference setting may insert a device-side synchronization point.
*
* The supported cache configurations are:
* - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
* - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
* - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
* - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
*
* \param func        - Device function symbol
* \param cacheConfig - Requested cache configuration
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction
* \notefnerr
* \note_string_api_deprecation2
* \note_init_rt
* \note_callback
*
* \sa 
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
* \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
* \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
* ::cuFuncSetCacheConfig
*/
int handle_cudaFuncSetCacheConfig(void *conn) {
    void* func;
    enum cudaFuncCache cacheConfig;

    if (rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFuncSetCacheConfig(&func, cacheConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the shared memory configuration for a device function
*
* On devices with configurable shared memory banks, this function will 
* force all subsequent launches of the specified device function to have
* the given shared memory bank size configuration. On any given launch of the
* function, the shared memory configuration of the device will be temporarily
* changed if needed to suit the function's preferred configuration. Changes in
* shared memory configuration between subsequent launches of functions, 
* may introduce a device side synchronization point.
*
* Any per-function setting of shared memory bank size set via 
* ::cudaFuncSetSharedMemConfig will override the device wide setting set by
* ::cudaDeviceSetSharedMemConfig.
*
* Changing the shared memory bank size will not increase shared memory usage
* or affect occupancy of kernels, but may have major effects on performance. 
* Larger bank sizes will allow for greater potential bandwidth to shared memory,
* but will change what kinds of accesses to shared memory will result in bank 
* conflicts.
*
* This function will do nothing on devices with fixed shared memory bank size.
*
* For templated functions, pass the function symbol as follows:
* func_name<template_arg_0,...,template_arg_N>
*
* The supported bank configurations are:
* - ::cudaSharedMemBankSizeDefault: use the device's shared memory configuration
*   when launching this function.
* - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be 
*   four bytes natively when launching this function.
* - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight 
*   bytes natively when launching this function.
*
* \param func   - Device function symbol
* \param config - Requested shared memory configuration
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_string_api_deprecation2
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceSetSharedMemConfig,
* ::cudaDeviceGetSharedMemConfig,
* ::cudaDeviceSetCacheConfig,
* ::cudaDeviceGetCacheConfig,
* ::cudaFuncSetCacheConfig,
* ::cuFuncSetSharedMemConfig
*/
int handle_cudaFuncSetSharedMemConfig(void *conn) {
    void* func;
    enum cudaSharedMemConfig config;

    if (rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &config, sizeof(enum cudaSharedMemConfig)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFuncSetSharedMemConfig(&func, config);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Find out attributes for a given function
*
* This function obtains the attributes of a function specified via \p func.
* \p func is a device function symbol and must be declared as a
* \c __global__ function. The fetched attributes are placed in \p attr.
* If the specified function does not exist, then
* ::cudaErrorInvalidDeviceFunction is returned. For templated functions, pass
* the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
*
* Note that some function attributes such as
* \ref ::cudaFuncAttributes::maxThreadsPerBlock "maxThreadsPerBlock"
* may vary based on the device that is currently being used.
*
* \param attr - Return pointer to function's attributes
* \param func - Device function symbol
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction
* \notefnerr
* \note_string_api_deprecation2
* \note_init_rt
* \note_callback
*
* \sa 
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
* \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, T*) "cudaFuncGetAttributes (C++ API)",
* \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
* ::cuFuncGetAttribute
*/
int handle_cudaFuncGetAttributes(void *conn) {
    struct cudaFuncAttributes attr;
    void* func;

    if (rpc_read(conn, &attr, sizeof(struct cudaFuncAttributes)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFuncGetAttributes(&attr, &func);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &attr, sizeof(struct cudaFuncAttributes)) < 0)
        return -1;

    return result;
}

/**
* \brief Set attributes for a given function
*
* This function sets the attributes of a function specified via \p func.
* The parameter \p func must be a pointer to a function that executes
* on the device. The parameter specified by \p func must be declared as a \p __global__
* function. The enumeration defined by \p attr is set to the value defined by \p value.
* If the specified function does not exist, then ::cudaErrorInvalidDeviceFunction is returned.
* If the specified attribute cannot be written, or if the value is incorrect, 
* then ::cudaErrorInvalidValue is returned.
*
* Valid values for \p attr are:
* - ::cudaFuncAttributeMaxDynamicSharedMemorySize - The requested maximum size in bytes of dynamically-allocated shared memory. The sum of this value and the function attribute ::sharedSizeBytes
*   cannot exceed the device attribute ::cudaDevAttrMaxSharedMemoryPerBlockOptin. The maximal size of requestable dynamic shared memory may differ by GPU architecture.
* - ::cudaFuncAttributePreferredSharedMemoryCarveout - On devices where the L1 cache and shared memory use the same hardware resources, 
*   this sets the shared memory carveout preference, in percent of the total shared memory. See ::cudaDevAttrMaxSharedMemoryPerMultiprocessor.
*   This is only a hint, and the driver can choose a different ratio if required to execute the function.
*
* \param func  - Function to get attributes of
* \param attr  - Attribute to set
* \param value - Value to set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C++ API)",
* \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
* \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
*/
int handle_cudaFuncSetAttribute(void *conn) {
    void* func;
    enum cudaFuncAttribute attr;
    int value;

    if (rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaFuncAttribute)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFuncSetAttribute(&func, attr, value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Converts a double argument to be executed on a device
*
* \param d - Double to convert
*
* \deprecated This function is deprecated as of CUDA 7.5
*
* Converts the double value of \p d to an internal float representation if
* the device does not support double arithmetic. If the device does natively
* support doubles, then this function does nothing.
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
* \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
* ::cudaSetDoubleForHost
*/
int handle_cudaSetDoubleForDevice(void *conn) {
    double d;

    if (rpc_read(conn, &d, sizeof(double)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSetDoubleForDevice(&d);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &d, sizeof(double)) < 0)
        return -1;

    return result;
}

/**
* \brief Converts a double argument after execution on a device
*
* \deprecated This function is deprecated as of CUDA 7.5
*
* Converts the double value of \p d from a potentially internal float
* representation if the device does not support double arithmetic. If the
* device does natively support doubles, then this function does nothing.
*
* \param d - Double to convert
*
* \return
* ::cudaSuccess
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
* \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
* ::cudaSetDoubleForDevice
*/
int handle_cudaSetDoubleForHost(void *conn) {
    double d;

    if (rpc_read(conn, &d, sizeof(double)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaSetDoubleForHost(&d);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &d, sizeof(double)) < 0)
        return -1;

    return result;
}

/**
* \brief Enqueues a host function call in a stream
*
* Enqueues a host function to run in a stream.  The function will be called
* after currently enqueued work and will block work added after it.
*
* The host function must not make any CUDA API calls.  Attempting to use a
* CUDA API may result in ::cudaErrorNotPermitted, but this is not required.
* The host function must not perform any synchronization that may depend on
* outstanding CUDA work not mandated to run earlier.  Host functions without a
* mandated order (such as in independent streams) execute in undefined order
* and may be serialized.
*
* For the purposes of Unified Memory, execution makes a number of guarantees:
* <ul>
*   <li>The stream is considered idle for the duration of the function's
*   execution.  Thus, for example, the function may always use memory attached
*   to the stream it was enqueued in.</li>
*   <li>The start of execution of the function has the same effect as
*   synchronizing an event recorded in the same stream immediately prior to
*   the function.  It thus synchronizes streams which have been "joined"
*   prior to the function.</li>
*   <li>Adding device work to any stream does not have the effect of making
*   the stream active until all preceding host functions and stream callbacks
*   have executed.  Thus, for
*   example, a function might use global attached memory even if work has
*   been added to another stream, if the work has been ordered behind the
*   function call with an event.</li>
*   <li>Completion of the function does not cause a stream to become
*   active except as described above.  The stream will remain idle
*   if no device work follows the function, and will remain idle across
*   consecutive host functions or stream callbacks without device work in
*   between.  Thus, for example,
*   stream synchronization can be done by signaling from a host function at the
*   end of the stream.</li>
* </ul>
*
* Note that, in constrast to ::cuStreamAddCallback, the function will not be
* called in the event of an error in the CUDA context.
*
* \param hStream  - Stream to enqueue function call in
* \param fn       - The function to call once preceding stream operations are complete
* \param userData - User-specified data to be passed to the function
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaStreamCreate,
* ::cudaStreamQuery,
* ::cudaStreamSynchronize,
* ::cudaStreamWaitEvent,
* ::cudaStreamDestroy,
* ::cudaMallocManaged,
* ::cudaStreamAttachMemAsync,
* ::cudaStreamAddCallback,
* ::cuLaunchHostFunc
*/
int handle_cudaLaunchHostFunc(void *conn) {
    cudaStream_t stream;
    cudaHostFn_t fn;
    void* userData;

    if (rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &fn, sizeof(cudaHostFn_t)) < 0 ||
        rpc_read(conn, &userData, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaLaunchHostFunc(stream, fn, &userData);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &userData, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns occupancy for a device function
*
* Returns in \p *numBlocks the maximum number of active blocks per
* streaming multiprocessor for the device function.
*
* \param numBlocks       - Returned occupancy
* \param func            - Kernel function for which occupancy is calculated
* \param blockSize       - Block size the kernel is intended to be launched with
* \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
* \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)",
* \ref ::cudaOccupancyAvailableDynamicSMemPerBlock(size_t*, T, int, int) "cudaOccupancyAvailableDynamicSMemPerBlock (C++ API)",
* ::cuOccupancyMaxActiveBlocksPerMultiprocessor
*/
int handle_cudaOccupancyMaxActiveBlocksPerMultiprocessor(void *conn) {
    int numBlocks;
    void* func;
    int blockSize;
    size_t dynamicSMemSize;

    if (rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_read(conn, &dynamicSMemSize, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, &func, blockSize, dynamicSMemSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numBlocks, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns dynamic shared memory available per block when launching \p numBlocks blocks on SM.
*
* Returns in \p *dynamicSmemSize the maximum size of dynamic shared memory to allow \p numBlocks blocks per SM. 
*
* \param dynamicSmemSize - Returned maximum dynamic shared memory 
* \param func            - Kernel function for which occupancy is calculated
* \param numBlocks       - Number of blocks to fit on SM 
* \param blockSize       - Size of the block
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
* \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)",
* ::cudaOccupancyAvailableDynamicSMemPerBlock
*/
int handle_cudaOccupancyAvailableDynamicSMemPerBlock(void *conn) {
    size_t dynamicSmemSize;
    void* func;
    int numBlocks;
    int blockSize;

    if (rpc_read(conn, &dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaOccupancyAvailableDynamicSMemPerBlock(&dynamicSmemSize, &func, numBlocks, blockSize);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dynamicSmemSize, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns occupancy for a device function with the specified flags
*
* Returns in \p *numBlocks the maximum number of active blocks per
* streaming multiprocessor for the device function.
*
* The \p flags parameter controls how special cases are handled. Valid flags include:
*
* - ::cudaOccupancyDefault: keeps the default behavior as
*   ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
*
* - ::cudaOccupancyDisableCachingOverride: This flag suppresses the default behavior
*   on platform where global caching affects occupancy. On such platforms, if caching
*   is enabled, but per-block SM resource usage would result in zero occupancy, the
*   occupancy calculator will calculate the occupancy as if caching is disabled.
*   Setting this flag makes the occupancy calculator to return 0 in such cases.
*   More information can be found about this feature in the "Unified L1/Texture Cache"
*   section of the Maxwell tuning guide.
*
* \param numBlocks       - Returned occupancy
* \param func            - Kernel function for which occupancy is calculated
* \param blockSize       - Block size the kernel is intended to be launched with
* \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
* \param flags           - Requested behavior for the occupancy calculator
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessor,
* \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)",
* \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)",
* \ref ::cudaOccupancyAvailableDynamicSMemPerBlock(size_t*, T, int, int) "cudaOccupancyAvailableDynamicSMemPerBlock (C++ API)",
* ::cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
*/
int handle_cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(void *conn) {
    int numBlocks;
    void* func;
    int blockSize;
    size_t dynamicSMemSize;
    unsigned int flags;

    if (rpc_read(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_read(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(&numBlocks, &func, blockSize, dynamicSMemSize, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numBlocks, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Given the kernel function (\p func) and launch configuration
* (\p config), return the maximum cluster size in \p *clusterSize.
*
* The cluster dimensions in \p config are ignored. If func has a required
* cluster size set (see ::cudaFuncGetAttributes),\p *clusterSize will reflect 
* the required cluster size.
*
* By default this function will always return a value that's portable on
* future hardware. A higher value may be returned if the kernel function
* allows non-portable cluster sizes.
*
* This function will respect the compile time launch bounds.
*
* \param clusterSize - Returned maximum cluster size that can be launched
*                      for the given kernel function and launch configuration
* \param func        - Kernel function for which maximum cluster
*                      size is calculated
* \param config      - Launch configuration for the given kernel function
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaFuncGetAttributes
* \ref ::cudaOccupancyMaxPotentialClusterSize(int*, T, const cudaLaunchConfig_t*) "cudaOccupancyMaxPotentialClusterSize (C++ API)",
* ::cuOccupancyMaxPotentialClusterSize
*/
int handle_cudaOccupancyMaxPotentialClusterSize(void *conn) {
    int clusterSize;
    void* func;
    cudaLaunchConfig_t launchConfig;

    if (rpc_read(conn, &clusterSize, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &launchConfig, sizeof(cudaLaunchConfig_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaOccupancyMaxPotentialClusterSize(&clusterSize, &func, &launchConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &clusterSize, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Given the kernel function (\p func) and launch configuration
* (\p config), return the maximum number of clusters that could co-exist
* on the target device in \p *numClusters.
*
* If the function has required cluster size already set (see
* ::cudaFuncGetAttributes), the cluster size from config must either be
* unspecified or match the required size.
* Without required sizes, the cluster size must be specified in config,
* else the function will return an error.
*
* Note that various attributes of the kernel function may affect occupancy
* calculation. Runtime environment may affect how the hardware schedules
* the clusters, so the calculated occupancy is not guaranteed to be achievable.
*
* \param numClusters - Returned maximum number of clusters that
*                      could co-exist on the target device
* \param func        - Kernel function for which maximum number
*                      of clusters are calculated
* \param config      - Launch configuration for the given kernel function
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDeviceFunction,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidClusterSize,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaFuncGetAttributes
* \ref ::cudaOccupancyMaxActiveClusters(int*, T, const cudaLaunchConfig_t*) "cudaOccupancyMaxActiveClusters (C++ API)",
* ::cuOccupancyMaxActiveClusters
*/
int handle_cudaOccupancyMaxActiveClusters(void *conn) {
    int numClusters;
    void* func;
    cudaLaunchConfig_t launchConfig;

    if (rpc_read(conn, &numClusters, sizeof(int)) < 0 ||
        rpc_read(conn, &func, sizeof(void*)) < 0 ||
        rpc_read(conn, &launchConfig, sizeof(cudaLaunchConfig_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaOccupancyMaxActiveClusters(&numClusters, &func, &launchConfig);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &numClusters, sizeof(int)) < 0)
        return -1;

    return result;
}

int handle_cudaMallocManaged(void *conn) {
    void* devPtr;
    size_t size;
    unsigned int flags = 0x01;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocManaged(&devPtr, size, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocate memory on the device
*
* Allocates \p size bytes of linear memory on the device and returns in
* \p *devPtr a pointer to the allocated memory. The allocated memory is
* suitably aligned for any kind of variable. The memory is not cleared.
* ::cudaMalloc() returns ::cudaErrorMemoryAllocation in case of failure.
*
* The device version of ::cudaFree cannot be used with a \p *devPtr
* allocated using the host API, and vice versa.
*
* \param devPtr - Pointer to allocated device memory
* \param size   - Requested allocation size in bytes
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
* ::cudaMalloc3D, ::cudaMalloc3DArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::cuMemAlloc
*/
int handle_cudaMalloc(void *conn) {
    void* devPtr;
    size_t size;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMalloc(&devPtr, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates page-locked memory on the host
*
* Allocates \p size bytes of host memory that is page-locked and accessible
* to the device. The driver tracks the virtual memory ranges allocated with
* this function and automatically accelerates calls to functions such as
* ::cudaMemcpy*(). Since the memory can be accessed directly by the device,
* it can be read or written with much higher bandwidth than pageable memory
* obtained with functions such as ::malloc(). Allocating excessive amounts of
* memory with ::cudaMallocHost() may degrade system performance, since it
* reduces the amount of memory available to the system for paging. As a
* result, this function is best used sparingly to allocate staging areas for
* data exchange between host and device.
*
* \param ptr  - Pointer to allocated host memory
* \param size - Requested allocation size in bytes
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocArray, ::cudaMalloc3D,
* ::cudaMalloc3DArray, ::cudaHostAlloc, ::cudaFree, ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t, unsigned int) "cudaMallocHost (C++ API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::cuMemAllocHost
*/
int handle_cudaMallocHost(void *conn) {
    void* ptr;
    size_t size;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocHost(&ptr, size);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates pitched memory on the device
*
* Allocates at least \p width (in bytes) * \p height bytes of linear memory
* on the device and returns in \p *devPtr a pointer to the allocated memory.
* The function may pad the allocation to ensure that corresponding pointers
* in any given row will continue to meet the alignment requirements for
* coalescing as the address is updated from row to row. The pitch returned in
* \p *pitch by ::cudaMallocPitch() is the width in bytes of the allocation.
* The intended usage of \p pitch is as a separate parameter of the allocation,
* used to compute addresses within the 2D array. Given the row and column of
* an array element of type \p T, the address is computed as:
* \code
    T* pElement = (T*)((char*)BaseAddress + Row * pitch) + Column;
   \endcode
*
* For allocations of 2D arrays, it is recommended that programmers consider
* performing pitch allocations using ::cudaMallocPitch(). Due to pitch
* alignment restrictions in the hardware, this is especially true if the
* application will be performing 2D memory copies between different regions
* of device memory (whether linear memory or CUDA arrays).
*
* \param devPtr - Pointer to allocated pitched device memory
* \param pitch  - Pitch for allocation
* \param width  - Requested pitched allocation width (in bytes)
* \param height - Requested pitched allocation height
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
* ::cudaHostAlloc,
* ::cuMemAllocPitch
*/
int handle_cudaMallocPitch(void *conn) {
    void* devPtr;
    size_t pitch;
    size_t width;
    size_t height;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &pitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocPitch(&devPtr, &pitch, width, height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &pitch, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocate an array on the device
*
* Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
* \p desc and returns a handle to the new CUDA array in \p *array.
*
* The ::cudaChannelFormatDesc is defined as:
* \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
    enum cudaChannelFormatKind f;
    };
    \endcode
* where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
* ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
*
* The \p flags parameter enables different options to be specified that affect
* the allocation, as follows.
* - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
* - ::cudaArraySurfaceLoadStore: Allocates an array that can be read from or written to using a surface reference
* - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the array.
* - ::cudaArraySparse: Allocates a CUDA array without physical backing memory. The subregions within this sparse array
*   can later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. 
*   The physical backing memory must be allocated via ::cuMemCreate.
* - ::cudaArrayDeferredMapping: Allocates a CUDA array without physical backing memory. The entire array can 
*   later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. 
*   The physical backing memory must be allocated via ::cuMemCreate.
*
* \p width and \p height must meet certain size requirements. See ::cudaMalloc3DArray() for more details.
*
* \param array  - Pointer to allocated array in device memory
* \param desc   - Requested channel format
* \param width  - Requested array allocation width
* \param height - Requested array allocation height
* \param flags  - Requested properties of allocated array
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
* ::cudaHostAlloc,
* ::cuArrayCreate
*/
int handle_cudaMallocArray(void *conn) {
    cudaArray_t array;
    struct cudaChannelFormatDesc desc;
    size_t width;
    size_t height = 0;
    unsigned int flags = 0;

    if (rpc_read(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocArray(&array, &desc, width, height, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees memory on the device
*
* Frees the memory space pointed to by \p devPtr, which must have been
* returned by a previous call to one of the following memory allocation APIs -
* ::cudaMalloc(), ::cudaMallocPitch(), ::cudaMallocManaged(), ::cudaMallocAsync(),
* ::cudaMallocFromPoolAsync().
* 
* Note - This API will not perform any implicit synchronization when the pointer was
* allocated with ::cudaMallocAsync or ::cudaMallocFromPoolAsync. Callers must ensure
* that all accesses to the pointer have completed before invoking ::cudaFree. For
* best performance and memory reuse, users should use ::cudaFreeAsync to free memory
* allocated via the stream ordered memory allocator.
* 
* If ::cudaFree(\p devPtr) has already been called before,
* an error is returned. If \p devPtr is 0, no operation is performed.
* ::cudaFree() returns ::cudaErrorValue in case of failure.
*
* The device version of ::cudaFree cannot be used with a \p *devPtr
* allocated using the host API, and vice versa.
*
* \param devPtr - Device pointer to memory to free
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocManaged, ::cudaMallocArray, ::cudaFreeArray, ::cudaMallocAsync, ::cudaMallocFromPoolAsync
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaFreeAsync
* ::cudaHostAlloc,
* ::cuMemFree
*/
int handle_cudaFree(void *conn) {
    void* devPtr;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFree(&devPtr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees page-locked memory
*
* Frees the memory space pointed to by \p hostPtr, which must have been
* returned by a previous call to ::cudaMallocHost() or ::cudaHostAlloc().
*
* \param ptr - Pointer to memory to free
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
* ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaHostAlloc,
* ::cuMemFreeHost
*/
int handle_cudaFreeHost(void *conn) {
    void* ptr;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFreeHost(&ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees an array on the device
*
* Frees the CUDA array \p array, which must have been returned by a
* previous call to ::cudaMallocArray(). If \p devPtr is 0,
* no operation is performed.
*
* \param array - Pointer to array to free
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::cuArrayDestroy
*/
int handle_cudaFreeArray(void *conn) {
    cudaArray_t array;

    if (rpc_read(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFreeArray(array);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Frees a mipmapped array on the device
*
* Frees the CUDA mipmapped array \p mipmappedArray, which must have been 
* returned by a previous call to ::cudaMallocMipmappedArray(). If \p devPtr
* is 0, no operation is performed.
*
* \param mipmappedArray - Pointer to mipmapped array to free
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::cuMipmappedArrayDestroy
*/
int handle_cudaFreeMipmappedArray(void *conn) {
    cudaMipmappedArray_t mipmappedArray;

    if (rpc_read(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFreeMipmappedArray(mipmappedArray);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates page-locked memory on the host
*
* Allocates \p size bytes of host memory that is page-locked and accessible
* to the device. The driver tracks the virtual memory ranges allocated with
* this function and automatically accelerates calls to functions such as
* ::cudaMemcpy(). Since the memory can be accessed directly by the device, it
* can be read or written with much higher bandwidth than pageable memory
* obtained with functions such as ::malloc(). Allocating excessive amounts of
* pinned memory may degrade system performance, since it reduces the amount
* of memory available to the system for paging. As a result, this function is
* best used sparingly to allocate staging areas for data exchange between host
* and device.
*
* The \p flags parameter enables different options to be specified that affect
* the allocation, as follows.
* - ::cudaHostAllocDefault: This flag's value is defined to be 0 and causes
* ::cudaHostAlloc() to emulate ::cudaMallocHost().
* - ::cudaHostAllocPortable: The memory returned by this call will be
* considered as pinned memory by all CUDA contexts, not just the one that
* performed the allocation.
* - ::cudaHostAllocMapped: Maps the allocation into the CUDA address space.
* The device pointer to the memory may be obtained by calling
* ::cudaHostGetDevicePointer().
* - ::cudaHostAllocWriteCombined: Allocates the memory as write-combined (WC).
* WC memory can be transferred across the PCI Express bus more quickly on some
* system configurations, but cannot be read efficiently by most CPUs.  WC
* memory is a good option for buffers that will be written by the CPU and read
* by the device via mapped pinned memory or host->device transfers.
*
* All of these flags are orthogonal to one another: a developer may allocate
* memory that is portable, mapped and/or write-combined with no restrictions.
*
* In order for the ::cudaHostAllocMapped flag to have any effect, the CUDA context
* must support the ::cudaDeviceMapHost flag, which can be checked via
* ::cudaGetDeviceFlags(). The ::cudaDeviceMapHost flag is implicitly set for
* contexts created via the runtime API.
*
* The ::cudaHostAllocMapped flag may be specified on CUDA contexts for devices
* that do not support mapped pinned memory. The failure is deferred to
* ::cudaHostGetDevicePointer() because the memory may be mapped into other
* CUDA contexts via the ::cudaHostAllocPortable flag.
*
* Memory allocated by this function must be freed with ::cudaFreeHost().
*
* \param pHost - Device pointer to allocated memory
* \param size  - Requested allocation size in bytes
* \param flags - Requested properties of allocated memory
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaSetDeviceFlags,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost,
* ::cudaGetDeviceFlags,
* ::cuMemHostAlloc
*/
int handle_cudaHostAlloc(void *conn) {
    void* pHost;
    size_t size;
    unsigned int flags;

    if (rpc_read(conn, &pHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaHostAlloc(&pHost, size, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Registers an existing host memory range for use by CUDA
*
* Page-locks the memory range specified by \p ptr and \p size and maps it
* for the device(s) as specified by \p flags. This memory range also is added
* to the same tracking mechanism as ::cudaHostAlloc() to automatically accelerate
* calls to functions such as ::cudaMemcpy(). Since the memory can be accessed 
* directly by the device, it can be read or written with much higher bandwidth 
* than pageable memory that has not been registered.  Page-locking excessive
* amounts of memory may degrade system performance, since it reduces the amount
* of memory available to the system for paging. As a result, this function is
* best used sparingly to register staging areas for data exchange between
* host and device.
*
* ::cudaHostRegister is supported only on I/O coherent devices that have a non-zero
* value for the device attribute ::cudaDevAttrHostRegisterSupported.
*
* The \p flags parameter enables different options to be specified that
* affect the allocation, as follows.
*
* - ::cudaHostRegisterDefault: On a system with unified virtual addressing,
*   the memory will be both mapped and portable.  On a system with no unified
*   virtual addressing, the memory will be neither mapped nor portable.
*
* - ::cudaHostRegisterPortable: The memory returned by this call will be
*   considered as pinned memory by all CUDA contexts, not just the one that
*   performed the allocation.
*
* - ::cudaHostRegisterMapped: Maps the allocation into the CUDA address
*   space. The device pointer to the memory may be obtained by calling
*   ::cudaHostGetDevicePointer().
*
* - ::cudaHostRegisterIoMemory: The passed memory pointer is treated as
*   pointing to some memory-mapped I/O space, e.g. belonging to a
*   third-party PCIe device, and it will marked as non cache-coherent and
*   contiguous.
*
* - ::cudaHostRegisterReadOnly: The passed memory pointer is treated as
*   pointing to memory that is considered read-only by the device.  On
*   platforms without ::cudaDevAttrPageableMemoryAccessUsesHostPageTables, this
*   flag is required in order to register memory mapped to the CPU as
*   read-only.  Support for the use of this flag can be queried from the device
*   attribute cudaDeviceAttrReadOnlyHostRegisterSupported.  Using this flag with
*   a current context associated with a device that does not have this attribute
*   set will cause ::cudaHostRegister to error with cudaErrorNotSupported.
*
* All of these flags are orthogonal to one another: a developer may page-lock
* memory that is portable or mapped with no restrictions.
*
* The CUDA context must have been created with the ::cudaMapHost flag in
* order for the ::cudaHostRegisterMapped flag to have any effect.
*
* The ::cudaHostRegisterMapped flag may be specified on CUDA contexts for
* devices that do not support mapped pinned memory. The failure is deferred
* to ::cudaHostGetDevicePointer() because the memory may be mapped into
* other CUDA contexts via the ::cudaHostRegisterPortable flag.
*
* For devices that have a non-zero value for the device attribute
* ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
* can also be accessed from the device using the host pointer \p ptr.
* The device pointer returned by ::cudaHostGetDevicePointer() may or may not
* match the original host pointer \p ptr and depends on the devices visible to the
* application. If all devices visible to the application have a non-zero value for the
* device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
* will match the original pointer \p ptr. If any device visible to the application
* has a zero value for the device attribute, the device pointer returned by
* ::cudaHostGetDevicePointer() will not match the original host pointer \p ptr,
* but it will be suitable for use on all devices provided Unified Virtual Addressing
* is enabled. In such systems, it is valid to access the memory using either pointer
* on devices that have a non-zero value for the device attribute. Note however that
* such devices should access the memory using only of the two pointers and not both.
*
* The memory page-locked by this function must be unregistered with ::cudaHostUnregister().
*
* \param ptr   - Host pointer to memory to page-lock
* \param size  - Size in bytes of the address range to page-lock in bytes
* \param flags - Flags for allocation request
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation,
* ::cudaErrorHostMemoryAlreadyRegistered,
* ::cudaErrorNotSupported
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaHostUnregister, ::cudaHostGetFlags, ::cudaHostGetDevicePointer,
* ::cuMemHostRegister
*/
int handle_cudaHostRegister(void *conn) {
    void* ptr;
    size_t size;
    unsigned int flags;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaHostRegister(&ptr, size, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Unregisters a memory range that was registered with cudaHostRegister
*
* Unmaps the memory range whose base address is specified by \p ptr, and makes
* it pageable again.
*
* The base address must be the same one specified to ::cudaHostRegister().
*
* \param ptr - Host pointer to memory to unregister
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorHostMemoryNotRegistered
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaHostUnregister,
* ::cuMemHostUnregister
*/
int handle_cudaHostUnregister(void *conn) {
    void* ptr;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaHostUnregister(&ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Passes back device pointer of mapped host memory allocated by
* cudaHostAlloc or registered by cudaHostRegister
*
* Passes back the device pointer corresponding to the mapped, pinned host
* buffer allocated by ::cudaHostAlloc() or registered by ::cudaHostRegister().
*
* ::cudaHostGetDevicePointer() will fail if the ::cudaDeviceMapHost flag was
* not specified before deferred context creation occurred, or if called on a
* device that does not support mapped, pinned memory.
*
* For devices that have a non-zero value for the device attribute
* ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
* can also be accessed from the device using the host pointer \p pHost.
* The device pointer returned by ::cudaHostGetDevicePointer() may or may not
* match the original host pointer \p pHost and depends on the devices visible to the
* application. If all devices visible to the application have a non-zero value for the
* device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
* will match the original pointer \p pHost. If any device visible to the application
* has a zero value for the device attribute, the device pointer returned by
* ::cudaHostGetDevicePointer() will not match the original host pointer \p pHost,
* but it will be suitable for use on all devices provided Unified Virtual Addressing
* is enabled. In such systems, it is valid to access the memory using either pointer
* on devices that have a non-zero value for the device attribute. Note however that
* such devices should access the memory using only of the two pointers and not both.
*
* \p flags provides for future releases.  For now, it must be set to 0.
*
* \param pDevice - Returned device pointer for mapped memory
* \param pHost   - Requested host pointer mapping
* \param flags   - Flags for extensions (must be 0 for now)
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaSetDeviceFlags, ::cudaHostAlloc,
* ::cuMemHostGetDevicePointer
*/
int handle_cudaHostGetDevicePointer(void *conn) {
    void* pDevice;
    void* pHost;
    unsigned int flags;

    if (rpc_read(conn, &pDevice, sizeof(void*)) < 0 ||
        rpc_read(conn, &pHost, sizeof(void*)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaHostGetDevicePointer(&pDevice, &pHost, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pDevice, sizeof(void*)) < 0 ||
        rpc_write(conn, &pHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Passes back flags used to allocate pinned host memory allocated by
* cudaHostAlloc
*
* ::cudaHostGetFlags() will fail if the input pointer does not
* reside in an address range allocated by ::cudaHostAlloc().
*
* \param pFlags - Returned flags word
* \param pHost - Host pointer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaHostAlloc,
* ::cuMemHostGetFlags
*/
int handle_cudaHostGetFlags(void *conn) {
    unsigned int pFlags;
    void* pHost;

    if (rpc_read(conn, &pFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &pHost, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaHostGetFlags(&pFlags, &pHost);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &pHost, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates logical 1D, 2D, or 3D memory objects on the device
*
* Allocates at least \p width * \p height * \p depth bytes of linear memory
* on the device and returns a ::cudaPitchedPtr in which \p ptr is a pointer
* to the allocated memory. The function may pad the allocation to ensure
* hardware alignment requirements are met. The pitch returned in the \p pitch
* field of \p pitchedDevPtr is the width in bytes of the allocation.
*
* The returned ::cudaPitchedPtr contains additional fields \p xsize and
* \p ysize, the logical width and height of the allocation, which are
* equivalent to the \p width and \p height \p extent parameters provided by
* the programmer during allocation.
*
* For allocations of 2D and 3D objects, it is highly recommended that
* programmers perform allocations using ::cudaMalloc3D() or
* ::cudaMallocPitch(). Due to alignment restrictions in the hardware, this is
* especially true if the application will be performing memory copies
* involving 2D or 3D objects (whether linear memory or CUDA arrays).
*
* \param pitchedDevPtr  - Pointer to allocated pitched device memory
* \param extent         - Requested allocation size (\p width field in bytes)
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMallocPitch, ::cudaFree, ::cudaMemcpy3D, ::cudaMemset3D,
* ::cudaMalloc3DArray, ::cudaMallocArray, ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc, ::make_cudaPitchedPtr, ::make_cudaExtent,
* ::cuMemAllocPitch
*/
int handle_cudaMalloc3D(void *conn) {
    struct cudaPitchedPtr pitchedDevPtr;
    struct cudaExtent extent;

    if (rpc_read(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMalloc3D(&pitchedDevPtr, extent);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocate an array on the device
*
* Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
* \p desc and returns a handle to the new CUDA array in \p *array.
*
* The ::cudaChannelFormatDesc is defined as:
* \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
        enum cudaChannelFormatKind f;
    };
    \endcode
* where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
* ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
*
* ::cudaMalloc3DArray() can allocate the following:
*
* - A 1D array is allocated if the height and depth extents are both zero.
* - A 2D array is allocated if only the depth extent is zero.
* - A 3D array is allocated if all three extents are non-zero.
* - A 1D layered CUDA array is allocated if only the height extent is zero and
* the cudaArrayLayered flag is set. Each layer is a 1D array. The number of layers is 
* determined by the depth extent.
* - A 2D layered CUDA array is allocated if all three extents are non-zero and 
* the cudaArrayLayered flag is set. Each layer is a 2D array. The number of layers is 
* determined by the depth extent.
* - A cubemap CUDA array is allocated if all three extents are non-zero and the
* cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six. A cubemap is
* a special type of 2D layered CUDA array, where the six layers represent the six faces of a cube. 
* The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
* - A cubemap layered CUDA array is allocated if all three extents are non-zero, and both,
* cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be 
* a multiple of six. A cubemap layered CUDA array is a special type of 2D layered CUDA array that consists 
* of a collection of cubemaps. The first six layers represent the first cubemap, the next six layers form 
* the second cubemap, and so on.
*
*
* The \p flags parameter enables different options to be specified that affect
* the allocation, as follows.
* - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
* - ::cudaArrayLayered: Allocates a layered CUDA array, with the depth extent indicating the number of layers
* - ::cudaArrayCubemap: Allocates a cubemap CUDA array. Width must be equal to height, and depth must be six.
*   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
* - ::cudaArraySurfaceLoadStore: Allocates a CUDA array that could be read from or written to using a surface
*   reference.
* - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA 
*   array. Texture gather can only be performed on 2D CUDA arrays.
* - ::cudaArraySparse: Allocates a CUDA array without physical backing memory. The subregions within this sparse array 
*   can later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. This flag can only be used for 
*   creating 2D, 3D or 2D layered sparse CUDA arrays. The physical backing memory must be allocated via ::cuMemCreate.
* - ::cudaArrayDeferredMapping: Allocates a CUDA array without physical backing memory. The entire array can
*   later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. The physical backing memory must be allocated
*   via ::cuMemCreate.
*
* The width, height and depth extents must meet certain size requirements as listed in the following table.
* All values are specified in elements.
*
* Note that 2D CUDA arrays have different size requirements if the ::cudaArrayTextureGather flag is set. In that
* case, the valid range for (width, height, depth) is ((1,maxTexture2DGather[0]), (1,maxTexture2DGather[1]), 0).
*
* \xmlonly
* <table outputclass="xmlonly">
* <tgroup cols="3" colsep="1" rowsep="1">
* <colspec colname="c1" colwidth="1.0*"/>
* <colspec colname="c2" colwidth="3.0*"/>
* <colspec colname="c3" colwidth="3.0*"/>
* <thead>
* <row>
* <entry>CUDA array type</entry>
* <entry>Valid extents that must always be met {(width range in elements),
* (height range), (depth range)}</entry>
* <entry>Valid extents with cudaArraySurfaceLoadStore set {(width range in
* elements), (height range), (depth range)}</entry>
* </row>
* </thead>
* <tbody>
* <row>
* <entry>1D</entry>
* <entry>{ (1,maxTexture1D), 0, 0 }</entry>
* <entry>{ (1,maxSurface1D), 0, 0 }</entry>
* </row>
* <row>
* <entry>2D</entry>
* <entry>{ (1,maxTexture2D[0]), (1,maxTexture2D[1]), 0 }</entry>
* <entry>{ (1,maxSurface2D[0]), (1,maxSurface2D[1]), 0 }</entry>
* </row>
* <row>
* <entry>3D</entry>
* <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }
* OR { (1,maxTexture3DAlt[0]), (1,maxTexture3DAlt[1]),
* (1,maxTexture3DAlt[2]) }</entry>
* <entry>{ (1,maxSurface3D[0]), (1,maxSurface3D[1]), (1,maxSurface3D[2]) }</entry>
* </row>
* <row>
* <entry>1D Layered</entry>
* <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
* <entry>{ (1,maxSurface1DLayered[0]), 0, (1,maxSurface1DLayered[1]) }</entry>
* </row>
* <row>
* <entry>2D Layered</entry>
* <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
* (1,maxTexture2DLayered[2]) }</entry>
* <entry>{ (1,maxSurface2DLayered[0]), (1,maxSurface2DLayered[1]),
* (1,maxSurface2DLayered[2]) }</entry>
* </row>
* <row>
* <entry>Cubemap</entry>
* <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
* <entry>{ (1,maxSurfaceCubemap), (1,maxSurfaceCubemap), 6 }</entry>
* </row>
* <row>
* <entry>Cubemap Layered</entry>
* <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
* (1,maxTextureCubemapLayered[1]) }</entry>
* <entry>{ (1,maxSurfaceCubemapLayered[0]), (1,maxSurfaceCubemapLayered[0]),
* (1,maxSurfaceCubemapLayered[1]) }</entry>
* </row>
* </tbody>
* </tgroup>
* </table>
* \endxmlonly
*
* \param array  - Pointer to allocated array in device memory
* \param desc   - Requested channel format
* \param extent - Requested allocation size (\p width field in elements)
* \param flags  - Flags for extensions
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
* ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::make_cudaExtent,
* ::cuArray3DCreate
*/
int handle_cudaMalloc3DArray(void *conn) {
    cudaArray_t array;
    struct cudaChannelFormatDesc desc;
    struct cudaExtent extent;
    unsigned int flags = 0;

    if (rpc_read(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMalloc3DArray(&array, &desc, extent, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocate a mipmapped array on the device
*
* Allocates a CUDA mipmapped array according to the ::cudaChannelFormatDesc structure
* \p desc and returns a handle to the new CUDA mipmapped array in \p *mipmappedArray.
* \p numLevels specifies the number of mipmap levels to be allocated. This value is
* clamped to the range [1, 1 + floor(log2(max(width, height, depth)))].
*
* The ::cudaChannelFormatDesc is defined as:
* \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
        enum cudaChannelFormatKind f;
    };
    \endcode
* where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
* ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
*
* ::cudaMallocMipmappedArray() can allocate the following:
*
* - A 1D mipmapped array is allocated if the height and depth extents are both zero.
* - A 2D mipmapped array is allocated if only the depth extent is zero.
* - A 3D mipmapped array is allocated if all three extents are non-zero.
* - A 1D layered CUDA mipmapped array is allocated if only the height extent is zero and
* the cudaArrayLayered flag is set. Each layer is a 1D mipmapped array. The number of layers is 
* determined by the depth extent.
* - A 2D layered CUDA mipmapped array is allocated if all three extents are non-zero and 
* the cudaArrayLayered flag is set. Each layer is a 2D mipmapped array. The number of layers is 
* determined by the depth extent.
* - A cubemap CUDA mipmapped array is allocated if all three extents are non-zero and the
* cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six.
* The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
* - A cubemap layered CUDA mipmapped array is allocated if all three extents are non-zero, and both,
* cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be 
* a multiple of six. A cubemap layered CUDA mipmapped array is a special type of 2D layered CUDA mipmapped
* array that consists of a collection of cubemap mipmapped arrays. The first six layers represent the 
* first cubemap mipmapped array, the next six layers form the second cubemap mipmapped array, and so on.
*
*
* The \p flags parameter enables different options to be specified that affect
* the allocation, as follows.
* - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default mipmapped array allocation
* - ::cudaArrayLayered: Allocates a layered CUDA mipmapped array, with the depth extent indicating the number of layers
* - ::cudaArrayCubemap: Allocates a cubemap CUDA mipmapped array. Width must be equal to height, and depth must be six.
*   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
* - ::cudaArraySurfaceLoadStore: This flag indicates that individual mipmap levels of the CUDA mipmapped array 
*   will be read from or written to using a surface reference.
* - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA 
*   array. Texture gather can only be performed on 2D CUDA mipmapped arrays, and the gather operations are
*   performed only on the most detailed mipmap level.
* - ::cudaArraySparse: Allocates a CUDA mipmapped array without physical backing memory. The subregions within this sparse array
*   can later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. This flag can only be used for creating 
*   2D, 3D or 2D layered sparse CUDA mipmapped arrays. The physical backing memory must be allocated via ::cuMemCreate.
* - ::cudaArrayDeferredMapping: Allocates a CUDA mipmapped array without physical backing memory. The entire array can
*   later be mapped onto a physical memory allocation by calling ::cuMemMapArrayAsync. The physical backing memory must be allocated
*   via ::cuMemCreate.
*
* The width, height and depth extents must meet certain size requirements as listed in the following table.
* All values are specified in elements.
*
* \xmlonly
* <table outputclass="xmlonly">
* <tgroup cols="3" colsep="1" rowsep="1">
* <colspec colname="c1" colwidth="1.0*"/>
* <colspec colname="c2" colwidth="3.0*"/>
* <colspec colname="c3" colwidth="3.0*"/>
* <thead>
* <row>
* <entry>CUDA array type</entry>
* <entry>Valid extents that must always be met {(width range in elements),
* (height range), (depth range)}</entry>
* <entry>Valid extents with cudaArraySurfaceLoadStore set {(width range in
* elements), (height range), (depth range)}</entry>
* </row>
* </thead>
* <tbody>
* <row>
* <entry>1D</entry>
* <entry>{ (1,maxTexture1DMipmap), 0, 0 }</entry>
* <entry>{ (1,maxSurface1D), 0, 0 }</entry>
* </row>
* <row>
* <entry>2D</entry>
* <entry>{ (1,maxTexture2DMipmap[0]), (1,maxTexture2DMipmap[1]), 0 }</entry>
* <entry>{ (1,maxSurface2D[0]), (1,maxSurface2D[1]), 0 }</entry>
* </row>
* <row>
* <entry>3D</entry>
* <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }
* OR { (1,maxTexture3DAlt[0]), (1,maxTexture3DAlt[1]),
* (1,maxTexture3DAlt[2]) }</entry>
* <entry>{ (1,maxSurface3D[0]), (1,maxSurface3D[1]), (1,maxSurface3D[2]) }</entry>
* </row>
* <row>
* <entry>1D Layered</entry>
* <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
* <entry>{ (1,maxSurface1DLayered[0]), 0, (1,maxSurface1DLayered[1]) }</entry>
* </row>
* <row>
* <entry>2D Layered</entry>
* <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
* (1,maxTexture2DLayered[2]) }</entry>
* <entry>{ (1,maxSurface2DLayered[0]), (1,maxSurface2DLayered[1]),
* (1,maxSurface2DLayered[2]) }</entry>
* </row>
* <row>
* <entry>Cubemap</entry>
* <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
* <entry>{ (1,maxSurfaceCubemap), (1,maxSurfaceCubemap), 6 }</entry>
* </row>
* <row>
* <entry>Cubemap Layered</entry>
* <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
* (1,maxTextureCubemapLayered[1]) }</entry>
* <entry>{ (1,maxSurfaceCubemapLayered[0]), (1,maxSurfaceCubemapLayered[0]),
* (1,maxSurfaceCubemapLayered[1]) }</entry>
* </row>
* </tbody>
* </tgroup>
* </table>
* \endxmlonly
*
* \param mipmappedArray  - Pointer to allocated mipmapped array in device memory
* \param desc            - Requested channel format
* \param extent          - Requested allocation size (\p width field in elements)
* \param numLevels       - Number of mipmap levels to allocate
* \param flags           - Flags for extensions
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
* ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::make_cudaExtent,
* ::cuMipmappedArrayCreate
*/
int handle_cudaMallocMipmappedArray(void *conn) {
    cudaMipmappedArray_t mipmappedArray;
    struct cudaChannelFormatDesc desc;
    struct cudaExtent extent;
    unsigned int numLevels;
    unsigned int flags = 0;

    if (rpc_read(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_read(conn, &numLevels, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocMipmappedArray(&mipmappedArray, &desc, extent, numLevels, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a mipmap level of a CUDA mipmapped array
*
* Returns in \p *levelArray a CUDA array that represents a single mipmap level
* of the CUDA mipmapped array \p mipmappedArray.
*
* If \p level is greater than the maximum number of levels in this mipmapped array,
* ::cudaErrorInvalidValue is returned.
*
* If \p mipmappedArray is NULL,
* ::cudaErrorInvalidResourceHandle is returned.
*
* \param levelArray     - Returned mipmap level CUDA array
* \param mipmappedArray - CUDA mipmapped array
* \param level          - Mipmap level
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* ::cudaErrorInvalidResourceHandle
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
* ::cudaFreeArray,
* \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
* ::cudaFreeHost, ::cudaHostAlloc,
* ::make_cudaExtent,
* ::cuMipmappedArrayGetLevel
*/
int handle_cudaGetMipmappedArrayLevel(void *conn) {
    cudaArray_t levelArray;
    cudaMipmappedArray_const_t mipmappedArray;
    unsigned int level;

    if (rpc_read(conn, &levelArray, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &mipmappedArray, sizeof(cudaMipmappedArray_const_t)) < 0 ||
        rpc_read(conn, &level, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetMipmappedArrayLevel(&levelArray, mipmappedArray, level);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &levelArray, sizeof(cudaArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between 3D objects
*
\code
struct cudaExtent {
  size_t width;
  size_t height;
  size_t depth;
};
struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);
struct cudaPos {
  size_t x;
  size_t y;
  size_t z;
};
struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);
struct cudaMemcpy3DParms {
  cudaArray_t           srcArray;
  struct cudaPos        srcPos;
  struct cudaPitchedPtr srcPtr;
  cudaArray_t           dstArray;
  struct cudaPos        dstPos;
  struct cudaPitchedPtr dstPtr;
  struct cudaExtent     extent;
  enum cudaMemcpyKind   kind;
};
\endcode
*
* ::cudaMemcpy3D() copies data between two 3D objects. The source and
* destination objects may be in either host memory, device memory, or a CUDA
* array. The source, destination, extent, and kind of copy performed is
* specified by the ::cudaMemcpy3DParms struct which should be initialized to
* zero before use:
\code
cudaMemcpy3DParms myParms = {0};
\endcode
*
* The struct passed to ::cudaMemcpy3D() must specify one of \p srcArray or
* \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
* non-zero source or destination will cause ::cudaMemcpy3D() to return an
* error.
*
* The \p srcPos and \p dstPos fields are optional offsets into the source and
* destination objects and are defined in units of each object's elements. The
* element for a host or device pointer is assumed to be <b>unsigned char</b>.
*
* The \p extent field defines the dimensions of the transferred area in
* elements. If a CUDA array is participating in the copy, the extent is
* defined in terms of that array's elements. If no CUDA array is
* participating in the copy then the extents are defined in elements of
* <b>unsigned char</b>.
*
* The \p kind field defines the direction of the copy. It must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* For ::cudaMemcpyHostToHost or ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost
* passed as kind and cudaArray type passed as source or destination, if the kind
* implies cudaArray type to be present on the host, ::cudaMemcpy3D() will
* disregard that implication and silently correct the kind based on the fact that
* cudaArray type can only be present on the device.
*
* If the source and destination are both arrays, ::cudaMemcpy3D() will return
* an error if they do not have the same element size.
*
* The source and destination object may not overlap. If overlapping source
* and destination objects are specified, undefined behavior will result.
*
* The source object must entirely contain the region defined by \p srcPos
* and \p extent. The destination object must entirely contain the region
* defined by \p dstPos and \p extent.
*
* ::cudaMemcpy3D() returns an error if the pitch of \p srcPtr or \p dstPtr
* exceeds the maximum allowed. The pitch of a ::cudaPitchedPtr allocated
* with ::cudaMalloc3D() will always be valid.
*
* \param p - 3D memory copy parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3DAsync,
* ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::make_cudaExtent, ::make_cudaPos,
* ::cuMemcpy3D
*/
int handle_cudaMemcpy3D(void *conn) {
    struct cudaMemcpy3DParms p;

    if (rpc_read(conn, &p, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy3D(&p);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between devices
*
* Perform a 3D memory copy according to the parameters specified in
* \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
* for documentation of its parameters.
*
* Note that this function is synchronous with respect to the host only if
* the source or destination of the transfer is host memory.  Note also 
* that this copy is serialized with respect to all pending and future 
* asynchronous work in to the current device, the copy's source device,
* and the copy's destination device (use ::cudaMemcpy3DPeerAsync to avoid 
* this synchronization).
*
* \param p - Parameters for the memory copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidPitchValue
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
* ::cudaMemcpy3DPeerAsync,
* ::cuMemcpy3DPeer
*/
int handle_cudaMemcpy3DPeer(void *conn) {
    struct cudaMemcpy3DPeerParms p;

    if (rpc_read(conn, &p, sizeof(struct cudaMemcpy3DPeerParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy3DPeer(&p);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between 3D objects
*
\code
struct cudaExtent {
  size_t width;
  size_t height;
  size_t depth;
};
struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);
struct cudaPos {
  size_t x;
  size_t y;
  size_t z;
};
struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);
struct cudaMemcpy3DParms {
  cudaArray_t           srcArray;
  struct cudaPos        srcPos;
  struct cudaPitchedPtr srcPtr;
  cudaArray_t           dstArray;
  struct cudaPos        dstPos;
  struct cudaPitchedPtr dstPtr;
  struct cudaExtent     extent;
  enum cudaMemcpyKind   kind;
};
\endcode
*
* ::cudaMemcpy3DAsync() copies data between two 3D objects. The source and
* destination objects may be in either host memory, device memory, or a CUDA
* array. The source, destination, extent, and kind of copy performed is
* specified by the ::cudaMemcpy3DParms struct which should be initialized to
* zero before use:
\code
cudaMemcpy3DParms myParms = {0};
\endcode
*
* The struct passed to ::cudaMemcpy3DAsync() must specify one of \p srcArray
* or \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
* non-zero source or destination will cause ::cudaMemcpy3DAsync() to return an
* error.
*
* The \p srcPos and \p dstPos fields are optional offsets into the source and
* destination objects and are defined in units of each object's elements. The
* element for a host or device pointer is assumed to be <b>unsigned char</b>.
* For CUDA arrays, positions must be in the range [0, 2048) for any
* dimension.
*
* The \p extent field defines the dimensions of the transferred area in
* elements. If a CUDA array is participating in the copy, the extent is
* defined in terms of that array's elements. If no CUDA array is
* participating in the copy then the extents are defined in elements of
* <b>unsigned char</b>.
*
* The \p kind field defines the direction of the copy. It must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* For ::cudaMemcpyHostToHost or ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost
* passed as kind and cudaArray type passed as source or destination, if the kind
* implies cudaArray type to be present on the host, ::cudaMemcpy3DAsync() will
* disregard that implication and silently correct the kind based on the fact that
* cudaArray type can only be present on the device.
*
* If the source and destination are both arrays, ::cudaMemcpy3DAsync() will
* return an error if they do not have the same element size.
*
* The source and destination object may not overlap. If overlapping source
* and destination objects are specified, undefined behavior will result.
*
* The source object must lie entirely within the region defined by \p srcPos
* and \p extent. The destination object must lie entirely within the region
* defined by \p dstPos and \p extent.
*
* ::cudaMemcpy3DAsync() returns an error if the pitch of \p srcPtr or
* \p dstPtr exceeds the maximum allowed. The pitch of a
* ::cudaPitchedPtr allocated with ::cudaMalloc3D() will always be valid.
*
* ::cudaMemcpy3DAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If
* \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
* is non-zero, the copy may overlap with operations in other streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param p      - 3D memory copy parameters
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3D,
* ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, :::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::make_cudaExtent, ::make_cudaPos,
* ::cuMemcpy3DAsync
*/
int handle_cudaMemcpy3DAsync(void *conn) {
    struct cudaMemcpy3DParms p;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &p, sizeof(struct cudaMemcpy3DParms)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy3DAsync(&p, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between devices asynchronously.
*
* Perform a 3D memory copy according to the parameters specified in
* \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
* for documentation of its parameters.
*
* \param p      - Parameters for the memory copy
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidPitchValue
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
* ::cudaMemcpy3DPeerAsync,
* ::cuMemcpy3DPeerAsync
*/
int handle_cudaMemcpy3DPeerAsync(void *conn) {
    struct cudaMemcpy3DPeerParms p;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &p, sizeof(struct cudaMemcpy3DPeerParms)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy3DPeerAsync(&p, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Gets free and total device memory
*
* Returns in \p *total the total amount of memory available to the the current context.
* Returns in \p *free the amount of memory on the device that is free according to the OS.
* CUDA is not guaranteed to be able to allocate all of the memory that the OS reports as free.
* In a multi-tenet situation, free estimate returned is prone to race condition where
* a new allocation/free done by a different process or a different thread in the same
* process between the time when free memory was estimated and reported, will result in
* deviation in free value reported and actual free memory.
*
* The integrated GPU on Tegra shares memory with CPU and other component
* of the SoC. The free and total values returned by the API excludes
* the SWAP memory space maintained by the OS on some platforms.
* The OS may move some of the memory pages into swap area as the GPU or
* CPU allocate or access memory. See Tegra app note on how to calculate
* total and free memory on Tegra.
*
* \param free  - Returned free memory in bytes
* \param total - Returned total memory in bytes
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorLaunchFailure
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cuMemGetInfo
*/
int handle_cudaMemGetInfo(void *conn) {
    size_t free;
    size_t total;

    if (rpc_read(conn, &free, sizeof(size_t)) < 0 ||
        rpc_read(conn, &total, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemGetInfo(&free, &total);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &free, sizeof(size_t)) < 0 ||
        rpc_write(conn, &total, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets info about the specified cudaArray
* 
* Returns in \p *desc, \p *extent and \p *flags respectively, the type, shape 
* and flags of \p array.
*
* Any of \p *desc, \p *extent and \p *flags may be specified as NULL.
*
* \param desc   - Returned array type
* \param extent - Returned array shape. 2D arrays will have depth of zero
* \param flags  - Returned array flags
* \param array  - The ::cudaArray to get info for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cuArrayGetDescriptor,
* ::cuArray3DGetDescriptor
*/
int handle_cudaArrayGetInfo(void *conn) {
    struct cudaChannelFormatDesc desc;
    struct cudaExtent extent;
    unsigned int flags;
    cudaArray_t array;

    if (rpc_read(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaArrayGetInfo(&desc, &extent, &flags, array);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a CUDA array plane from a CUDA array
*
* Returns in \p pPlaneArray a CUDA array that represents a single format plane
* of the CUDA array \p hArray.
*
* If \p planeIdx is greater than the maximum number of planes in this array or if the array does
* not have a multi-planar format e.g: ::cudaChannelFormatKindNV12, then ::cudaErrorInvalidValue is returned.
*
* Note that if the \p hArray has format ::cudaChannelFormatKindNV12, then passing in 0 for \p planeIdx returns
* a CUDA array of the same size as \p hArray but with one 8-bit channel and ::cudaChannelFormatKindUnsigned as its format kind.
* If 1 is passed for \p planeIdx, then the returned CUDA array has half the height and width
* of \p hArray with two 8-bit channels and ::cudaChannelFormatKindUnsigned as its format kind.
*
* \param pPlaneArray   - Returned CUDA array referenced by the \p planeIdx
* \param hArray        - CUDA array
* \param planeIdx      - Plane index
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* ::cudaErrorInvalidResourceHandle
* \notefnerr
*
* \sa
* ::cuArrayGetPlane
*/
int handle_cudaArrayGetPlane(void *conn) {
    cudaArray_t pPlaneArray;
    cudaArray_t hArray;
    unsigned int planeIdx;

    if (rpc_read(conn, &pPlaneArray, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &hArray, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &planeIdx, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaArrayGetPlane(&pPlaneArray, hArray, planeIdx);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pPlaneArray, sizeof(cudaArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the memory requirements of a CUDA array
*
* Returns the memory requirements of a CUDA array in \p memoryRequirements
* If the CUDA array is not allocated with flag ::cudaArrayDeferredMapping
* ::cudaErrorInvalidValue will be returned.
*
* The returned value in ::cudaArrayMemoryRequirements::size
* represents the total size of the CUDA array.
* The returned value in ::cudaArrayMemoryRequirements::alignment
* represents the alignment necessary for mapping the CUDA array.
*
* \return
* ::cudaSuccess
* ::cudaErrorInvalidValue
*
* \param[out] memoryRequirements - Pointer to ::cudaArrayMemoryRequirements
* \param[in] array - CUDA array to get the memory requirements of
* \param[in] device - Device to get the memory requirements for
* \sa ::cudaMipmappedArrayGetMemoryRequirements
*/
int handle_cudaArrayGetMemoryRequirements(void *conn) {
    struct cudaArrayMemoryRequirements memoryRequirements;
    cudaArray_t array;
    int device;

    if (rpc_read(conn, &memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_read(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaArrayGetMemoryRequirements(&memoryRequirements, array, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the memory requirements of a CUDA mipmapped array
*
* Returns the memory requirements of a CUDA mipmapped array in \p memoryRequirements
* If the CUDA mipmapped array is not allocated with flag ::cudaArrayDeferredMapping
* ::cudaErrorInvalidValue will be returned.
*
* The returned value in ::cudaArrayMemoryRequirements::size
* represents the total size of the CUDA mipmapped array.
* The returned value in ::cudaArrayMemoryRequirements::alignment
* represents the alignment necessary for mapping the CUDA mipmapped
* array.
*
* \return
* ::cudaSuccess
* ::cudaErrorInvalidValue
*
* \param[out] memoryRequirements - Pointer to ::cudaArrayMemoryRequirements
* \param[in] mipmap - CUDA mipmapped array to get the memory requirements of
* \param[in] device - Device to get the memory requirements for
* \sa ::cudaArrayGetMemoryRequirements
*/
int handle_cudaMipmappedArrayGetMemoryRequirements(void *conn) {
    struct cudaArrayMemoryRequirements memoryRequirements;
    cudaMipmappedArray_t mipmap;
    int device;

    if (rpc_read(conn, &memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_read(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMipmappedArrayGetMemoryRequirements(&memoryRequirements, mipmap, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0)
        return -1;

    return result;
}

int handle_cudaArrayGetSparseProperties(void *conn) {
    struct cudaArraySparseProperties sparseProperties;
    cudaArray_t array;

    if (rpc_read(conn, &sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_read(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaArrayGetSparseProperties(&sparseProperties, array);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0)
        return -1;

    return result;
}

int handle_cudaMipmappedArrayGetSparseProperties(void *conn) {
    struct cudaArraySparseProperties sparseProperties;
    cudaMipmappedArray_t mipmap;

    if (rpc_read(conn, &sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_read(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMipmappedArrayGetSparseProperties(&sparseProperties, mipmap);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies \p count bytes from the memory area pointed to by \p src to the
* memory area pointed to by \p dst, where \p kind specifies the direction
* of the copy, and must be one of ::cudaMemcpyHostToHost,
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing. Calling
* ::cudaMemcpy() with dst and src pointers that do not match the direction of
* the copy results in an undefined behavior.
*
* \param dst   - Destination memory address
* \param src   - Source memory address
* \param count - Size in bytes to copy
* \param kind  - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_init_rt
* \note_callback
*
* \note_sync
* \note_memcpy
*
* \sa ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyDtoH,
* ::cuMemcpyHtoD,
* ::cuMemcpyDtoD,
* ::cuMemcpy
*/
int handle_cudaMemcpy(void *conn) {
    void* dst;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy(&dst, &src, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between two devices
*
* Copies memory from one device to memory on another device.  \p dst is the 
* base device pointer of the destination memory and \p dstDevice is the 
* destination device.  \p src is the base device pointer of the source memory 
* and \p srcDevice is the source device.  \p count specifies the number of bytes 
* to copy.
*
* Note that this function is asynchronous with respect to the host, but 
* serialized with respect all pending and future asynchronous work in to the 
* current device, \p srcDevice, and \p dstDevice (use ::cudaMemcpyPeerAsync 
* to avoid this synchronization).
*
* \param dst       - Destination device pointer
* \param dstDevice - Destination device
* \param src       - Source device pointer
* \param srcDevice - Source device
* \param count     - Size of memory copy in bytes
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
* ::cudaMemcpy3DPeerAsync,
* ::cuMemcpyPeer
*/
int handle_cudaMemcpyPeer(void *conn) {
    void* dst;
    int dstDevice;
    void* src;
    int srcDevice;
    size_t count;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyPeer(&dst, dstDevice, &src, srcDevice, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the memory
* area pointed to by \p src to the memory area pointed to by \p dst, where
* \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing. \p dpitch and
* \p spitch are the widths in memory in bytes of the 2D arrays pointed to by
* \p dst and \p src, including any padding added to the end of each row. The
* memory areas may not overlap. \p width must not exceed either \p dpitch or
* \p spitch. Calling ::cudaMemcpy2D() with \p dst and \p src pointers that do
* not match the direction of the copy results in an undefined behavior.
* ::cudaMemcpy2D() returns an error if \p dpitch or \p spitch exceeds
* the maximum allowed.
*
* \param dst    - Destination memory address
* \param dpitch - Pitch of destination memory
* \param src    - Source memory address
* \param spitch - Pitch of source memory
* \param width  - Width of matrix transfer (columns in bytes)
* \param height - Height of matrix transfer (rows)
* \param kind   - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2D,
* ::cuMemcpy2DUnaligned
*/
int handle_cudaMemcpy2D(void *conn) {
    void* dst;
    size_t dpitch;
    void* src;
    size_t spitch;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dpitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2D(&dst, dpitch, &src, spitch, width, height, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the memory
* area pointed to by \p src to the CUDA array \p dst starting at
* \p hOffset rows and \p wOffset bytes from the upper left corner,
* where \p kind specifies the direction of the copy, and must be one
* of ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* \p spitch is the width in memory in bytes of the 2D array pointed to by
* \p src, including any padding added to the end of each row. \p wOffset +
* \p width must not exceed the width of the CUDA array \p dst. \p width must
* not exceed \p spitch. ::cudaMemcpy2DToArray() returns an error if \p spitch
* exceeds the maximum allowed.
*
* \param dst     - Destination memory address
* \param wOffset - Destination starting X offset (columns in bytes)
* \param hOffset - Destination starting Y offset (rows)
* \param src     - Source memory address
* \param spitch  - Pitch of source memory
* \param width   - Width of matrix transfer (columns in bytes)
* \param height  - Height of matrix transfer (rows)
* \param kind    - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2D,
* ::cuMemcpy2DUnaligned
*/
int handle_cudaMemcpy2DToArray(void *conn) {
    cudaArray_t dst;
    size_t wOffset;
    size_t hOffset;
    void* src;
    size_t spitch;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DToArray(dst, wOffset, hOffset, &src, spitch, width, height, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the CUDA
* array \p src starting at \p hOffset rows and \p wOffset bytes from the
* upper left corner to the memory area pointed to by \p dst, where
* \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing. \p dpitch is the
* width in memory in bytes of the 2D array pointed to by \p dst, including any
* padding added to the end of each row. \p wOffset + \p width must not exceed
* the width of the CUDA array \p src. \p width must not exceed \p dpitch.
* ::cudaMemcpy2DFromArray() returns an error if \p dpitch exceeds the maximum
* allowed.
*
* \param dst     - Destination memory address
* \param dpitch  - Pitch of destination memory
* \param src     - Source memory address
* \param wOffset - Source starting X offset (columns in bytes)
* \param hOffset - Source starting Y offset (rows)
* \param width   - Width of matrix transfer (columns in bytes)
* \param height  - Height of matrix transfer (rows)
* \param kind    - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2D,
* ::cuMemcpy2DUnaligned
*/
int handle_cudaMemcpy2DFromArray(void *conn) {
    void* dst;
    size_t dpitch;
    cudaArray_const_t src;
    size_t wOffset;
    size_t hOffset;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dpitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DFromArray(&dst, dpitch, src, wOffset, hOffset, width, height, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the CUDA
* array \p src starting at \p hOffsetSrc rows and \p wOffsetSrc bytes from the
* upper left corner to the CUDA array \p dst starting at \p hOffsetDst rows
* and \p wOffsetDst bytes from the upper left corner, where \p kind
* specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* \p wOffsetDst + \p width must not exceed the width of the CUDA array \p dst.
* \p wOffsetSrc + \p width must not exceed the width of the CUDA array \p src.
*
* \param dst        - Destination memory address
* \param wOffsetDst - Destination starting X offset (columns in bytes)
* \param hOffsetDst - Destination starting Y offset (rows)
* \param src        - Source memory address
* \param wOffsetSrc - Source starting X offset (columns in bytes)
* \param hOffsetSrc - Source starting Y offset (rows)
* \param width      - Width of matrix transfer (columns in bytes)
* \param height     - Height of matrix transfer (rows)
* \param kind       - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2D,
* ::cuMemcpy2DUnaligned
*/
int handle_cudaMemcpy2DArrayToArray(void *conn) {
    cudaArray_t dst;
    size_t wOffsetDst;
    size_t hOffsetDst;
    cudaArray_const_t src;
    size_t wOffsetSrc;
    size_t hOffsetSrc;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind = cudaMemcpyDeviceToDevice;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffsetDst, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffsetDst, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DArrayToArray(dst, wOffsetDst, hOffsetDst, src, wOffsetSrc, hOffsetSrc, width, height, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data to the given symbol on the device
*
* Copies \p count bytes from the memory area pointed to by \p src
* to the memory area pointed to by \p offset bytes from the start of symbol
* \p symbol. The memory areas may not overlap. \p symbol is a variable that
* resides in global or constant memory space. \p kind can be either
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
* Passing ::cudaMemcpyDefault is recommended, in which case the type of
* transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
* is only allowed on systems that support unified virtual addressing.
*
* \param symbol - Device symbol address
* \param src    - Source memory address
* \param count  - Size in bytes to copy
* \param offset - Offset from start of symbol in bytes
* \param kind   - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_sync
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray,  ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy,
* ::cuMemcpyHtoD,
* ::cuMemcpyDtoD
*/
int handle_cudaMemcpyToSymbol(void *conn) {
    void* symbol;
    void* src;
    size_t count;
    size_t offset = 0;
    enum cudaMemcpyKind kind = cudaMemcpyHostToDevice;

    if (rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyToSymbol(&symbol, &src, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data from the given symbol on the device
*
* Copies \p count bytes from the memory area pointed to by \p offset bytes
* from the start of symbol \p symbol to the memory area pointed to by \p dst.
* The memory areas may not overlap. \p symbol is a variable that
* resides in global or constant memory space. \p kind can be either
* ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
* Passing ::cudaMemcpyDefault is recommended, in which case the type of
* transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
* is only allowed on systems that support unified virtual addressing.
*
* \param dst    - Destination memory address
* \param symbol - Device symbol address
* \param count  - Size in bytes to copy
* \param offset - Offset from start of symbol in bytes
* \param kind   - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_sync
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy,
* ::cuMemcpyDtoH,
* ::cuMemcpyDtoD
*/
int handle_cudaMemcpyFromSymbol(void *conn) {
    void* dst;
    void* symbol;
    size_t count;
    size_t offset = 0;
    enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyFromSymbol(&dst, &symbol, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies \p count bytes from the memory area pointed to by \p src to the
* memory area pointed to by \p dst, where \p kind specifies the
* direction of the copy, and must be one of ::cudaMemcpyHostToHost,
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* 
* The memory areas may not overlap. Calling ::cudaMemcpyAsync() with \p dst and
* \p src pointers that do not match the direction of the copy results in an
* undefined behavior.
*
* ::cudaMemcpyAsync() is asynchronous with respect to the host, so the call
* may return before the copy is complete. The copy can optionally be
* associated to a stream by passing a non-zero \p stream argument. If \p kind
* is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and the \p stream is
* non-zero, the copy may overlap with operations in other streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param dst    - Destination memory address
* \param src    - Source memory address
* \param count  - Size in bytes to copy
* \param kind   - Type of transfer
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyAsync,
* ::cuMemcpyDtoHAsync,
* ::cuMemcpyHtoDAsync,
* ::cuMemcpyDtoDAsync
*/
int handle_cudaMemcpyAsync(void *conn) {
    void* dst;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyAsync(&dst, &src, count, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies memory between two devices asynchronously.
*
* Copies memory from one device to memory on another device.  \p dst is the 
* base device pointer of the destination memory and \p dstDevice is the 
* destination device.  \p src is the base device pointer of the source memory 
* and \p srcDevice is the source device.  \p count specifies the number of bytes 
* to copy.
*
* Note that this function is asynchronous with respect to the host and all work
* on other devices.
*
* \param dst       - Destination device pointer
* \param dstDevice - Destination device
* \param src       - Source device pointer
* \param srcDevice - Source device
* \param count     - Size of memory copy in bytes
* \param stream    - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
* ::cudaMemcpy3DPeerAsync,
* ::cuMemcpyPeerAsync
*/
int handle_cudaMemcpyPeerAsync(void *conn) {
    void* dst;
    int dstDevice;
    void* src;
    int srcDevice;
    size_t count;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &srcDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyPeerAsync(&dst, dstDevice, &src, srcDevice, count, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the memory
* area pointed to by \p src to the memory area pointed to by \p dst, where
* \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* \p dpitch and \p spitch are the widths in memory in bytes of the 2D arrays
* pointed to by \p dst and \p src, including any padding added to the end of
* each row. The memory areas may not overlap. \p width must not exceed either
* \p dpitch or \p spitch.
*
* Calling ::cudaMemcpy2DAsync() with \p dst and \p src pointers that do not
* match the direction of the copy results in an undefined behavior.
* ::cudaMemcpy2DAsync() returns an error if \p dpitch or \p spitch is greater
* than the maximum allowed.
*
* ::cudaMemcpy2DAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If
* \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
* \p stream is non-zero, the copy may overlap with operations in other
* streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param dst    - Destination memory address
* \param dpitch - Pitch of destination memory
* \param src    - Source memory address
* \param spitch - Pitch of source memory
* \param width  - Width of matrix transfer (columns in bytes)
* \param height - Height of matrix transfer (rows)
* \param kind   - Type of transfer
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2DAsync
*/
int handle_cudaMemcpy2DAsync(void *conn) {
    void* dst;
    size_t dpitch;
    void* src;
    size_t spitch;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dpitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DAsync(&dst, dpitch, &src, spitch, width, height, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the memory
* area pointed to by \p src to the CUDA array \p dst starting at \p hOffset
* rows and \p wOffset bytes from the upper left corner, where \p kind specifies
* the direction of the copy, and must be one of ::cudaMemcpyHostToHost,
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* \p spitch is the width in memory in bytes of the 2D array pointed to by
* \p src, including any padding added to the end of each row. \p wOffset +
* \p width must not exceed the width of the CUDA array \p dst. \p width must
* not exceed \p spitch. ::cudaMemcpy2DToArrayAsync() returns an error if
* \p spitch exceeds the maximum allowed.
*
* ::cudaMemcpy2DToArrayAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If
* \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
* \p stream is non-zero, the copy may overlap with operations in other
* streams.
*
* \param dst     - Destination memory address
* \param wOffset - Destination starting X offset (columns in bytes)
* \param hOffset - Destination starting Y offset (rows)
* \param src     - Source memory address
* \param spitch  - Pitch of source memory
* \param width   - Width of matrix transfer (columns in bytes)
* \param height  - Height of matrix transfer (rows)
* \param kind    - Type of transfer
* \param stream  - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
*
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2DAsync
*/
int handle_cudaMemcpy2DToArrayAsync(void *conn) {
    cudaArray_t dst;
    size_t wOffset;
    size_t hOffset;
    void* src;
    size_t spitch;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DToArrayAsync(dst, wOffset, hOffset, &src, spitch, width, height, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* Copies a matrix (\p height rows of \p width bytes each) from the CUDA
* array \p src starting at \p hOffset rows and \p wOffset bytes from the
* upper left corner to the memory area pointed to by \p dst,
* where \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
* \p dpitch is the width in memory in bytes of the 2D
* array pointed to by \p dst, including any padding added to the end of each
* row. \p wOffset + \p width must not exceed the width of the CUDA array
* \p src. \p width must not exceed \p dpitch. ::cudaMemcpy2DFromArrayAsync()
* returns an error if \p dpitch exceeds the maximum allowed.
*
* ::cudaMemcpy2DFromArrayAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally be
* associated to a stream by passing a non-zero \p stream argument. If \p kind
* is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream is
* non-zero, the copy may overlap with operations in other streams.
*
* \param dst     - Destination memory address
* \param dpitch  - Pitch of destination memory
* \param src     - Source memory address
* \param wOffset - Source starting X offset (columns in bytes)
* \param hOffset - Source starting Y offset (rows)
* \param width   - Width of matrix transfer (columns in bytes)
* \param height  - Height of matrix transfer (rows)
* \param kind    - Type of transfer
* \param stream  - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidPitchValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
* \note_memcpy
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
*
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpy2DAsync
*/
int handle_cudaMemcpy2DFromArrayAsync(void *conn) {
    void* dst;
    size_t dpitch;
    cudaArray_const_t src;
    size_t wOffset;
    size_t hOffset;
    size_t width;
    size_t height;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &dpitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpy2DFromArrayAsync(&dst, dpitch, src, wOffset, hOffset, width, height, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data to the given symbol on the device
*
* Copies \p count bytes from the memory area pointed to by \p src
* to the memory area pointed to by \p offset bytes from the start of symbol
* \p symbol. The memory areas may not overlap. \p symbol is a variable that
* resides in global or constant memory space. \p kind can be either
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
* Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
* is inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* ::cudaMemcpyToSymbolAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If
* \p kind is ::cudaMemcpyHostToDevice and \p stream is non-zero, the copy
* may overlap with operations in other streams.
*
* \param symbol - Device symbol address
* \param src    - Source memory address
* \param count  - Size in bytes to copy
* \param offset - Offset from start of symbol in bytes
* \param kind   - Type of transfer
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_async
* \note_null_stream
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyAsync,
* ::cuMemcpyHtoDAsync,
* ::cuMemcpyDtoDAsync
*/
int handle_cudaMemcpyToSymbolAsync(void *conn) {
    void* symbol;
    void* src;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyToSymbolAsync(&symbol, &src, count, offset, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data from the given symbol on the device
*
* Copies \p count bytes from the memory area pointed to by \p offset bytes
* from the start of symbol \p symbol to the memory area pointed to by \p dst.
* The memory areas may not overlap. \p symbol is a variable that resides in
* global or constant memory space. \p kind can be either
* ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
* Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
* is inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* ::cudaMemcpyFromSymbolAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally be
* associated to a stream by passing a non-zero \p stream argument. If \p kind
* is ::cudaMemcpyDeviceToHost and \p stream is non-zero, the copy may overlap
* with operations in other streams.
*
* \param dst    - Destination memory address
* \param symbol - Device symbol address
* \param count  - Size in bytes to copy
* \param offset - Offset from start of symbol in bytes
* \param kind   - Type of transfer
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorInvalidMemcpyDirection,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_async
* \note_null_stream
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync,
* ::cuMemcpyAsync,
* ::cuMemcpyDtoHAsync,
* ::cuMemcpyDtoDAsync
*/
int handle_cudaMemcpyFromSymbolAsync(void *conn) {
    void* dst;
    void* symbol;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyFromSymbolAsync(&dst, &symbol, count, offset, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Fills the first \p count bytes of the memory area pointed to by \p devPtr
* with the constant byte value \p value.
*
* Note that this function is asynchronous with respect to the host unless
* \p devPtr refers to pinned host memory.
*
* \param devPtr - Pointer to device memory
* \param value  - Value to set for each byte of specified memory
* \param count  - Size in bytes to set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_init_rt
* \note_callback
*
* \sa
* ::cuMemsetD8,
* ::cuMemsetD16,
* ::cuMemsetD32
*/
int handle_cudaMemset(void *conn) {
    void* devPtr;
    int value;
    size_t count;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemset(&devPtr, value, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Sets to the specified value \p value a matrix (\p height rows of \p width
* bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
* 2D array pointed to by \p dstPtr, including any padding added to the end
* of each row. This function performs fastest when the pitch is one that has
* been passed back by ::cudaMallocPitch().
*
* Note that this function is asynchronous with respect to the host unless
* \p devPtr refers to pinned host memory.
*
* \param devPtr - Pointer to 2D device memory
* \param pitch  - Pitch in bytes of 2D device memory(Unused if \p height is 1)
* \param value  - Value to set for each byte of specified memory
* \param width  - Width of matrix set (columns in bytes)
* \param height - Height of matrix set (rows)
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemset, ::cudaMemset3D, ::cudaMemsetAsync,
* ::cudaMemset2DAsync, ::cudaMemset3DAsync,
* ::cuMemsetD2D8,
* ::cuMemsetD2D16,
* ::cuMemsetD2D32
*/
int handle_cudaMemset2D(void *conn) {
    void* devPtr;
    size_t pitch;
    int value;
    size_t width;
    size_t height;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &pitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemset2D(&devPtr, pitch, value, width, height);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Initializes each element of a 3D array to the specified value \p value.
* The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
* of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
* to by \p pitchedDevPtr, including any padding added to the end of each row.
* The \p xsize field specifies the logical width of each row in bytes, while
* the \p ysize field specifies the height of each 2D slice in rows.
* The \p pitch field of \p pitchedDevPtr is ignored when \p height and \p depth 
* are both equal to 1. 
*
* The extents of the initialized region are specified as a \p width in bytes,
* a \p height in rows, and a \p depth in slices.
*
* Extents with \p width greater than or equal to the \p xsize of
* \p pitchedDevPtr may perform significantly faster than extents narrower
* than the \p xsize. Secondarily, extents with \p height equal to the
* \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
* shorter than the \p ysize.
*
* This function performs fastest when the \p pitchedDevPtr has been allocated
* by ::cudaMalloc3D().
*
* Note that this function is asynchronous with respect to the host unless
* \p pitchedDevPtr refers to pinned host memory.
*
* \param pitchedDevPtr - Pointer to pitched device memory
* \param value         - Value to set for each byte of specified memory
* \param extent        - Size parameters for where to set device memory (\p width field in bytes)
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemset, ::cudaMemset2D,
* ::cudaMemsetAsync, ::cudaMemset2DAsync, ::cudaMemset3DAsync,
* ::cudaMalloc3D, ::make_cudaPitchedPtr,
* ::make_cudaExtent
*/
int handle_cudaMemset3D(void *conn) {
    struct cudaPitchedPtr pitchedDevPtr;
    int value;
    struct cudaExtent extent;

    if (rpc_read(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemset3D(pitchedDevPtr, value, extent);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Fills the first \p count bytes of the memory area pointed to by \p devPtr
* with the constant byte value \p value.
*
* ::cudaMemsetAsync() is asynchronous with respect to the host, so
* the call may return before the memset is complete. The operation can optionally
* be associated to a stream by passing a non-zero \p stream argument.
* If \p stream is non-zero, the operation may overlap with operations in other streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param devPtr - Pointer to device memory
* \param value  - Value to set for each byte of specified memory
* \param count  - Size in bytes to set
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
* ::cudaMemset2DAsync, ::cudaMemset3DAsync,
* ::cuMemsetD8Async,
* ::cuMemsetD16Async,
* ::cuMemsetD32Async
*/
int handle_cudaMemsetAsync(void *conn) {
    void* devPtr;
    int value;
    size_t count;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemsetAsync(&devPtr, value, count, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Sets to the specified value \p value a matrix (\p height rows of \p width
* bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
* 2D array pointed to by \p dstPtr, including any padding added to the end
* of each row. This function performs fastest when the pitch is one that has
* been passed back by ::cudaMallocPitch().
*
* ::cudaMemset2DAsync() is asynchronous with respect to the host, so
* the call may return before the memset is complete. The operation can optionally
* be associated to a stream by passing a non-zero \p stream argument.
* If \p stream is non-zero, the operation may overlap with operations in other streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param devPtr - Pointer to 2D device memory
* \param pitch  - Pitch in bytes of 2D device memory(Unused if \p height is 1)
* \param value  - Value to set for each byte of specified memory
* \param width  - Width of matrix set (columns in bytes)
* \param height - Height of matrix set (rows)
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
* ::cudaMemsetAsync, ::cudaMemset3DAsync,
* ::cuMemsetD2D8Async,
* ::cuMemsetD2D16Async,
* ::cuMemsetD2D32Async
*/
int handle_cudaMemset2DAsync(void *conn) {
    void* devPtr;
    size_t pitch;
    int value;
    size_t width;
    size_t height;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &pitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &width, sizeof(size_t)) < 0 ||
        rpc_read(conn, &height, sizeof(size_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemset2DAsync(&devPtr, pitch, value, width, height, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Initializes or sets device memory to a value
*
* Initializes each element of a 3D array to the specified value \p value.
* The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
* of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
* to by \p pitchedDevPtr, including any padding added to the end of each row.
* The \p xsize field specifies the logical width of each row in bytes, while
* the \p ysize field specifies the height of each 2D slice in rows.
* The \p pitch field of \p pitchedDevPtr is ignored when \p height and \p depth 
* are both equal to 1. 
*
* The extents of the initialized region are specified as a \p width in bytes,
* a \p height in rows, and a \p depth in slices.
*
* Extents with \p width greater than or equal to the \p xsize of
* \p pitchedDevPtr may perform significantly faster than extents narrower
* than the \p xsize. Secondarily, extents with \p height equal to the
* \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
* shorter than the \p ysize.
*
* This function performs fastest when the \p pitchedDevPtr has been allocated
* by ::cudaMalloc3D().
*
* ::cudaMemset3DAsync() is asynchronous with respect to the host, so
* the call may return before the memset is complete. The operation can optionally
* be associated to a stream by passing a non-zero \p stream argument.
* If \p stream is non-zero, the operation may overlap with operations in other streams.
*
* The device version of this function only handles device to device copies and
* cannot be given local or shared pointers.
*
* \param pitchedDevPtr - Pointer to pitched device memory
* \param value         - Value to set for each byte of specified memory
* \param extent        - Size parameters for where to set device memory (\p width field in bytes)
* \param stream - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \notefnerr
* \note_memset
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
* ::cudaMemsetAsync, ::cudaMemset2DAsync,
* ::cudaMalloc3D, ::make_cudaPitchedPtr,
* ::make_cudaExtent
*/
int handle_cudaMemset3DAsync(void *conn) {
    struct cudaPitchedPtr pitchedDevPtr;
    int value;
    struct cudaExtent extent;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_read(conn, &value, sizeof(int)) < 0 ||
        rpc_read(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemset3DAsync(pitchedDevPtr, value, extent, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Finds the address associated with a CUDA symbol
*
* Returns in \p *devPtr the address of symbol \p symbol on the device.
* \p symbol is a variable that resides in global or constant memory space.
* If \p symbol cannot be found, or if \p symbol is not declared in the
* global or constant memory space, \p *devPtr is unchanged and the error
* ::cudaErrorInvalidSymbol is returned.
*
* \param devPtr - Return device pointer associated with symbol
* \param symbol - Device symbol address
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaGetSymbolAddress(void**, const T&) "cudaGetSymbolAddress (C++ API)",
* \ref ::cudaGetSymbolSize(size_t*, const void*) "cudaGetSymbolSize (C API)",
* ::cuModuleGetGlobal
*/
int handle_cudaGetSymbolAddress(void *conn) {
    void* devPtr;
    void* symbol;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetSymbolAddress(&devPtr, &symbol);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Finds the size of the object associated with a CUDA symbol
*
* Returns in \p *size the size of symbol \p symbol. \p symbol is a variable that
* resides in global or constant memory space. If \p symbol cannot be found, or
* if \p symbol is not declared in global or constant memory space, \p *size is
* unchanged and the error ::cudaErrorInvalidSymbol is returned.
*
* \param size   - Size of object associated with symbol
* \param symbol - Device symbol address
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidSymbol,
* ::cudaErrorNoKernelImageForDevice
* \notefnerr
* \note_string_api_deprecation
* \note_init_rt
* \note_callback
*
* \sa
* \ref ::cudaGetSymbolAddress(void**, const void*) "cudaGetSymbolAddress (C API)",
* \ref ::cudaGetSymbolSize(size_t*, const T&) "cudaGetSymbolSize (C++ API)",
* ::cuModuleGetGlobal
*/
int handle_cudaGetSymbolSize(void *conn) {
    size_t size;
    void* symbol;

    if (rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetSymbolSize(&size, &symbol);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &size, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Prefetches memory to the specified destination device
*
* Prefetches memory to the specified destination device.  \p devPtr is the 
* base device pointer of the memory to be prefetched and \p dstDevice is the 
* destination device. \p count specifies the number of bytes to copy. \p stream
* is the stream in which the operation is enqueued. The memory range must refer
* to managed memory allocated via ::cudaMallocManaged or declared via __managed__ variables.
*
* Passing in cudaCpuDeviceId for \p dstDevice will prefetch the data to host memory. If
* \p dstDevice is a GPU, then the device attribute ::cudaDevAttrConcurrentManagedAccess
* must be non-zero. Additionally, \p stream must be associated with a device that has a
* non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
*
* The start address and end address of the memory range will be rounded down and rounded up
* respectively to be aligned to CPU page size before the prefetch operation is enqueued
* in the stream.
*
* If no physical memory has been allocated for this region, then this memory region
* will be populated and mapped on the destination device. If there's insufficient
* memory to prefetch the desired region, the Unified Memory driver may evict pages from other
* ::cudaMallocManaged allocations to host memory in order to make room. Device memory
* allocated using ::cudaMalloc or ::cudaMallocArray will not be evicted.
*
* By default, any mappings to the previous location of the migrated pages are removed and
* mappings for the new location are only setup on \p dstDevice. The exact behavior however
* also depends on the settings applied to this memory range via ::cudaMemAdvise as described
* below:
*
* If ::cudaMemAdviseSetReadMostly was set on any subset of this memory range,
* then that subset will create a read-only copy of the pages on \p dstDevice.
*
* If ::cudaMemAdviseSetPreferredLocation was called on any subset of this memory
* range, then the pages will be migrated to \p dstDevice even if \p dstDevice is not the
* preferred location of any pages in the memory range.
*
* If ::cudaMemAdviseSetAccessedBy was called on any subset of this memory range,
* then mappings to those pages from all the appropriate processors are updated to
* refer to the new location if establishing such a mapping is possible. Otherwise,
* those mappings are cleared.
*
* Note that this API is not required for functionality and only serves to improve performance
* by allowing the application to migrate data to a suitable location before it is accessed.
* Memory accesses to this range are always coherent and are allowed even when the data is
* actively being migrated.
*
* Note that this function is asynchronous with respect to the host and all work
* on other devices.
*
* \param devPtr    - Pointer to be prefetched
* \param count     - Size in bytes
* \param dstDevice - Destination device to prefetch to
* \param stream    - Stream to enqueue prefetch operation
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
* ::cudaMemcpy3DPeerAsync, ::cudaMemAdvise,
* ::cuMemPrefetchAsync
*/
int handle_cudaMemPrefetchAsync(void *conn) {
    void* devPtr;
    size_t count;
    int dstDevice;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dstDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPrefetchAsync(&devPtr, count, dstDevice, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Advise about the usage of a given memory range
*
* Advise the Unified Memory subsystem about the usage pattern for the memory range
* starting at \p devPtr with a size of \p count bytes. The start address and end address of the memory
* range will be rounded down and rounded up respectively to be aligned to CPU page size before the
* advice is applied. The memory range must refer to managed memory allocated via ::cudaMallocManaged
* or declared via __managed__ variables. The memory range could also refer to system-allocated pageable
* memory provided it represents a valid, host-accessible region of memory and all additional constraints
* imposed by \p advice as outlined below are also satisfied. Specifying an invalid system-allocated pageable
* memory range results in an error being returned.
*
* The \p advice parameter can take the following values:
* - ::cudaMemAdviseSetReadMostly: This implies that the data is mostly going to be read
* from and only occasionally written to. Any read accesses from any processor to this region will create a
* read-only copy of at least the accessed pages in that processor's memory. Additionally, if ::cudaMemPrefetchAsync
* is called on this region, it will create a read-only copy of the data on the destination processor.
* If any processor writes to this region, all copies of the corresponding page will be invalidated
* except for the one where the write occurred. The \p device argument is ignored for this advice.
* Note that for a page to be read-duplicated, the accessing processor must either be the CPU or a GPU
* that has a non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
* Also, if a context is created on a device that does not have the device attribute
* ::cudaDevAttrConcurrentManagedAccess set, then read-duplication will not occur until
* all such contexts are destroyed.
* If the memory region refers to valid system-allocated pageable memory, then the accessing device must
* have a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccess for a read-only
* copy to be created on that device. Note however that if the accessing device also has a non-zero value for the
* device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables, then setting this advice
* will not create a read-only copy when that device accesses this memory region.
*
* - ::cudaMemAdviceUnsetReadMostly: Undoes the effect of ::cudaMemAdviceReadMostly and also prevents the
* Unified Memory driver from attempting heuristic read-duplication on the memory range. Any read-duplicated
* copies of the data will be collapsed into a single copy. The location for the collapsed
* copy will be the preferred location if the page has a preferred location and one of the read-duplicated
* copies was resident at that location. Otherwise, the location chosen is arbitrary.
*
* - ::cudaMemAdviseSetPreferredLocation: This advice sets the preferred location for the
* data to be the memory belonging to \p device. Passing in cudaCpuDeviceId for \p device sets the
* preferred location as host memory. If \p device is a GPU, then it must have a non-zero value for the
* device attribute ::cudaDevAttrConcurrentManagedAccess. Setting the preferred location
* does not cause data to migrate to that location immediately. Instead, it guides the migration policy
* when a fault occurs on that memory region. If the data is already in its preferred location and the
* faulting processor can establish a mapping without requiring the data to be migrated, then
* data migration will be avoided. On the other hand, if the data is not in its preferred location
* or if a direct mapping cannot be established, then it will be migrated to the processor accessing
* it. It is important to note that setting the preferred location does not prevent data prefetching
* done using ::cudaMemPrefetchAsync.
* Having a preferred location can override the page thrash detection and resolution logic in the Unified
* Memory driver. Normally, if a page is detected to be constantly thrashing between for example host and device
* memory, the page may eventually be pinned to host memory by the Unified Memory driver. But
* if the preferred location is set as device memory, then the page will continue to thrash indefinitely.
* If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
* policies associated with that advice will override the policies of this advice, unless read accesses from
* \p device will not result in a read-only copy being created on that device as outlined in description for
* the advice ::cudaMemAdviseSetReadMostly.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
* a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
* then this call has no effect. Note however that this behavior may change in the future.
*
* - ::cudaMemAdviseUnsetPreferredLocation: Undoes the effect of ::cudaMemAdviseSetPreferredLocation
* and changes the preferred location to none.
*
* - ::cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by \p device.
* Passing in ::cudaCpuDeviceId for \p device will set the advice for the CPU. If \p device is a GPU, then
* the device attribute ::cudaDevAttrConcurrentManagedAccess must be non-zero.
* This advice does not cause data migration and has no impact on the location of the data per se. Instead,
* it causes the data to always be mapped in the specified processor's page tables, as long as the
* location of the data permits a mapping to be established. If the data gets migrated for any reason,
* the mappings are updated accordingly.
* This advice is recommended in scenarios where data locality is not important, but avoiding faults is.
* Consider for example a system containing multiple GPUs with peer-to-peer access enabled, where the
* data located on one GPU is occasionally accessed by peer GPUs. In such scenarios, migrating data
* over to the other GPUs is not as important because the accesses are infrequent and the overhead of
* migration may be too high. But preventing faults can still help improve performance, and so having
* a mapping set up in advance is useful. Note that on CPU access of this data, the data may be migrated
* to host memory because the CPU typically cannot access device memory directly. Any GPU that had the
* ::cudaMemAdviceSetAccessedBy flag set for this data will now have its mapping updated to point to the
* page in host memory.
* If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
* policies associated with that advice will override the policies of this advice. Additionally, if the
* preferred location of this memory region or any subset of it is also \p device, then the policies
* associated with ::cudaMemAdviseSetPreferredLocation will override the policies of this advice.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
* a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
* then this call has no effect.
*
* - ::cudaMemAdviseUnsetAccessedBy: Undoes the effect of ::cudaMemAdviseSetAccessedBy. Any mappings to
* the data from \p device may be removed at any time causing accesses to result in non-fatal page faults.
* If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
* value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
* a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
* then this call has no effect.
*
* \param devPtr - Pointer to memory to set the advice for
* \param count  - Size in bytes of the memory range
* \param advice - Advice to be applied for the specified memory range
* \param device - Device to apply the advice for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
* ::cudaMemcpy3DPeerAsync, ::cudaMemPrefetchAsync,
* ::cuMemAdvise
*/
int handle_cudaMemAdvise(void *conn) {
    void* devPtr;
    size_t count;
    enum cudaMemoryAdvise advice;
    int device;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &advice, sizeof(enum cudaMemoryAdvise)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemAdvise(&devPtr, count, advice, device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Query an attribute of a given memory range
*
* Query an attribute about the memory range starting at \p devPtr with a size of \p count bytes. The
* memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
* __managed__ variables.
*
* The \p attribute parameter can take the following values:
* - ::cudaMemRangeAttributeReadMostly: If this attribute is specified, \p data will be interpreted
* as a 32-bit integer, and \p dataSize must be 4. The result returned will be 1 if all pages in the given
* memory range have read-duplication enabled, or 0 otherwise.
* - ::cudaMemRangeAttributePreferredLocation: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be a GPU device
* id if all pages in the memory range have that GPU as their preferred location, or it will be cudaCpuDeviceId
* if all pages in the memory range have the CPU as their preferred location, or it will be cudaInvalidDeviceId
* if either all the pages don't have the same preferred location or some of the pages don't have a
* preferred location at all. Note that the actual location of the pages in the memory range at the time of
* the query may be different from the preferred location.
* - ::cudaMemRangeAttributeAccessedBy: If this attribute is specified, \p data will be interpreted
* as an array of 32-bit integers, and \p dataSize must be a non-zero multiple of 4. The result returned
* will be a list of device ids that had ::cudaMemAdviceSetAccessedBy set for that entire memory range.
* If any device does not have that advice set for the entire memory range, that device will not be included.
* If \p data is larger than the number of devices that have that advice set for that memory range,
* cudaInvalidDeviceId will be returned in all the extra space provided. For ex., if \p dataSize is 12
* (i.e. \p data has 3 elements) and only device 0 has the advice set, then the result returned will be
* { 0, cudaInvalidDeviceId, cudaInvalidDeviceId }. If \p data is smaller than the number of devices that have
* that advice set, then only as many devices will be returned as can fit in the array. There is no
* guarantee on which specific devices will be returned, however.
* - ::cudaMemRangeAttributeLastPrefetchLocation: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be the last location
* to which all pages in the memory range were prefetched explicitly via ::cudaMemPrefetchAsync. This will either be
* a GPU id or cudaCpuDeviceId depending on whether the last location for prefetch was a GPU or the CPU
* respectively. If any page in the memory range was never explicitly prefetched or if all pages were not
* prefetched to the same location, cudaInvalidDeviceId will be returned. Note that this simply returns the
* last location that the applicaton requested to prefetch the memory range to. It gives no indication as to
* whether the prefetch operation to that location has completed or even begun.
*
* \param data      - A pointers to a memory location where the result
*                    of each attribute query will be written to.
* \param dataSize  - Array containing the size of data
* \param attribute - The attribute to query
* \param devPtr    - Start of the range to query
* \param count     - Size of the range to query
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemRangeGetAttributes, ::cudaMemPrefetchAsync,
* ::cudaMemAdvise,
* ::cuMemRangeGetAttribute
*/
int handle_cudaMemRangeGetAttribute(void *conn) {
    void* data;
    size_t dataSize;
    enum cudaMemRangeAttribute attribute;
    void* devPtr;
    size_t count;

    if (rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &dataSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &attribute, sizeof(enum cudaMemRangeAttribute)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemRangeGetAttribute(&data, dataSize, attribute, &devPtr, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Query attributes of a given memory range.
*
* Query attributes of the memory range starting at \p devPtr with a size of \p count bytes. The
* memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
* __managed__ variables. The \p attributes array will be interpreted to have \p numAttributes
* entries. The \p dataSizes array will also be interpreted to have \p numAttributes entries.
* The results of the query will be stored in \p data.
*
* The list of supported attributes are given below. Please refer to ::cudaMemRangeGetAttribute for
* attribute descriptions and restrictions.
*
* - ::cudaMemRangeAttributeReadMostly
* - ::cudaMemRangeAttributePreferredLocation
* - ::cudaMemRangeAttributeAccessedBy
* - ::cudaMemRangeAttributeLastPrefetchLocation
*
* \param data          - A two-dimensional array containing pointers to memory
*                        locations where the result of each attribute query will be written to.
* \param dataSizes     - Array containing the sizes of each result
* \param attributes    - An array of attributes to query
*                        (numAttributes and the number of attributes in this array should match)
* \param numAttributes - Number of attributes to query
* \param devPtr        - Start of the range to query
* \param count         - Size of the range to query
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemRangeGetAttribute, ::cudaMemAdvise,
* ::cudaMemPrefetchAsync,
* ::cuMemRangeGetAttributes
*/
int handle_cudaMemRangeGetAttributes(void *conn) {
    void* data;
    size_t dataSizes;
    enum cudaMemRangeAttribute attributes;
    size_t numAttributes;
    void* devPtr;
    size_t count;

    if (rpc_read(conn, &data, sizeof(void*)) < 0 ||
        rpc_read(conn, &dataSizes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &attributes, sizeof(enum cudaMemRangeAttribute)) < 0 ||
        rpc_read(conn, &numAttributes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemRangeGetAttributes(&data, &dataSizes, &attributes, numAttributes, &devPtr, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &data, sizeof(void*)) < 0 ||
        rpc_write(conn, &dataSizes, sizeof(size_t)) < 0 ||
        rpc_write(conn, &attributes, sizeof(enum cudaMemRangeAttribute)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* \deprecated
*
* Copies \p count bytes from the memory area pointed to by \p src to the
* CUDA array \p dst starting at \p hOffset rows and \p wOffset bytes from
* the upper left corner, where \p kind specifies the direction
* of the copy, and must be one of ::cudaMemcpyHostToHost,
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* \param dst     - Destination memory address
* \param wOffset - Destination starting X offset (columns in bytes)
* \param hOffset - Destination starting Y offset (rows)
* \param src     - Source memory address
* \param count   - Size in bytes to copy
* \param kind    - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D,
* ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyHtoA,
* ::cuMemcpyDtoA
*/
int handle_cudaMemcpyToArray(void *conn) {
    cudaArray_t dst;
    size_t wOffset;
    size_t hOffset;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyToArray(dst, wOffset, hOffset, &src, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* \deprecated
*
* Copies \p count bytes from the CUDA array \p src starting at \p hOffset rows
* and \p wOffset bytes from the upper left corner to the memory area pointed to
* by \p dst, where \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* \param dst     - Destination memory address
* \param src     - Source memory address
* \param wOffset - Source starting X offset (columns in bytes)
* \param hOffset - Source starting Y offset (rows)
* \param count   - Size in bytes to copy
* \param kind    - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_sync
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
* ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyAtoH,
* ::cuMemcpyAtoD
*/
int handle_cudaMemcpyFromArray(void *conn) {
    void* dst;
    cudaArray_const_t src;
    size_t wOffset;
    size_t hOffset;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyFromArray(&dst, src, wOffset, hOffset, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* \deprecated
*
* Copies \p count bytes from the CUDA array \p src starting at \p hOffsetSrc
* rows and \p wOffsetSrc bytes from the upper left corner to the CUDA array
* \p dst starting at \p hOffsetDst rows and \p wOffsetDst bytes from the upper
* left corner, where \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* \param dst        - Destination memory address
* \param wOffsetDst - Destination starting X offset (columns in bytes)
* \param hOffsetDst - Destination starting Y offset (rows)
* \param src        - Source memory address
* \param wOffsetSrc - Source starting X offset (columns in bytes)
* \param hOffsetSrc - Source starting Y offset (rows)
* \param count      - Size in bytes to copy
* \param kind       - Type of transfer
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
* ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyAtoA
*/
int handle_cudaMemcpyArrayToArray(void *conn) {
    cudaArray_t dst;
    size_t wOffsetDst;
    size_t hOffsetDst;
    cudaArray_const_t src;
    size_t wOffsetSrc;
    size_t hOffsetSrc;
    size_t count;
    enum cudaMemcpyKind kind = cudaMemcpyDeviceToDevice;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffsetDst, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffsetDst, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyArrayToArray(dst, wOffsetDst, hOffsetDst, src, wOffsetSrc, hOffsetSrc, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* \deprecated
*
* Copies \p count bytes from the memory area pointed to by \p src to the
* CUDA array \p dst starting at \p hOffset rows and \p wOffset bytes from
* the upper left corner, where \p kind specifies the
* direction of the copy, and must be one of ::cudaMemcpyHostToHost,
* ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* ::cudaMemcpyToArrayAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If \p
* kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
* is non-zero, the copy may overlap with operations in other streams.
*
* \param dst     - Destination memory address
* \param wOffset - Destination starting X offset (columns in bytes)
* \param hOffset - Destination starting Y offset (rows)
* \param src     - Source memory address
* \param count   - Size in bytes to copy
* \param kind    - Type of transfer
* \param stream  - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
* ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyHtoAAsync,
* ::cuMemcpy2DAsync
*/
int handle_cudaMemcpyToArrayAsync(void *conn) {
    cudaArray_t dst;
    size_t wOffset;
    size_t hOffset;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyToArrayAsync(dst, wOffset, hOffset, &src, count, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies data between host and device
*
* \deprecated
*
* Copies \p count bytes from the CUDA array \p src starting at \p hOffset rows
* and \p wOffset bytes from the upper left corner to the memory area pointed to
* by \p dst, where \p kind specifies the direction of the copy, and must be one of
* ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
* ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
* ::cudaMemcpyDefault is recommended, in which case the type of transfer is
* inferred from the pointer values. However, ::cudaMemcpyDefault is only
* allowed on systems that support unified virtual addressing.
*
* ::cudaMemcpyFromArrayAsync() is asynchronous with respect to the host, so
* the call may return before the copy is complete. The copy can optionally
* be associated to a stream by passing a non-zero \p stream argument. If \p
* kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
* is non-zero, the copy may overlap with operations in other streams.
*
* \param dst     - Destination memory address
* \param src     - Source memory address
* \param wOffset - Source starting X offset (columns in bytes)
* \param hOffset - Source starting Y offset (rows)
* \param count   - Size in bytes to copy
* \param kind    - Type of transfer
* \param stream  - Stream identifier
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidMemcpyDirection
* \notefnerr
* \note_async
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
* ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
* ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
* ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
* ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
* ::cudaMemcpy2DFromArrayAsync,
* ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
* ::cuMemcpyAtoHAsync,
* ::cuMemcpy2DAsync
*/
int handle_cudaMemcpyFromArrayAsync(void *conn) {
    void* dst;
    cudaArray_const_t src;
    size_t wOffset;
    size_t hOffset;
    size_t count;
    enum cudaMemcpyKind kind;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_read(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemcpyFromArrayAsync(&dst, src, wOffset, hOffset, count, kind, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates memory with stream ordered semantics
*
* Inserts an allocation operation into \p hStream.
* A pointer to the allocated memory is returned immediately in *dptr.
* The allocation must not be accessed until the the allocation operation completes.
* The allocation comes from the memory pool associated with the stream's device.
*
* \note The default memory pool of a device contains device memory from that device.
* \note Basic stream ordering allows future work submitted into the same stream to use the allocation.
*       Stream query, stream synchronize, and CUDA events can be used to guarantee that the allocation
*       operation completes before work submitted in a separate stream runs.
* \note During stream capture, this function results in the creation of an allocation node.  In this case,
*       the allocation is owned by the graph instead of the memory pool. The memory pool's properties
*       are used to set the node's creation parameters.
*
* \param[out] devPtr  - Returned device pointer
* \param[in] size     - Number of bytes to allocate
* \param[in] hStream  - The stream establishing the stream ordering contract and the memory pool to allocate from
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported,
* ::cudaErrorOutOfMemory,
* \notefnerr
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cuMemAllocAsync,
* \ref ::cudaMallocAsync(void** ptr, size_t size, cudaMemPool_t memPool, cudaStream_t stream)  "cudaMallocAsync (C++ API)", 
* ::cudaMallocFromPoolAsync, ::cudaFreeAsync, ::cudaDeviceSetMemPool, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool, ::cudaMemPoolSetAccess, ::cudaMemPoolSetAttribute, ::cudaMemPoolGetAttribute
*/
int handle_cudaMallocAsync(void *conn) {
    void* devPtr;
    size_t size;
    cudaStream_t hStream;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocAsync(&devPtr, size, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Frees memory with stream ordered semantics
*
* Inserts a free operation into \p hStream.
* The allocation must not be accessed after stream execution reaches the free.
* After this API returns, accessing the memory from any subsequent work launched on the GPU
* or querying its pointer attributes results in undefined behavior.
*
* \note During stream capture, this function results in the creation of a free node and
*       must therefore be passed the address of a graph allocation.
*
* \param dptr - memory to free
* \param hStream - The stream establishing the stream ordering promise
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported
* \notefnerr
* \note_null_stream
* \note_init_rt
* \note_callback
*
* \sa ::cuMemFreeAsync, ::cudaMallocAsync
*/
int handle_cudaFreeAsync(void *conn) {
    void* devPtr;
    cudaStream_t hStream;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &hStream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaFreeAsync(&devPtr, hStream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Tries to release memory back to the OS
*
* Releases memory back to the OS until the pool contains fewer than minBytesToKeep
* reserved bytes, or there is no more memory that the allocator can safely release.
* The allocator cannot release OS allocations that back outstanding asynchronous allocations.
* The OS allocations may happen at different granularity from the user allocations.
*
* \note: Allocations that have not been freed count as outstanding.
* \note: Allocations that have been asynchronously freed but whose completion has
*        not been observed on the host (eg. by a synchronize) can count as outstanding.
*
* \param[in] pool           - The memory pool to trim
* \param[in] minBytesToKeep - If the pool has less than minBytesToKeep reserved,
* the TrimTo operation is a no-op.  Otherwise the pool will be guaranteed to have
* at least minBytesToKeep bytes reserved after the operation.
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_callback
*
* \sa ::cuMemPoolTrimTo, ::cudaMallocAsync, ::cudaFreeAsync, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool, ::cudaMemPoolCreate
*/
int handle_cudaMemPoolTrimTo(void *conn) {
    cudaMemPool_t memPool;
    size_t minBytesToKeep;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &minBytesToKeep, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolTrimTo(memPool, minBytesToKeep);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets attributes of a memory pool
*
* Supported attributes are:
* - ::cudaMemPoolAttrReleaseThreshold: (value type = cuuint64_t)
*                    Amount of reserved memory in bytes to hold onto before trying
*                    to release memory back to the OS. When more than the release
*                    threshold bytes of memory are held by the memory pool, the
*                    allocator will try to release memory back to the OS on the
*                    next call to stream, event or context synchronize. (default 0)
* - ::cudaMemPoolReuseFollowEventDependencies: (value type = int)
*                    Allow ::cudaMallocAsync to use memory asynchronously freed
*                    in another stream as long as a stream ordering dependency
*                    of the allocating stream on the free action exists.
*                    Cuda events and null stream interactions can create the required
*                    stream ordered dependencies. (default enabled)
* - ::cudaMemPoolReuseAllowOpportunistic: (value type = int)
*                    Allow reuse of already completed frees when there is no dependency
*                    between the free and allocation. (default enabled)
* - ::cudaMemPoolReuseAllowInternalDependencies: (value type = int)
*                    Allow ::cudaMallocAsync to insert new stream dependencies
*                    in order to establish the stream ordering required to reuse
*                    a piece of memory released by ::cudaFreeAsync (default enabled).
* - ::cudaMemPoolAttrReservedMemHigh: (value type = cuuint64_t)
*                    Reset the high watermark that tracks the amount of backing memory that was
*                    allocated for the memory pool. It is illegal to set this attribute to a non-zero value.
* - ::cudaMemPoolAttrUsedMemHigh: (value type = cuuint64_t)
*                    Reset the high watermark that tracks the amount of used memory that was
*                    allocated for the memory pool. It is illegal to set this attribute to a non-zero value.
*
* \param[in] pool  - The memory pool to modify
* \param[in] attr  - The attribute to modify
* \param[in] value - Pointer to the value to assign
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_callback
*
* \sa ::cuMemPoolSetAttribute, ::cudaMallocAsync, ::cudaFreeAsync, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool, ::cudaMemPoolCreate
*/
int handle_cudaMemPoolSetAttribute(void *conn) {
    cudaMemPool_t memPool;
    enum cudaMemPoolAttr attr;
    void* value;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaMemPoolAttr)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolSetAttribute(memPool, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets attributes of a memory pool
*
* Supported attributes are:
* - ::cudaMemPoolAttrReleaseThreshold: (value type = cuuint64_t)
*                    Amount of reserved memory in bytes to hold onto before trying
*                    to release memory back to the OS. When more than the release
*                    threshold bytes of memory are held by the memory pool, the
*                    allocator will try to release memory back to the OS on the
*                    next call to stream, event or context synchronize. (default 0)
* - ::cudaMemPoolReuseFollowEventDependencies: (value type = int)
*                    Allow ::cudaMallocAsync to use memory asynchronously freed
*                    in another stream as long as a stream ordering dependency
*                    of the allocating stream on the free action exists.
*                    Cuda events and null stream interactions can create the required
*                    stream ordered dependencies. (default enabled)
* - ::cudaMemPoolReuseAllowOpportunistic: (value type = int)
*                    Allow reuse of already completed frees when there is no dependency
*                    between the free and allocation. (default enabled)
* - ::cudaMemPoolReuseAllowInternalDependencies: (value type = int)
*                    Allow ::cudaMallocAsync to insert new stream dependencies
*                    in order to establish the stream ordering required to reuse
*                    a piece of memory released by ::cudaFreeAsync (default enabled).
* - ::cudaMemPoolAttrReservedMemCurrent: (value type = cuuint64_t)
*                    Amount of backing memory currently allocated for the mempool.
* - ::cudaMemPoolAttrReservedMemHigh: (value type = cuuint64_t)
*                    High watermark of backing memory allocated for the mempool since
*                    the last time it was reset.
* - ::cudaMemPoolAttrUsedMemCurrent: (value type = cuuint64_t)
*                    Amount of memory from the pool that is currently in use by the application.
* - ::cudaMemPoolAttrUsedMemHigh: (value type = cuuint64_t)
*                    High watermark of the amount of memory from the pool that was in use by the
*                    application since the last time it was reset.
*
* \param[in] pool  - The memory pool to get attributes of 
* \param[in] attr  - The attribute to get
* \param[in] value - Retrieved value 
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_callback
*
* \sa ::cuMemPoolGetAttribute, ::cudaMallocAsync, ::cudaFreeAsync, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool, ::cudaMemPoolCreate
*/
int handle_cudaMemPoolGetAttribute(void *conn) {
    cudaMemPool_t memPool;
    enum cudaMemPoolAttr attr;
    void* value;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaMemPoolAttr)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolGetAttribute(memPool, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Controls visibility of pools between devices
*
* \param[in] pool  - The pool being modified
* \param[in] map   - Array of access descriptors. Each descriptor instructs the access to enable for a single gpu
* \param[in] count - Number of descriptors in the map array.
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa ::cuMemPoolSetAccess, ::cudaMemPoolGetAccess, ::cudaMallocAsync, cudaFreeAsync
*/
int handle_cudaMemPoolSetAccess(void *conn) {
    cudaMemPool_t memPool;
    struct cudaMemAccessDesc descList;
    size_t count;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &descList, sizeof(struct cudaMemAccessDesc)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolSetAccess(memPool, &descList, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the accessibility of a pool from a device
*
* Returns the accessibility of the pool's memory from the specified location.
*
* \param[out] flags   - the accessibility of the pool from the specified location
* \param[in] memPool  - the pool being queried
* \param[in] location - the location accessing the pool
*
* \sa ::cuMemPoolGetAccess, ::cudaMemPoolSetAccess
*/
int handle_cudaMemPoolGetAccess(void *conn) {
    enum cudaMemAccessFlags flags;
    cudaMemPool_t memPool;
    struct cudaMemLocation location;

    if (rpc_read(conn, &flags, sizeof(enum cudaMemAccessFlags)) < 0 ||
        rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &location, sizeof(struct cudaMemLocation)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolGetAccess(&flags, memPool, &location);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(enum cudaMemAccessFlags)) < 0 ||
        rpc_write(conn, &location, sizeof(struct cudaMemLocation)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memory pool
*
* Creates a CUDA memory pool and returns the handle in \p pool.  The \p poolProps determines
* the properties of the pool such as the backing device and IPC capabilities.
*
* By default, the pool's memory will be accessible from the device it is allocated on.
*
* \note Specifying cudaMemHandleTypeNone creates a memory pool that will not support IPC.
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported
*
* \sa ::cuMemPoolCreate, ::cudaDeviceSetMemPool, ::cudaMallocFromPoolAsync, ::cudaMemPoolExportToShareableHandle, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool
*/
int handle_cudaMemPoolCreate(void *conn) {
    cudaMemPool_t memPool;
    struct cudaMemPoolProps poolProps;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &poolProps, sizeof(struct cudaMemPoolProps)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolCreate(&memPool, &poolProps);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys the specified memory pool 
*
* If any pointers obtained from this pool haven't been freed or
* the pool has free operations that haven't completed
* when ::cudaMemPoolDestroy is invoked, the function will return immediately and the
* resources associated with the pool will be released automatically
* once there are no more outstanding allocations.
*
* Destroying the current mempool of a device sets the default mempool of
* that device as the current mempool for that device.
*
* \note A device's default memory pool cannot be destroyed.
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa cuMemPoolDestroy, ::cudaFreeAsync, ::cudaDeviceSetMemPool, ::cudaDeviceGetDefaultMemPool, ::cudaDeviceGetMemPool, ::cudaMemPoolCreate
*/
int handle_cudaMemPoolDestroy(void *conn) {
    cudaMemPool_t memPool;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolDestroy(memPool);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Allocates memory from a specified pool with stream ordered semantics.
*
* Inserts an allocation operation into \p hStream.
* A pointer to the allocated memory is returned immediately in *dptr.
* The allocation must not be accessed until the the allocation operation completes.
* The allocation comes from the specified memory pool.
*
* \note
*    -  The specified memory pool may be from a device different than that of the specified \p hStream.
*
*    -  Basic stream ordering allows future work submitted into the same stream to use the allocation.
*       Stream query, stream synchronize, and CUDA events can be used to guarantee that the allocation
*       operation completes before work submitted in a separate stream runs.
*
* \note During stream capture, this function results in the creation of an allocation node.  In this case,
*       the allocation is owned by the graph instead of the memory pool. The memory pool's properties
*       are used to set the node's creation parameters.
*
* \param[out] ptr     - Returned device pointer
* \param[in] bytesize - Number of bytes to allocate
* \param[in] memPool  - The pool to allocate from
* \param[in] stream   - The stream establishing the stream ordering semantic
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorNotSupported,
* ::cudaErrorOutOfMemory
*
* \sa ::cuMemAllocFromPoolAsync,
* \ref ::cudaMallocAsync(void** ptr, size_t size, cudaMemPool_t memPool, cudaStream_t stream)  "cudaMallocAsync (C++ API)", 
* ::cudaMallocAsync, ::cudaFreeAsync, ::cudaDeviceGetDefaultMemPool, ::cudaMemPoolCreate, ::cudaMemPoolSetAccess, ::cudaMemPoolSetAttribute
*/
int handle_cudaMallocFromPoolAsync(void *conn) {
    void* ptr;
    size_t size;
    cudaMemPool_t memPool;
    cudaStream_t stream;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMallocFromPoolAsync(&ptr, size, memPool, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Exports a memory pool to the requested handle type.
*
* Given an IPC capable mempool, create an OS handle to share the pool with another process.
* A recipient process can convert the shareable handle into a mempool with ::cudaMemPoolImportFromShareableHandle.
* Individual pointers can then be shared with the ::cudaMemPoolExportPointer and ::cudaMemPoolImportPointer APIs.
* The implementation of what the shareable handle is and how it can be transferred is defined by the requested
* handle type.
*
* \note: To create an IPC capable mempool, create a mempool with a CUmemAllocationHandleType other than cudaMemHandleTypeNone.
*
* \param[out] handle_out  - pointer to the location in which to store the requested handle 
* \param[in] pool         - pool to export
* \param[in] handleType   - the type of handle to create
* \param[in] flags        - must be 0
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorOutOfMemory
*
* \sa ::cuMemPoolExportToShareableHandle, ::cudaMemPoolImportFromShareableHandle, ::cudaMemPoolExportPointer, ::cudaMemPoolImportPointer
*/
int handle_cudaMemPoolExportToShareableHandle(void *conn) {
    void* shareableHandle;
    cudaMemPool_t memPool;
    enum cudaMemAllocationHandleType handleType;
    unsigned int flags;

    if (rpc_read(conn, &shareableHandle, sizeof(void*)) < 0 ||
        rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &handleType, sizeof(enum cudaMemAllocationHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolExportToShareableHandle(&shareableHandle, memPool, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &shareableHandle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief imports a memory pool from a shared handle.
*
* Specific allocations can be imported from the imported pool with ::cudaMemPoolImportPointer.
*
* \note Imported memory pools do not support creating new allocations.
*       As such imported memory pools may not be used in ::cudaDeviceSetMemPool
*       or ::cudaMallocFromPoolAsync calls.
*
* \param[out] pool_out    - Returned memory pool
* \param[in] handle       - OS handle of the pool to open
* \param[in] handleType   - The type of handle being imported
* \param[in] flags        - must be 0
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorOutOfMemory
*
* \sa ::cuMemPoolImportFromShareableHandle, ::cudaMemPoolExportToShareableHandle, ::cudaMemPoolExportPointer, ::cudaMemPoolImportPointer
*/
int handle_cudaMemPoolImportFromShareableHandle(void *conn) {
    cudaMemPool_t memPool;
    void* shareableHandle;
    enum cudaMemAllocationHandleType handleType;
    unsigned int flags;

    if (rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &shareableHandle, sizeof(void*)) < 0 ||
        rpc_read(conn, &handleType, sizeof(enum cudaMemAllocationHandleType)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolImportFromShareableHandle(&memPool, &shareableHandle, handleType, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &shareableHandle, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Export data to share a memory pool allocation between processes.
*
* Constructs \p shareData_out for sharing a specific allocation from an already shared memory pool.
* The recipient process can import the allocation with the ::cudaMemPoolImportPointer api.
* The data is not a handle and may be shared through any IPC mechanism.
*
* \param[out] shareData_out - Returned export data
* \param[in] ptr            - pointer to memory being exported
*
* \returns
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorOutOfMemory
*
* \sa ::cuMemPoolExportPointer, ::cudaMemPoolExportToShareableHandle, ::cudaMemPoolImportFromShareableHandle, ::cudaMemPoolImportPointer
*/
int handle_cudaMemPoolExportPointer(void *conn) {
    struct cudaMemPoolPtrExportData exportData;
    void* ptr;

    if (rpc_read(conn, &exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0 ||
        rpc_read(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolExportPointer(&exportData, &ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0 ||
        rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Import a memory pool allocation from another process.
*
* Returns in \p ptr_out a pointer to the imported memory.
* The imported memory must not be accessed before the allocation operation completes
* in the exporting process. The imported memory must be freed from all importing processes before
* being freed in the exporting process. The pointer may be freed with cudaFree
* or cudaFreeAsync.  If ::cudaFreeAsync is used, the free must be completed
* on the importing process before the free operation on the exporting process.
*
* \note The ::cudaFreeAsync api may be used in the exporting process before
*       the ::cudaFreeAsync operation completes in its stream as long as the
*       ::cudaFreeAsync in the exporting process specifies a stream with
*       a stream dependency on the importing process's ::cudaFreeAsync.
*
* \param[out] ptr_out  - pointer to imported memory
* \param[in] pool      - pool from which to import
* \param[in] shareData - data specifying the memory to import
*
* \returns
* ::CUDA_SUCCESS,
* ::CUDA_ERROR_INVALID_VALUE,
* ::CUDA_ERROR_NOT_INITIALIZED,
* ::CUDA_ERROR_OUT_OF_MEMORY
*
* \sa ::cuMemPoolImportPointer, ::cudaMemPoolExportToShareableHandle, ::cudaMemPoolImportFromShareableHandle, ::cudaMemPoolExportPointer
*/
int handle_cudaMemPoolImportPointer(void *conn) {
    void* ptr;
    cudaMemPool_t memPool;
    struct cudaMemPoolPtrExportData exportData;

    if (rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaMemPoolImportPointer(&ptr, memPool, &exportData);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_write(conn, &exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns attributes about a specified pointer
*
* Returns in \p *attributes the attributes of the pointer \p ptr.
* If pointer was not allocated in, mapped by or registered with context
* supporting unified addressing ::cudaErrorInvalidValue is returned.
*
* \note In CUDA 11.0 forward passing host pointer will return ::cudaMemoryTypeUnregistered
* in ::cudaPointerAttributes::type and call will return ::cudaSuccess.
*
* The ::cudaPointerAttributes structure is defined as:
* \code
    struct cudaPointerAttributes {
        enum cudaMemoryType type;
        int device;
        void *devicePointer;
        void *hostPointer;
    }
    \endcode
* In this structure, the individual fields mean
*
* - \ref ::cudaPointerAttributes::type identifies type of memory. It can be
*    ::cudaMemoryTypeUnregistered for unregistered host memory,
*    ::cudaMemoryTypeHost for registered host memory, ::cudaMemoryTypeDevice for device
*    memory or  ::cudaMemoryTypeManaged for managed memory.
*
* - \ref ::cudaPointerAttributes::device "device" is the device against which
*   \p ptr was allocated.  If \p ptr has memory type ::cudaMemoryTypeDevice
*   then this identifies the device on which the memory referred to by \p ptr
*   physically resides.  If \p ptr has memory type ::cudaMemoryTypeHost then this
*   identifies the device which was current when the allocation was made
*   (and if that device is deinitialized then this allocation will vanish
*   with that device's state).
*
* - \ref ::cudaPointerAttributes::devicePointer "devicePointer" is
*   the device pointer alias through which the memory referred to by \p ptr
*   may be accessed on the current device.
*   If the memory referred to by \p ptr cannot be accessed directly by the 
*   current device then this is NULL.  
*
* - \ref ::cudaPointerAttributes::hostPointer "hostPointer" is
*   the host pointer alias through which the memory referred to by \p ptr
*   may be accessed on the host.
*   If the memory referred to by \p ptr cannot be accessed directly by the
*   host then this is NULL.
*
* \param attributes - Attributes for the specified pointer
* \param ptr        - Pointer to get attributes for
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
* ::cudaChooseDevice,
* ::cudaInitDevice,
* ::cuPointerGetAttributes
*/
int handle_cudaPointerGetAttributes(void *conn) {
    struct cudaPointerAttributes attributes;
    void* ptr;

    if (rpc_read(conn, &attributes, sizeof(struct cudaPointerAttributes)) < 0 ||
        rpc_read(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaPointerGetAttributes(&attributes, &ptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &attributes, sizeof(struct cudaPointerAttributes)) < 0)
        return -1;

    return result;
}

/**
* \brief Queries if a device may directly access a peer device's memory.
*
* Returns in \p *canAccessPeer a value of 1 if device \p device is capable of
* directly accessing memory from \p peerDevice and 0 otherwise.  If direct
* access of \p peerDevice from \p device is possible, then access may be
* enabled by calling ::cudaDeviceEnablePeerAccess().
*
* \param canAccessPeer - Returned access capability
* \param device        - Device from which allocations on \p peerDevice are to
*                        be directly accessed.
* \param peerDevice    - Device on which the allocations to be directly accessed 
*                        by \p device reside.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceEnablePeerAccess,
* ::cudaDeviceDisablePeerAccess,
* ::cuDeviceCanAccessPeer
*/
int handle_cudaDeviceCanAccessPeer(void *conn) {
    int canAccessPeer;
    int device;
    int peerDevice;

    if (rpc_read(conn, &canAccessPeer, sizeof(int)) < 0 ||
        rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &peerDevice, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceCanAccessPeer(&canAccessPeer, device, peerDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &canAccessPeer, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Enables direct access to memory allocations on a peer device.
*
* On success, all allocations from \p peerDevice will immediately be accessible by
* the current device.  They will remain accessible until access is explicitly
* disabled using ::cudaDeviceDisablePeerAccess() or either device is reset using
* ::cudaDeviceReset().
*
* Note that access granted by this call is unidirectional and that in order to access
* memory on the current device from \p peerDevice, a separate symmetric call 
* to ::cudaDeviceEnablePeerAccess() is required.
*
* Note that there are both device-wide and system-wide limitations per system
* configuration, as noted in the CUDA Programming Guide under the section
* "Peer-to-Peer Memory Access".
*
* Returns ::cudaErrorInvalidDevice if ::cudaDeviceCanAccessPeer() indicates
* that the current device cannot directly access memory from \p peerDevice.
*
* Returns ::cudaErrorPeerAccessAlreadyEnabled if direct access of
* \p peerDevice from the current device has already been enabled.
*
* Returns ::cudaErrorInvalidValue if \p flags is not 0.
*
* \param peerDevice  - Peer device to enable direct access to from the current device
* \param flags       - Reserved for future use and must be set to 0
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidDevice,
* ::cudaErrorPeerAccessAlreadyEnabled,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceCanAccessPeer,
* ::cudaDeviceDisablePeerAccess,
* ::cuCtxEnablePeerAccess
*/
int handle_cudaDeviceEnablePeerAccess(void *conn) {
    int peerDevice;
    unsigned int flags;

    if (rpc_read(conn, &peerDevice, sizeof(int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceEnablePeerAccess(peerDevice, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Disables direct access to memory allocations on a peer device.
*
* Returns ::cudaErrorPeerAccessNotEnabled if direct access to memory on
* \p peerDevice has not yet been enabled from the current device.
*
* \param peerDevice - Peer device to disable direct access to
*
* \return
* ::cudaSuccess,
* ::cudaErrorPeerAccessNotEnabled,
* ::cudaErrorInvalidDevice
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa ::cudaDeviceCanAccessPeer,
* ::cudaDeviceEnablePeerAccess,
* ::cuCtxDisablePeerAccess
*/
int handle_cudaDeviceDisablePeerAccess(void *conn) {
    int peerDevice;

    if (rpc_read(conn, &peerDevice, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceDisablePeerAccess(peerDevice);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Unregisters a graphics resource for access by CUDA
*
* Unregisters the graphics resource \p resource so it is not accessible by
* CUDA unless registered again.
*
* If \p resource is invalid then ::cudaErrorInvalidResourceHandle is
* returned.
*
* \param resource - Resource to unregister
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaGraphicsD3D9RegisterResource,
* ::cudaGraphicsD3D10RegisterResource,
* ::cudaGraphicsD3D11RegisterResource,
* ::cudaGraphicsGLRegisterBuffer,
* ::cudaGraphicsGLRegisterImage,
* ::cuGraphicsUnregisterResource
*/
int handle_cudaGraphicsUnregisterResource(void *conn) {
    cudaGraphicsResource_t resource;

    if (rpc_read(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsUnregisterResource(resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Set usage flags for mapping a graphics resource
*
* Set \p flags for mapping the graphics resource \p resource.
*
* Changes to \p flags will take effect the next time \p resource is mapped.
* The \p flags argument may be any of the following:
* - ::cudaGraphicsMapFlagsNone: Specifies no hints about how \p resource will
*     be used. It is therefore assumed that CUDA may read from or write to \p resource.
* - ::cudaGraphicsMapFlagsReadOnly: Specifies that CUDA will not write to \p resource.
* - ::cudaGraphicsMapFlagsWriteDiscard: Specifies CUDA will not read from \p resource and will
*   write over the entire contents of \p resource, so none of the data
*   previously stored in \p resource will be preserved.
*
* If \p resource is presently mapped for access by CUDA then ::cudaErrorUnknown is returned.
* If \p flags is not one of the above values then ::cudaErrorInvalidValue is returned.
*
* \param resource - Registered resource to set flags for
* \param flags    - Parameters for resource mapping
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown,
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsMapResources,
* ::cuGraphicsResourceSetMapFlags
*/
int handle_cudaGraphicsResourceSetMapFlags(void *conn) {
    cudaGraphicsResource_t resource;
    unsigned int flags;

    if (rpc_read(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsResourceSetMapFlags(resource, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Map graphics resources for access by CUDA
*
* Maps the \p count graphics resources in \p resources for access by CUDA.
*
* The resources in \p resources may be accessed by CUDA until they
* are unmapped. The graphics API from which \p resources were registered
* should not access any resources while they are mapped by CUDA. If an
* application does so, the results are undefined.
*
* This function provides the synchronization guarantee that any graphics calls
* issued before ::cudaGraphicsMapResources() will complete before any subsequent CUDA
* work issued in \p stream begins.
*
* If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
* is returned. If any of \p resources are presently mapped for access by
* CUDA then ::cudaErrorUnknown is returned.
*
* \param count     - Number of resources to map
* \param resources - Resources to map for CUDA
* \param stream    - Stream for synchronization
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsResourceGetMappedPointer,
* ::cudaGraphicsSubResourceGetMappedArray,
* ::cudaGraphicsUnmapResources,
* ::cuGraphicsMapResources
*/
int handle_cudaGraphicsMapResources(void *conn) {
    int count;
    cudaGraphicsResource_t resources;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &count, sizeof(int)) < 0 ||
        rpc_read(conn, &resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsMapResources(count, &resources, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resources, sizeof(cudaGraphicsResource_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Unmap graphics resources.
*
* Unmaps the \p count graphics resources in \p resources.
*
* Once unmapped, the resources in \p resources may not be accessed by CUDA
* until they are mapped again.
*
* This function provides the synchronization guarantee that any CUDA work issued
* in \p stream before ::cudaGraphicsUnmapResources() will complete before any
* subsequently issued graphics work begins.
*
* If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
* is returned. If any of \p resources are not presently mapped for access by
* CUDA then ::cudaErrorUnknown is returned.
*
* \param count     - Number of resources to unmap
* \param resources - Resources to unmap
* \param stream    - Stream for synchronization
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \note_null_stream
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsMapResources,
* ::cuGraphicsUnmapResources
*/
int handle_cudaGraphicsUnmapResources(void *conn) {
    int count;
    cudaGraphicsResource_t resources;
    cudaStream_t stream = 0;

    if (rpc_read(conn, &count, sizeof(int)) < 0 ||
        rpc_read(conn, &resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsUnmapResources(count, &resources, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resources, sizeof(cudaGraphicsResource_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Get an device pointer through which to access a mapped graphics resource.
*
* Returns in \p *devPtr a pointer through which the mapped graphics resource
* \p resource may be accessed.
* Returns in \p *size the size of the memory in bytes which may be accessed from that pointer.
* The value set in \p devPtr may change every time that \p resource is mapped.
*
* If \p resource is not a buffer then it cannot be accessed via a pointer and
* ::cudaErrorUnknown is returned.
* If \p resource is not mapped then ::cudaErrorUnknown is returned.
* *
* \param devPtr     - Returned pointer through which \p resource may be accessed
* \param size       - Returned size of the buffer accessible starting at \p *devPtr
* \param resource   - Mapped resource to access
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsMapResources,
* ::cudaGraphicsSubResourceGetMappedArray,
* ::cuGraphicsResourceGetMappedPointer
*/
int handle_cudaGraphicsResourceGetMappedPointer(void *conn) {
    void* devPtr;
    size_t size;
    cudaGraphicsResource_t resource;

    if (rpc_read(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsResourceGetMappedPointer(&devPtr, &size, resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Get an array through which to access a subresource of a mapped graphics resource.
*
* Returns in \p *array an array through which the subresource of the mapped
* graphics resource \p resource which corresponds to array index \p arrayIndex
* and mipmap level \p mipLevel may be accessed.  The value set in \p array may
* change every time that \p resource is mapped.
*
* If \p resource is not a texture then it cannot be accessed via an array and
* ::cudaErrorUnknown is returned.
* If \p arrayIndex is not a valid array index for \p resource then
* ::cudaErrorInvalidValue is returned.
* If \p mipLevel is not a valid mipmap level for \p resource then
* ::cudaErrorInvalidValue is returned.
* If \p resource is not mapped then ::cudaErrorUnknown is returned.
*
* \param array       - Returned array through which a subresource of \p resource may be accessed
* \param resource    - Mapped resource to access
* \param arrayIndex  - Array index for array textures or cubemap face
*                      index as defined by ::cudaGraphicsCubeFace for
*                      cubemap textures for the subresource to access
* \param mipLevel    - Mipmap level for the subresource to access
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsResourceGetMappedPointer,
* ::cuGraphicsSubResourceGetMappedArray
*/
int handle_cudaGraphicsSubResourceGetMappedArray(void *conn) {
    cudaArray_t array;
    cudaGraphicsResource_t resource;
    unsigned int arrayIndex;
    unsigned int mipLevel;

    if (rpc_read(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &arrayIndex, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &mipLevel, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsSubResourceGetMappedArray(&array, resource, arrayIndex, mipLevel);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &array, sizeof(cudaArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Get a mipmapped array through which to access a mapped graphics resource.
*
* Returns in \p *mipmappedArray a mipmapped array through which the mapped
* graphics resource \p resource may be accessed. The value set in \p mipmappedArray may
* change every time that \p resource is mapped.
*
* If \p resource is not a texture then it cannot be accessed via an array and
* ::cudaErrorUnknown is returned.
* If \p resource is not mapped then ::cudaErrorUnknown is returned.
*
* \param mipmappedArray - Returned mipmapped array through which \p resource may be accessed
* \param resource       - Mapped resource to access
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorUnknown
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphicsResourceGetMappedPointer,
* ::cuGraphicsResourceGetMappedMipmappedArray
*/
int handle_cudaGraphicsResourceGetMappedMipmappedArray(void *conn) {
    cudaMipmappedArray_t mipmappedArray;
    cudaGraphicsResource_t resource;

    if (rpc_read(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphicsResourceGetMappedMipmappedArray(&mipmappedArray, resource);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Get the channel descriptor of an array
*
* Returns in \p *desc the channel descriptor of the CUDA array \p array.
*
* \param desc  - Channel format
* \param array - Memory array on device
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
* ::cudaCreateTextureObject, ::cudaCreateSurfaceObject
*/
int handle_cudaGetChannelDesc(void *conn) {
    struct cudaChannelFormatDesc desc;
    cudaArray_const_t array;

    if (rpc_read(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &array, sizeof(cudaArray_const_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetChannelDesc(&desc, array);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &desc, sizeof(struct cudaChannelFormatDesc)) < 0)
        return -1;

    return result;
}

int handle_cudaCreateTextureObject(void *conn) {
    cudaTextureObject_t pTexObject;
    struct cudaResourceDesc pResDesc;
    struct cudaTextureDesc pTexDesc;
    struct cudaResourceViewDesc pResViewDesc;

    if (rpc_read(conn, &pTexObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_read(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_read(conn, &pTexDesc, sizeof(struct cudaTextureDesc)) < 0 ||
        rpc_read(conn, &pResViewDesc, sizeof(struct cudaResourceViewDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaCreateTextureObject(&pTexObject, &pResDesc, &pTexDesc, &pResViewDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexObject, sizeof(cudaTextureObject_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a texture object
*
* Destroys the texture object specified by \p texObject.
*
* \param texObject - Texture object to destroy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaCreateTextureObject,
* ::cuTexObjectDestroy
*/
int handle_cudaDestroyTextureObject(void *conn) {
    cudaTextureObject_t texObject;

    if (rpc_read(conn, &texObject, sizeof(cudaTextureObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDestroyTextureObject(texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's resource descriptor
*
* Returns the resource descriptor for the texture object specified by \p texObject.
*
* \param pResDesc  - Resource descriptor
* \param texObject - Texture object
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaCreateTextureObject,
* ::cuTexObjectGetResourceDesc
*/
int handle_cudaGetTextureObjectResourceDesc(void *conn) {
    struct cudaResourceDesc pResDesc;
    cudaTextureObject_t texObject;

    if (rpc_read(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_read(conn, &texObject, sizeof(cudaTextureObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetTextureObjectResourceDesc(&pResDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's texture descriptor
*
* Returns the texture descriptor for the texture object specified by \p texObject.
*
* \param pTexDesc  - Texture descriptor
* \param texObject - Texture object
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaCreateTextureObject,
* ::cuTexObjectGetTextureDesc
*/
int handle_cudaGetTextureObjectTextureDesc(void *conn) {
    struct cudaTextureDesc pTexDesc;
    cudaTextureObject_t texObject;

    if (rpc_read(conn, &pTexDesc, sizeof(struct cudaTextureDesc)) < 0 ||
        rpc_read(conn, &texObject, sizeof(cudaTextureObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetTextureObjectTextureDesc(&pTexDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pTexDesc, sizeof(struct cudaTextureDesc)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a texture object's resource view descriptor
*
* Returns the resource view descriptor for the texture object specified by \p texObject.
* If no resource view was specified, ::cudaErrorInvalidValue is returned.
*
* \param pResViewDesc - Resource view descriptor
* \param texObject    - Texture object
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaCreateTextureObject,
* ::cuTexObjectGetResourceViewDesc
*/
int handle_cudaGetTextureObjectResourceViewDesc(void *conn) {
    struct cudaResourceViewDesc pResViewDesc;
    cudaTextureObject_t texObject;

    if (rpc_read(conn, &pResViewDesc, sizeof(struct cudaResourceViewDesc)) < 0 ||
        rpc_read(conn, &texObject, sizeof(cudaTextureObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetTextureObjectResourceViewDesc(&pResViewDesc, texObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResViewDesc, sizeof(struct cudaResourceViewDesc)) < 0)
        return -1;

    return result;
}

int handle_cudaCreateSurfaceObject(void *conn) {
    cudaSurfaceObject_t pSurfObject;
    struct cudaResourceDesc pResDesc;

    if (rpc_read(conn, &pSurfObject, sizeof(cudaSurfaceObject_t)) < 0 ||
        rpc_read(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaCreateSurfaceObject(&pSurfObject, &pResDesc);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pSurfObject, sizeof(cudaSurfaceObject_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a surface object
*
* Destroys the surface object specified by \p surfObject.
*
* \param surfObject - Surface object to destroy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaCreateSurfaceObject,
* ::cuSurfObjectDestroy
*/
int handle_cudaDestroySurfaceObject(void *conn) {
    cudaSurfaceObject_t surfObject;

    if (rpc_read(conn, &surfObject, sizeof(cudaSurfaceObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDestroySurfaceObject(surfObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a surface object's resource descriptor
* Returns the resource descriptor for the surface object specified by \p surfObject.
*
* \param pResDesc   - Resource descriptor
* \param surfObject - Surface object
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaCreateSurfaceObject,
* ::cuSurfObjectGetResourceDesc
*/
int handle_cudaGetSurfaceObjectResourceDesc(void *conn) {
    struct cudaResourceDesc pResDesc;
    cudaSurfaceObject_t surfObject;

    if (rpc_read(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_read(conn, &surfObject, sizeof(cudaSurfaceObject_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetSurfaceObjectResourceDesc(&pResDesc, surfObject);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pResDesc, sizeof(struct cudaResourceDesc)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the latest version of CUDA supported by the driver
*
* Returns in \p *driverVersion the latest version of CUDA supported by
* the driver. The version is returned as (1000 &times; major + 10 &times; minor).
* For example, CUDA 9.2 would be represented by 9020. If no driver is installed,
* then 0 is returned as the driver version.
*
* This function automatically returns ::cudaErrorInvalidValue
* if \p driverVersion is NULL.
*
* \param driverVersion - Returns the CUDA driver version.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaRuntimeGetVersion,
* ::cuDriverGetVersion
*/
int handle_cudaDriverGetVersion(void *conn) {
    int driverVersion;

    if (rpc_read(conn, &driverVersion, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDriverGetVersion(&driverVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &driverVersion, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns the CUDA Runtime version
*
* Returns in \p *runtimeVersion the version number of the current CUDA
* Runtime instance. The version is returned as
* (1000 &times; major + 10 &times; minor). For example,
* CUDA 9.2 would be represented by 9020.
*
* As of CUDA 12.0, this function no longer initializes CUDA. The purpose
* of this API is solely to return a compile-time constant stating the
* CUDA Toolkit version in the above format.
*
* This function automatically returns ::cudaErrorInvalidValue if
* the \p runtimeVersion argument is NULL.
*
* \param runtimeVersion - Returns the CUDA Runtime version.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaDriverGetVersion,
* ::cuDriverGetVersion
*/
int handle_cudaRuntimeGetVersion(void *conn) {
    int runtimeVersion;

    if (rpc_read(conn, &runtimeVersion, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaRuntimeGetVersion(&runtimeVersion);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &runtimeVersion, sizeof(int)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a graph
*
* Creates an empty graph, which is returned via \p pGraph.
*
* \param pGraph - Returns newly created graph
* \param flags   - Graph creation flags, must be 0
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode,
* ::cudaGraphInstantiate,
* ::cudaGraphDestroy,
* ::cudaGraphGetNodes,
* ::cudaGraphGetRootNodes,
* ::cudaGraphGetEdges,
* ::cudaGraphClone
*/
int handle_cudaGraphCreate(void *conn) {
    cudaGraph_t pGraph;
    unsigned int flags;

    if (rpc_read(conn, &pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphCreate(&pGraph, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a kernel execution node and adds it to a graph
*
* Creates a new kernel execution node and adds it to \p graph with \p numDependencies
* dependencies specified via \p pDependencies and arguments specified in \p pNodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* The cudaKernelNodeParams structure is defined as:
*
* \code
*  struct cudaKernelNodeParams
*  {
*      void* func;
*      dim3 gridDim;
*      dim3 blockDim;
*      unsigned int sharedMemBytes;
*      void **kernelParams;
*      void **extra;
*  };
* \endcode
*
* When the graph is launched, the node will invoke kernel \p func on a (\p gridDim.x x
* \p gridDim.y x \p gridDim.z) grid of blocks. Each block contains
* (\p blockDim.x x \p blockDim.y x \p blockDim.z) threads.
*
* \p sharedMem sets the amount of dynamic shared memory that will be
* available to each thread block.
*
* Kernel parameters to \p func can be specified in one of two ways:
*
* 1) Kernel parameters can be specified via \p kernelParams. If the kernel has N
* parameters, then \p kernelParams needs to be an array of N pointers. Each pointer,
* from \p kernelParams[0] to \p kernelParams[N-1], points to the region of memory from which the actual
* parameter will be copied. The number of kernel parameters and their offsets and sizes do not need
* to be specified as that information is retrieved directly from the kernel's image.
*
* 2) Kernel parameters can also be packaged by the application into a single buffer that is passed in
* via \p extra. This places the burden on the application of knowing each kernel
* parameter's size and alignment/padding within the buffer. The \p extra parameter exists
* to allow this function to take additional less commonly used arguments. \p extra specifies
* a list of names of extra settings and their corresponding values. Each extra setting name is
* immediately followed by the corresponding value. The list must be terminated with either NULL or
* CU_LAUNCH_PARAM_END.
*
* - ::CU_LAUNCH_PARAM_END, which indicates the end of the \p extra
*   array;
* - ::CU_LAUNCH_PARAM_BUFFER_POINTER, which specifies that the next
*   value in \p extra will be a pointer to a buffer
*   containing all the kernel parameters for launching kernel
*   \p func;
* - ::CU_LAUNCH_PARAM_BUFFER_SIZE, which specifies that the next
*   value in \p extra will be a pointer to a size_t
*   containing the size of the buffer specified with
*   ::CU_LAUNCH_PARAM_BUFFER_POINTER;
*
* The error ::cudaErrorInvalidValue will be returned if kernel parameters are specified with both
* \p kernelParams and \p extra (i.e. both \p kernelParams and
* \p extra are non-NULL).
*
* The \p kernelParams or \p extra array, as well as the argument values it points to,
* are copied during this call.
*
* \note Kernels launched using graphs must not use texture and surface references. Reading or
*       writing through any texture or surface reference is undefined behavior.
*       This restriction does not apply to texture and surface objects.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param pNodeParams      - Parameters for the GPU execution node
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDeviceFunction
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchKernel,
* ::cudaGraphKernelNodeGetParams,
* ::cudaGraphKernelNodeSetParams,
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode
*/
int handle_cudaGraphAddKernelNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaKernelNodeParams pNodeParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddKernelNode(&pGraphNode, graph, &pDependencies, numDependencies, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a kernel node's parameters
*
* Returns the parameters of kernel node \p node in \p pNodeParams.
* The \p kernelParams or \p extra array returned in \p pNodeParams,
* as well as the argument values it points to, are owned by the node.
* This memory remains valid until the node is destroyed or its
* parameters are modified, and should not be modified
* directly. Use ::cudaGraphKernelNodeSetParams to update the
* parameters of this node.
*
* The params will contain either \p kernelParams or \p extra,
* according to which of these was most recently set on the node.
*
* \param node        - Node to get the parameters for
* \param pNodeParams - Pointer to return the parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDeviceFunction
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchKernel,
* ::cudaGraphAddKernelNode,
* ::cudaGraphKernelNodeSetParams
*/
int handle_cudaGraphKernelNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaKernelNodeParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphKernelNodeGetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a kernel node's parameters
*
* Sets the parameters of kernel node \p node to \p pNodeParams.
*
* \param node        - Node to set the parameters for
* \param pNodeParams - Parameters to copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle,
* ::cudaErrorMemoryAllocation
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchKernel,
* ::cudaGraphAddKernelNode,
* ::cudaGraphKernelNodeGetParams
*/
int handle_cudaGraphKernelNodeSetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaKernelNodeParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphKernelNodeSetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Copies attributes from source node to destination node.
*
* Copies attributes from source node \p src to destination node \p dst.
* Both node must have the same context.
*
* \param[out] dst Destination node
* \param[in] src Source node
* For list of attributes see ::cudaKernelNodeAttrID
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidContext
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaGraphKernelNodeCopyAttributes(void *conn) {
    cudaGraphNode_t hSrc;
    cudaGraphNode_t hDst;

    if (rpc_read(conn, &hSrc, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &hDst, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphKernelNodeCopyAttributes(hSrc, hDst);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Queries node attribute.
*
* Queries attribute \p attr from node \p hNode and stores it in corresponding
* member of \p value_out.
*
* \param[in] hNode
* \param[in] attr
* \param[out] value_out
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaGraphKernelNodeGetAttribute(void *conn) {
    cudaGraphNode_t hNode;
    cudaLaunchAttributeID attr;
    cudaLaunchAttributeValue value_out;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_read(conn, &value_out, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphKernelNodeGetAttribute(hNode, attr, &value_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value_out, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets node attribute.
*
* Sets attribute \p attr on node \p hNode from corresponding attribute of
* \p value.
*
* \param[out] hNode
* \param[in] attr
* \param[out] value
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidResourceHandle
* \notefnerr
*
* \sa
* ::cudaAccessPolicyWindow
*/
int handle_cudaGraphKernelNodeSetAttribute(void *conn) {
    cudaGraphNode_t hNode;
    cudaLaunchAttributeID attr;
    cudaLaunchAttributeValue value;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_read(conn, &value, sizeof(cudaLaunchAttributeValue)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphKernelNodeSetAttribute(hNode, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memcpy node and adds it to a graph
*
* Creates a new memcpy node and adds it to \p graph with \p numDependencies
* dependencies specified via \p pDependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* When the graph is launched, the node will perform the memcpy described by \p pCopyParams.
* See ::cudaMemcpy3D() for a description of the structure and its restrictions.
*
* Memcpy nodes have some additional restrictions with regards to managed memory, if the
* system contains at least one device which has a zero value for the device attribute
* ::cudaDevAttrConcurrentManagedAccess.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param pCopyParams      - Parameters for the memory copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemcpy3D,
* ::cudaGraphAddMemcpyNodeToSymbol,
* ::cudaGraphAddMemcpyNodeFromSymbol,
* ::cudaGraphAddMemcpyNode1D,
* ::cudaGraphMemcpyNodeGetParams,
* ::cudaGraphMemcpyNodeSetParams,
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemsetNode
*/
int handle_cudaGraphAddMemcpyNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaMemcpy3DParms pCopyParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &pCopyParams, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemcpyNode(&pGraphNode, graph, &pDependencies, numDependencies, &pCopyParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddMemcpyNodeToSymbol(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    void* symbol;
    void* src;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemcpyNodeToSymbol(&pGraphNode, graph, &pDependencies, numDependencies, &symbol, &src, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddMemcpyNodeFromSymbol(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    void* dst;
    void* symbol;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemcpyNodeFromSymbol(&pGraphNode, graph, &pDependencies, numDependencies, &dst, &symbol, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddMemcpyNode1D(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    void* dst;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemcpyNode1D(&pGraphNode, graph, &pDependencies, numDependencies, &dst, &src, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memcpy node's parameters
*
* Returns the parameters of memcpy node \p node in \p pNodeParams.
*
* \param node        - Node to get the parameters for
* \param pNodeParams - Pointer to return the parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemcpy3D,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphMemcpyNodeSetParams
*/
int handle_cudaGraphMemcpyNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaMemcpy3DParms pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemcpyNodeGetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a memcpy node's parameters
*
* Sets the parameters of memcpy node \p node to \p pNodeParams.
*
* \param node        - Node to set the parameters for
* \param pNodeParams - Parameters to copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemcpy3D,
* ::cudaGraphMemcpyNodeSetParamsToSymbol,
* ::cudaGraphMemcpyNodeSetParamsFromSymbol,
* ::cudaGraphMemcpyNodeSetParams1D,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphMemcpyNodeGetParams
*/
int handle_cudaGraphMemcpyNodeSetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaMemcpy3DParms pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemcpyNodeSetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphMemcpyNodeSetParamsToSymbol(void *conn) {
    cudaGraphNode_t node;
    void* symbol;
    void* src;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemcpyNodeSetParamsToSymbol(node, &symbol, &src, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphMemcpyNodeSetParamsFromSymbol(void *conn) {
    cudaGraphNode_t node;
    void* dst;
    void* symbol;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemcpyNodeSetParamsFromSymbol(node, &dst, &symbol, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphMemcpyNodeSetParams1D(void *conn) {
    cudaGraphNode_t node;
    void* dst;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemcpyNodeSetParams1D(node, &dst, &src, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a memset node and adds it to a graph
*
* Creates a new memset node and adds it to \p graph with \p numDependencies
* dependencies specified via \p pDependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* The element size must be 1, 2, or 4 bytes.
* When the graph is launched, the node will perform the memset described by \p pMemsetParams.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param pMemsetParams    - Parameters for the memory set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorInvalidDevice
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemset2D,
* ::cudaGraphMemsetNodeGetParams,
* ::cudaGraphMemsetNodeSetParams,
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode
*/
int handle_cudaGraphAddMemsetNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaMemsetParams pMemsetParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &pMemsetParams, sizeof(struct cudaMemsetParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemsetNode(&pGraphNode, graph, &pDependencies, numDependencies, &pMemsetParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a memset node's parameters
*
* Returns the parameters of memset node \p node in \p pNodeParams.
*
* \param node        - Node to get the parameters for
* \param pNodeParams - Pointer to return the parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemset2D,
* ::cudaGraphAddMemsetNode,
* ::cudaGraphMemsetNodeSetParams
*/
int handle_cudaGraphMemsetNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaMemsetParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemsetParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemsetNodeGetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNodeParams, sizeof(struct cudaMemsetParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a memset node's parameters
*
* Sets the parameters of memset node \p node to \p pNodeParams.
*
* \param node        - Node to set the parameters for
* \param pNodeParams - Parameters to copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaMemset2D,
* ::cudaGraphAddMemsetNode,
* ::cudaGraphMemsetNodeGetParams
*/
int handle_cudaGraphMemsetNodeSetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaMemsetParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemsetParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemsetNodeSetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a host execution node and adds it to a graph
*
* Creates a new CPU execution node and adds it to \p graph with \p numDependencies
* dependencies specified via \p pDependencies and arguments specified in \p pNodeParams.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* When the graph is launched, the node will invoke the specified CPU function.
* Host nodes are not supported under MPS with pre-Volta GPUs.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param pNodeParams      - Parameters for the host node
*
* \return
* ::cudaSuccess,
* ::cudaErrorNotSupported,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchHostFunc,
* ::cudaGraphHostNodeGetParams,
* ::cudaGraphHostNodeSetParams,
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode
*/
int handle_cudaGraphAddHostNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaHostNodeParams pNodeParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaHostNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddHostNode(&pGraphNode, graph, &pDependencies, numDependencies, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a host node's parameters
*
* Returns the parameters of host node \p node in \p pNodeParams.
*
* \param node        - Node to get the parameters for
* \param pNodeParams - Pointer to return the parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchHostFunc,
* ::cudaGraphAddHostNode,
* ::cudaGraphHostNodeSetParams
*/
int handle_cudaGraphHostNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaHostNodeParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaHostNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphHostNodeGetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNodeParams, sizeof(struct cudaHostNodeParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets a host node's parameters
*
* Sets the parameters of host node \p node to \p nodeParams.
*
* \param node        - Node to set the parameters for
* \param pNodeParams - Parameters to copy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaLaunchHostFunc,
* ::cudaGraphAddHostNode,
* ::cudaGraphHostNodeGetParams
*/
int handle_cudaGraphHostNodeSetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaHostNodeParams pNodeParams;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaHostNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphHostNodeSetParams(node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates a child graph node and adds it to a graph
*
* Creates a new node which executes an embedded graph, and adds it to \p graph with
* \p numDependencies dependencies specified via \p pDependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* If \p hGraph contains allocation or free nodes, this call will return an error.
*
* The node executes an embedded child graph. The child graph is cloned in this call.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
* \param childGraph      - The graph to clone into this node
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphChildGraphNodeGetGraph,
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode,
* ::cudaGraphClone
*/
int handle_cudaGraphAddChildGraphNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    cudaGraph_t childGraph;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &childGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddChildGraphNode(&pGraphNode, graph, &pDependencies, numDependencies, childGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Gets a handle to the embedded graph of a child graph node
*
* Gets a handle to the embedded graph in a child graph node. This call
* does not clone the graph. Changes to the graph will be reflected in
* the node, and the node retains ownership of the graph.
*
* Allocation and free nodes cannot be added to the returned graph.
* Attempting to do so will return an error.
*
* \param node   - Node to get the embedded graph for
* \param pGraph - Location to store a handle to the graph
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphNodeFindInClone
*/
int handle_cudaGraphChildGraphNodeGetGraph(void *conn) {
    cudaGraphNode_t node;
    cudaGraph_t pGraph;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphChildGraphNodeGetGraph(node, &pGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an empty node and adds it to a graph
*
* Creates a new node which performs no operation, and adds it to \p graph with
* \p numDependencies dependencies specified via \p pDependencies.
* It is possible for \p numDependencies to be 0, in which case the node will be placed
* at the root of the graph. \p pDependencies may not have any duplicate entries.
* A handle to the new node will be returned in \p pGraphNode.
*
* An empty node performs no operation during execution, but can be used for
* transitive ordering. For example, a phased execution graph with 2 groups of n
* nodes with a barrier between them can be represented using an empty node and
* 2*n dependency edges, rather than no empty node and n^2 dependency edges.
*
* \param pGraphNode     - Returns newly created node
* \param graph          - Graph to which to add the node
* \param pDependencies    - Dependencies of the node
* \param numDependencies - Number of dependencies
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphCreate,
* ::cudaGraphDestroyNode,
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode
*/
int handle_cudaGraphAddEmptyNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddEmptyNode(&pGraphNode, graph, &pDependencies, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddEventRecordNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    cudaEvent_t event;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddEventRecordNode(&pGraphNode, graph, &pDependencies, numDependencies, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphEventRecordNodeGetEvent(void *conn) {
    cudaGraphNode_t node;
    cudaEvent_t event_out;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event_out, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphEventRecordNodeGetEvent(node, &event_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event_out, sizeof(cudaEvent_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphEventRecordNodeSetEvent(void *conn) {
    cudaGraphNode_t node;
    cudaEvent_t event;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphEventRecordNodeSetEvent(node, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddEventWaitNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    cudaEvent_t event;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddEventWaitNode(&pGraphNode, graph, &pDependencies, numDependencies, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphEventWaitNodeGetEvent(void *conn) {
    cudaGraphNode_t node;
    cudaEvent_t event_out;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event_out, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphEventWaitNodeGetEvent(node, &event_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &event_out, sizeof(cudaEvent_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphEventWaitNodeSetEvent(void *conn) {
    cudaGraphNode_t node;
    cudaEvent_t event;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphEventWaitNodeSetEvent(node, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddExternalSemaphoresSignalNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaExternalSemaphoreSignalNodeParams nodeParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddExternalSemaphoresSignalNode(&pGraphNode, graph, &pDependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExternalSemaphoresSignalNodeGetParams(void *conn) {
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreSignalNodeParams params_out;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &params_out, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExternalSemaphoresSignalNodeGetParams(hNode, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExternalSemaphoresSignalNodeSetParams(void *conn) {
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreSignalNodeParams nodeParams;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExternalSemaphoresSignalNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddExternalSemaphoresWaitNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaExternalSemaphoreWaitNodeParams nodeParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddExternalSemaphoresWaitNode(&pGraphNode, graph, &pDependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExternalSemaphoresWaitNodeGetParams(void *conn) {
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreWaitNodeParams params_out;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &params_out, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExternalSemaphoresWaitNodeGetParams(hNode, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExternalSemaphoresWaitNodeSetParams(void *conn) {
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreWaitNodeParams nodeParams;

    if (rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExternalSemaphoresWaitNodeSetParams(hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddMemAllocNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    struct cudaMemAllocNodeParams nodeParams;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaMemAllocNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemAllocNode(&pGraphNode, graph, &pDependencies, numDependencies, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(struct cudaMemAllocNodeParams)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphMemAllocNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    struct cudaMemAllocNodeParams params_out;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &params_out, sizeof(struct cudaMemAllocNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemAllocNodeGetParams(node, &params_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &params_out, sizeof(struct cudaMemAllocNodeParams)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphAddMemFreeNode(void *conn) {
    cudaGraphNode_t pGraphNode;
    cudaGraph_t graph;
    cudaGraphNode_t pDependencies;
    size_t numDependencies;
    void* dptr;

    if (rpc_read(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &dptr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddMemFreeNode(&pGraphNode, graph, &pDependencies, numDependencies, &dptr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &dptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphMemFreeNodeGetParams(void *conn) {
    cudaGraphNode_t node;
    void* dptr_out;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &dptr_out, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphMemFreeNodeGetParams(node, &dptr_out);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dptr_out, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaDeviceGraphMemTrim(void *conn) {
    int device;

    if (rpc_read(conn, &device, sizeof(int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGraphMemTrim(device);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaDeviceGetGraphMemAttribute(void *conn) {
    int device;
    enum cudaGraphMemAttributeType attr;
    void* value;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaGraphMemAttributeType)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceGetGraphMemAttribute(device, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaDeviceSetGraphMemAttribute(void *conn) {
    int device;
    enum cudaGraphMemAttributeType attr;
    void* value;

    if (rpc_read(conn, &device, sizeof(int)) < 0 ||
        rpc_read(conn, &attr, sizeof(enum cudaGraphMemAttributeType)) < 0 ||
        rpc_read(conn, &value, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaDeviceSetGraphMemAttribute(device, attr, &value);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &value, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Clones a graph
*
* This function creates a copy of \p originalGraph and returns it in \p pGraphClone.
* All parameters are copied into the cloned graph. The original graph may be modified 
* after this call without affecting the clone.
*
* Child graph nodes in the original graph are recursively copied into the clone.
*
* \param pGraphClone  - Returns newly created cloned graph
* \param originalGraph - Graph to clone
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorMemoryAllocation
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphCreate,
* ::cudaGraphNodeFindInClone
*/
int handle_cudaGraphClone(void *conn) {
    cudaGraph_t pGraphClone;
    cudaGraph_t originalGraph;

    if (rpc_read(conn, &pGraphClone, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &originalGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphClone(&pGraphClone, originalGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphClone, sizeof(cudaGraph_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Finds a cloned version of a node
*
* This function returns the node in \p clonedGraph corresponding to \p originalNode 
* in the original graph.
*
* \p clonedGraph must have been cloned from \p originalGraph via ::cudaGraphClone. 
* \p originalNode must have been in \p originalGraph at the time of the call to 
* ::cudaGraphClone, and the corresponding cloned node in \p clonedGraph must not have 
* been removed. The cloned node is then returned via \p pClonedNode.
*
* \param pNode  - Returns handle to the cloned node
* \param originalNode - Handle to the original node
* \param clonedGraph - Cloned graph to query
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphClone
*/
int handle_cudaGraphNodeFindInClone(void *conn) {
    cudaGraphNode_t pNode;
    cudaGraphNode_t originalNode;
    cudaGraph_t clonedGraph;

    if (rpc_read(conn, &pNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &originalNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &clonedGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeFindInClone(&pNode, originalNode, clonedGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pNode, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's type
*
* Returns the node type of \p node in \p pType.
*
* \param node - Node to query
* \param pType  - Pointer to return the node type
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphGetNodes,
* ::cudaGraphGetRootNodes,
* ::cudaGraphChildGraphNodeGetGraph,
* ::cudaGraphKernelNodeGetParams,
* ::cudaGraphKernelNodeSetParams,
* ::cudaGraphHostNodeGetParams,
* ::cudaGraphHostNodeSetParams,
* ::cudaGraphMemcpyNodeGetParams,
* ::cudaGraphMemcpyNodeSetParams,
* ::cudaGraphMemsetNodeGetParams,
* ::cudaGraphMemsetNodeSetParams
*/
int handle_cudaGraphNodeGetType(void *conn) {
    cudaGraphNode_t node;
    enum cudaGraphNodeType pType;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pType, sizeof(enum cudaGraphNodeType)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeGetType(node, &pType);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pType, sizeof(enum cudaGraphNodeType)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's nodes
*
* Returns a list of \p graph's nodes. \p nodes may be NULL, in which case this
* function will return the number of nodes in \p numNodes. Otherwise,
* \p numNodes entries will be filled in. If \p numNodes is higher than the actual
* number of nodes, the remaining entries in \p nodes will be set to NULL, and the
* number of nodes actually obtained will be returned in \p numNodes.
*
* \param graph    - Graph to query
* \param nodes    - Pointer to return the nodes
* \param numNodes - See description
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphCreate,
* ::cudaGraphGetRootNodes,
* ::cudaGraphGetEdges,
* ::cudaGraphNodeGetType,
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphNodeGetDependentNodes
*/
int handle_cudaGraphGetNodes(void *conn) {
    cudaGraph_t graph;
    cudaGraphNode_t nodes;
    size_t numNodes;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &nodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphGetNodes(graph, &nodes, &numNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &nodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &numNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's root nodes
*
* Returns a list of \p graph's root nodes. \p pRootNodes may be NULL, in which case this
* function will return the number of root nodes in \p pNumRootNodes. Otherwise,
* \p pNumRootNodes entries will be filled in. If \p pNumRootNodes is higher than the actual
* number of root nodes, the remaining entries in \p pRootNodes will be set to NULL, and the
* number of nodes actually obtained will be returned in \p pNumRootNodes.
*
* \param graph       - Graph to query
* \param pRootNodes    - Pointer to return the root nodes
* \param pNumRootNodes - See description
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphCreate,
* ::cudaGraphGetNodes,
* ::cudaGraphGetEdges,
* ::cudaGraphNodeGetType,
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphNodeGetDependentNodes
*/
int handle_cudaGraphGetRootNodes(void *conn) {
    cudaGraph_t graph;
    cudaGraphNode_t pRootNodes;
    size_t pNumRootNodes;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &pRootNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNumRootNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphGetRootNodes(graph, &pRootNodes, &pNumRootNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pRootNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNumRootNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a graph's dependency edges
*
* Returns a list of \p graph's dependency edges. Edges are returned via corresponding
* indices in \p from and \p to; that is, the node in \p to[i] has a dependency on the
* node in \p from[i]. \p from and \p to may both be NULL, in which
* case this function only returns the number of edges in \p numEdges. Otherwise,
* \p numEdges entries will be filled in. If \p numEdges is higher than the actual
* number of edges, the remaining entries in \p from and \p to will be set to NULL, and
* the number of edges actually returned will be written to \p numEdges.
*
* \param graph    - Graph to get the edges from
* \param from     - Location to return edge endpoints
* \param to       - Location to return edge endpoints
* \param numEdges - See description
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphGetNodes,
* ::cudaGraphGetRootNodes,
* ::cudaGraphAddDependencies,
* ::cudaGraphRemoveDependencies,
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphNodeGetDependentNodes
*/
int handle_cudaGraphGetEdges(void *conn) {
    cudaGraph_t graph;
    cudaGraphNode_t from;
    cudaGraphNode_t to;
    size_t numEdges;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numEdges, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphGetEdges(graph, &from, &to, &numEdges);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &numEdges, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's dependencies
*
* Returns a list of \p node's dependencies. \p pDependencies may be NULL, in which case this
* function will return the number of dependencies in \p pNumDependencies. Otherwise,
* \p pNumDependencies entries will be filled in. If \p pNumDependencies is higher than the actual
* number of dependencies, the remaining entries in \p pDependencies will be set to NULL, and the
* number of nodes actually obtained will be returned in \p pNumDependencies.
*
* \param node           - Node to query
* \param pDependencies    - Pointer to return the dependencies
* \param pNumDependencies - See description
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphNodeGetDependentNodes,
* ::cudaGraphGetNodes,
* ::cudaGraphGetRootNodes,
* ::cudaGraphGetEdges,
* ::cudaGraphAddDependencies,
* ::cudaGraphRemoveDependencies
*/
int handle_cudaGraphNodeGetDependencies(void *conn) {
    cudaGraphNode_t node;
    cudaGraphNode_t pDependencies;
    size_t pNumDependencies;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNumDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeGetDependencies(node, &pDependencies, &pNumDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNumDependencies, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Returns a node's dependent nodes
*
* Returns a list of \p node's dependent nodes. \p pDependentNodes may be NULL, in which
* case this function will return the number of dependent nodes in \p pNumDependentNodes.
* Otherwise, \p pNumDependentNodes entries will be filled in. If \p pNumDependentNodes is
* higher than the actual number of dependent nodes, the remaining entries in
* \p pDependentNodes will be set to NULL, and the number of nodes actually obtained will
* be returned in \p pNumDependentNodes.
*
* \param node             - Node to query
* \param pDependentNodes    - Pointer to return the dependent nodes
* \param pNumDependentNodes - See description
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphGetNodes,
* ::cudaGraphGetRootNodes,
* ::cudaGraphGetEdges,
* ::cudaGraphAddDependencies,
* ::cudaGraphRemoveDependencies
*/
int handle_cudaGraphNodeGetDependentNodes(void *conn) {
    cudaGraphNode_t node;
    cudaGraphNode_t pDependentNodes;
    size_t pNumDependentNodes;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNumDependentNodes, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeGetDependentNodes(node, &pDependentNodes, &pNumDependentNodes);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNumDependentNodes, sizeof(size_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Adds dependency edges to a graph.
*
* The number of dependencies to be added is defined by \p numDependencies
* Elements in \p pFrom and \p pTo at corresponding indices define a dependency.
* Each node in \p pFrom and \p pTo must belong to \p graph.
*
* If \p numDependencies is 0, elements in \p pFrom and \p pTo will be ignored.
* Specifying an existing dependency will return an error.
*
* \param graph - Graph to which dependencies are added
* \param from - Array of nodes that provide the dependencies
* \param to - Array of dependent nodes
* \param numDependencies - Number of dependencies to be added
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphRemoveDependencies,
* ::cudaGraphGetEdges,
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphNodeGetDependentNodes
*/
int handle_cudaGraphAddDependencies(void *conn) {
    cudaGraph_t graph;
    cudaGraphNode_t from;
    cudaGraphNode_t to;
    size_t numDependencies;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphAddDependencies(graph, &from, &to, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Removes dependency edges from a graph.
*
* The number of \p pDependencies to be removed is defined by \p numDependencies.
* Elements in \p pFrom and \p pTo at corresponding indices define a dependency.
* Each node in \p pFrom and \p pTo must belong to \p graph.
*
* If \p numDependencies is 0, elements in \p pFrom and \p pTo will be ignored.
* Specifying a non-existing dependency will return an error.
*
* \param graph - Graph from which to remove dependencies
* \param from - Array of nodes that provide the dependencies
* \param to - Array of dependent nodes
* \param numDependencies - Number of dependencies to be removed
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddDependencies,
* ::cudaGraphGetEdges,
* ::cudaGraphNodeGetDependencies,
* ::cudaGraphNodeGetDependentNodes
*/
int handle_cudaGraphRemoveDependencies(void *conn) {
    cudaGraph_t graph;
    cudaGraphNode_t from;
    cudaGraphNode_t to;
    size_t numDependencies;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &numDependencies, sizeof(size_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphRemoveDependencies(graph, &from, &to, numDependencies);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Remove a node from the graph
*
* Removes \p node from its graph. This operation also severs any dependencies of other nodes 
* on \p node and vice versa.
*
* Dependencies cannot be removed from graphs which contain allocation or free nodes.
* Any attempt to do so will return an error.
*
* \param node  - Node to remove
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaGraphAddChildGraphNode,
* ::cudaGraphAddEmptyNode,
* ::cudaGraphAddKernelNode,
* ::cudaGraphAddHostNode,
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphAddMemsetNode
*/
int handle_cudaGraphDestroyNode(void *conn) {
    cudaGraphNode_t node;

    if (rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphDestroyNode(node);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an executable graph from a graph
*
* Instantiates \p graph as an executable graph. The graph is validated for any
* structural constraints or intra-node constraints which were not previously
* validated. If instantiation is successful, a handle to the instantiated graph
* is returned in \p pGraphExec.
*
* The \p flags parameter controls the behavior of instantiation and subsequent
* graph launches.  Valid flags are:
*
* - ::cudaGraphInstantiateFlagAutoFreeOnLaunch, which configures a
* graph containing memory allocation nodes to automatically free any
* unfreed memory allocations before the graph is relaunched.
*
* - ::cudaGraphInstantiateFlagDeviceLaunch, which configures the graph for launch
* from the device. If this flag is passed, the executable graph handle returned can be
* used to launch the graph from both the host and device. This flag cannot be used in
* conjunction with ::cudaGraphInstantiateFlagAutoFreeOnLaunch.
*
* - ::cudaGraphInstantiateFlagUseNodePriority, which causes the graph
* to use the priorities from the per-node attributes rather than the priority
* of the launch stream during execution. Note that priorities are only available
* on kernel nodes, and are copied from stream priority during stream capture.
*
* If \p graph contains any allocation or free nodes, there can be at most one
* executable graph in existence for that graph at a time. An attempt to
* instantiate a second executable graph before destroying the first with
* ::cudaGraphExecDestroy will result in an error.
* 
* Graphs instantiated for launch on the device have additional restrictions which do not
* apply to host graphs:
*
* - The graph's nodes must reside on a single device.
*
* - The graph can only contain kernel nodes. Furthermore, use of CUDA Dynamic Parallelism
* is not permitted. Cooperative launches are permitted as long as MPS is not in use.
*
* If \p graph is not instantiated for launch on the device but contains kernels which
* call device-side cudaGraphLaunch() from multiple devices, this will result in an error.
*
* \param pGraphExec - Returns instantiated graph
* \param graph      - Graph to instantiate
* \param flags      - Flags to control instantiation.  See ::CUgraphInstantiate_flags.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphInstantiateWithFlags,
* ::cudaGraphCreate,
* ::cudaGraphUpload,
* ::cudaGraphLaunch,
* ::cudaGraphExecDestroy
*/
int handle_cudaGraphInstantiate(void *conn) {
    cudaGraphExec_t pGraphExec;
    cudaGraph_t graph;
    unsigned long long flags = 0;

    if (rpc_read(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphInstantiate(&pGraphExec, graph, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphInstantiateWithFlags(void *conn) {
    cudaGraphExec_t pGraphExec;
    cudaGraph_t graph;
    unsigned long long flags = 0;

    if (rpc_read(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphInstantiateWithFlags(&pGraphExec, graph, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0)
        return -1;

    return result;
}

/**
* \brief Creates an executable graph from a graph
*
* Instantiates \p graph as an executable graph according to the \p instantiateParams structure.
* The graph is validated for any structural constraints or intra-node constraints
* which were not previously validated. If instantiation is successful, a handle to
* the instantiated graph is returned in \p pGraphExec.
*
* \p instantiateParams controls the behavior of instantiation and subsequent
* graph launches, as well as returning more detailed information in the event of an error.
* ::cudaGraphInstantiateParams is defined as:
*
* \code
    typedef struct {
        unsigned long long flags;
        cudaStream_t uploadStream;
        cudaGraphNode_t errNode_out;
        cudaGraphInstantiateResult result_out;
    } cudaGraphInstantiateParams;
* \endcode
*
* The \p flags field controls the behavior of instantiation and subsequent
* graph launches. Valid flags are:
*
* - ::cudaGraphInstantiateFlagAutoFreeOnLaunch, which configures a
* graph containing memory allocation nodes to automatically free any
* unfreed memory allocations before the graph is relaunched.
*
* - ::cudaGraphInstantiateFlagUpload, which will perform an upload of the graph
* into \p uploadStream once the graph has been instantiated.
*
* - ::cudaGraphInstantiateFlagDeviceLaunch, which configures the graph for launch
* from the device. If this flag is passed, the executable graph handle returned can be
* used to launch the graph from both the host and device. This flag can only be used
* on platforms which support unified addressing. This flag cannot be used in
* conjunction with ::cudaGraphInstantiateFlagAutoFreeOnLaunch.
*
* - ::cudaGraphInstantiateFlagUseNodePriority, which causes the graph
* to use the priorities from the per-node attributes rather than the priority
* of the launch stream during execution. Note that priorities are only available
* on kernel nodes, and are copied from stream priority during stream capture.
*
* If \p graph contains any allocation or free nodes, there can be at most one
* executable graph in existence for that graph at a time. An attempt to instantiate a
* second executable graph before destroying the first with ::cudaGraphExecDestroy will
* result in an error.
*
* If \p graph contains kernels which call device-side cudaGraphLaunch() from multiple
* devices, this will result in an error.
*
* Graphs instantiated for launch on the device have additional restrictions which do not
* apply to host graphs:
*
* - The graph's nodes must reside on a single device.
* - The graph can only contain kernel nodes, memcpy nodes, memset nodes, and child graph nodes.
*   Operation-specific restrictions are outlined below.
* - Kernel nodes:
*   - Use of CUDA Dynamic Parallelism is not permitted.
*   - Cooperative launches are permitted as long as MPS is not in use.
* - Memcpy nodes:
*   - Only copies involving device memory and/or pinned device-mapped host memory are permitted.
*   - Copies involving CUDA arrays are not permitted.
*   - Both operands must be accessible from the current device, and the current device must
*     match the device of other nodes in the graph.
*
* In the event of an error, the \p result_out and \p errNode_out fields will contain more
* information about the nature of the error. Possible error reporting includes:
*
* - ::cudaGraphInstantiateError, if passed an invalid value or if an unexpected error occurred
*   which is described by the return value of the function. \p errNode_out will be set to NULL.
* - ::cudaGraphInstantiateInvalidStructure, if the graph structure is invalid. \p errNode_out
*   will be set to one of the offending nodes.
* - ::cudaGraphInstantiateNodeOperationNotSupported, if the graph is instantiated for device
*   launch but contains a node of an unsupported node type, or a node which performs unsupported
*   operations, such as use of CUDA dynamic parallelism within a kernel node. \p errNode_out will
*   be set to this node.
* - ::cudaGraphInstantiateMultipleDevicesNotSupported, if the graph is instantiated for device
*   launch but a node’s device differs from that of another node. This error can also be returned
*   if a graph is not instantiated for device launch and it contains kernels which call device-side
*   cudaGraphLaunch() from multiple devices. \p errNode_out will be set to this node.
*
* If instantiation is successful, \p result_out will be set to ::cudaGraphInstantiateSuccess,
* and \p hErrNode_out will be set to NULL.
*
* \param pGraphExec       - Returns instantiated graph
* \param graph            - Graph to instantiate
* \param instantiateParams - Instantiation parameters
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphCreate,
* ::cudaGraphInstantiate,
* ::cudaGraphInstantiateWithFlags,
* ::cudaGraphExecDestroy
*/
int handle_cudaGraphInstantiateWithParams(void *conn) {
    cudaGraphExec_t pGraphExec;
    cudaGraph_t graph;
    cudaGraphInstantiateParams instantiateParams;

    if (rpc_read(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &instantiateParams, sizeof(cudaGraphInstantiateParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphInstantiateWithParams(&pGraphExec, graph, &instantiateParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &instantiateParams, sizeof(cudaGraphInstantiateParams)) < 0)
        return -1;

    return result;
}

/**
* \brief Query the instantiation flags of an executable graph
*
* Returns the flags that were passed to instantiation for the given executable graph.
* ::cudaGraphInstantiateFlagUpload will not be returned by this API as it does
* not affect the resulting executable graph.
*
* \param graphExec - The executable graph to query
* \param flags     - Returns the instantiation flags
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphInstantiate,
* ::cudaGraphInstantiateWithFlags,
* ::cudaGraphInstantiateWithParams
*/
int handle_cudaGraphExecGetFlags(void *conn) {
    cudaGraphExec_t graphExec;
    unsigned long long flags;

    if (rpc_read(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecGetFlags(graphExec, &flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &flags, sizeof(unsigned long long)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a kernel node in the given graphExec
*
* Sets the parameters of a kernel node in an executable graph \p hGraphExec. 
* The node is identified by the corresponding node \p node in the 
* non-executable graph, from which the executable graph was instantiated. 
*
* \p hNode must not have been removed from the original graph. All \p nodeParams 
* fields may change, but the following restrictions apply to \p func updates: 
*
*   - The owning device of the function cannot change.
*   - A node whose function originally did not use CUDA dynamic parallelism cannot be updated
*     to a function which uses CDP
*   - If \p hGraphExec was not instantiated for device launch, a node whose function originally
*     did not use device-side cudaGraphLaunch() cannot be updated to a function which uses
*     device-side cudaGraphLaunch() unless the node resides on the same device as nodes which
*     contained such calls at instantiate-time. If no such calls were present at instantiation,
*     these updates cannot be performed at all.
*
* The modifications only affect future launches of \p hGraphExec. Already 
* enqueued or running launches of \p hGraphExec are not affected by this call. 
* \p node is also not modified by this call.
*
* \param hGraphExec  - The executable graph in which to set the specified node
* \param node        - kernel node from the graph from which graphExec was instantiated
* \param pNodeParams - Updated Parameters to set
* 
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddKernelNode,
* ::cudaGraphKernelNodeSetParams,
* ::cudaGraphExecMemcpyNodeSetParams,
* ::cudaGraphExecMemsetNodeSetParams,
* ::cudaGraphExecHostNodeSetParams,
* ::cudaGraphExecChildGraphNodeSetParams,
* ::cudaGraphExecEventRecordNodeSetEvent,
* ::cudaGraphExecEventWaitNodeSetEvent,
* ::cudaGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cudaGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cudaGraphExecUpdate,
* ::cudaGraphInstantiate
*/
int handle_cudaGraphExecKernelNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    struct cudaKernelNodeParams pNodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecKernelNodeSetParams(hGraphExec, node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a memcpy node in the given graphExec.
*
* Updates the work represented by \p node in \p hGraphExec as though \p node had 
* contained \p pNodeParams at instantiation.  \p node must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from \p node are ignored.
*
* The source and destination memory in \p pNodeParams must be allocated from the same 
* contexts as the original source and destination memory.  Both the instantiation-time 
* memory operands and the memory operands in \p pNodeParams must be 1-dimensional.
* Zero-length operations are not supported.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  \p node is also 
* not modified by this call.
*
* Returns ::cudaErrorInvalidValue if the memory operands' mappings changed or
* either the original or new memory operands are multidimensional.
*
* \param hGraphExec  - The executable graph in which to set the specified node
* \param node        - Memcpy node from the graph which was used to instantiate graphExec
* \param pNodeParams - Updated Parameters to set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddMemcpyNode,
* ::cudaGraphMemcpyNodeSetParams,
* ::cudaGraphExecMemcpyNodeSetParamsToSymbol,
* ::cudaGraphExecMemcpyNodeSetParamsFromSymbol,
* ::cudaGraphExecMemcpyNodeSetParams1D,
* ::cudaGraphExecKernelNodeSetParams,
* ::cudaGraphExecMemsetNodeSetParams,
* ::cudaGraphExecHostNodeSetParams,
* ::cudaGraphExecChildGraphNodeSetParams,
* ::cudaGraphExecEventRecordNodeSetEvent,
* ::cudaGraphExecEventWaitNodeSetEvent,
* ::cudaGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cudaGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cudaGraphExecUpdate,
* ::cudaGraphInstantiate
*/
int handle_cudaGraphExecMemcpyNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    struct cudaMemcpy3DParms pNodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecMemcpyNodeSetParams(hGraphExec, node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecMemcpyNodeSetParamsToSymbol(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    void* symbol;
    void* src;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecMemcpyNodeSetParamsToSymbol(hGraphExec, node, &symbol, &src, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecMemcpyNodeSetParamsFromSymbol(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    void* dst;
    void* symbol;
    size_t count;
    size_t offset;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &symbol, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecMemcpyNodeSetParamsFromSymbol(hGraphExec, node, &dst, &symbol, count, offset, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecMemcpyNodeSetParams1D(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    void* dst;
    void* src;
    size_t count;
    enum cudaMemcpyKind kind;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &dst, sizeof(void*)) < 0 ||
        rpc_read(conn, &src, sizeof(void*)) < 0 ||
        rpc_read(conn, &count, sizeof(size_t)) < 0 ||
        rpc_read(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecMemcpyNodeSetParams1D(hGraphExec, node, &dst, &src, count, kind);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &dst, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a memset node in the given graphExec.
*
* Updates the work represented by \p node in \p hGraphExec as though \p node had 
* contained \p pNodeParams at instantiation.  \p node must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from \p node are ignored.
*
* The destination memory in \p pNodeParams must be allocated from the same 
* context as the original destination memory.  Both the instantiation-time 
* memory operand and the memory operand in \p pNodeParams must be 1-dimensional.
* Zero-length operations are not supported.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  \p node is also 
* not modified by this call.
*
* Returns cudaErrorInvalidValue if the memory operand's mappings changed or
* either the original or new memory operand are multidimensional.
*
* \param hGraphExec  - The executable graph in which to set the specified node
* \param node        - Memset node from the graph which was used to instantiate graphExec
* \param pNodeParams - Updated Parameters to set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddMemsetNode,
* ::cudaGraphMemsetNodeSetParams,
* ::cudaGraphExecKernelNodeSetParams,
* ::cudaGraphExecMemcpyNodeSetParams,
* ::cudaGraphExecHostNodeSetParams,
* ::cudaGraphExecChildGraphNodeSetParams,
* ::cudaGraphExecEventRecordNodeSetEvent,
* ::cudaGraphExecEventWaitNodeSetEvent,
* ::cudaGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cudaGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cudaGraphExecUpdate,
* ::cudaGraphInstantiate
*/
int handle_cudaGraphExecMemsetNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    struct cudaMemsetParams pNodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaMemsetParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecMemsetNodeSetParams(hGraphExec, node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Sets the parameters for a host node in the given graphExec.
*
* Updates the work represented by \p node in \p hGraphExec as though \p node had 
* contained \p pNodeParams at instantiation.  \p node must remain in the graph which was 
* used to instantiate \p hGraphExec.  Changed edges to and from \p node are ignored.
*
* The modifications only affect future launches of \p hGraphExec.  Already enqueued 
* or running launches of \p hGraphExec are not affected by this call.  \p node is also 
* not modified by this call.
*
* \param hGraphExec  - The executable graph in which to set the specified node
* \param node        - Host node from the graph which was used to instantiate graphExec
* \param pNodeParams - Updated Parameters to set
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphAddHostNode,
* ::cudaGraphHostNodeSetParams,
* ::cudaGraphExecKernelNodeSetParams,
* ::cudaGraphExecMemcpyNodeSetParams,
* ::cudaGraphExecMemsetNodeSetParams,
* ::cudaGraphExecChildGraphNodeSetParams,
* ::cudaGraphExecEventRecordNodeSetEvent,
* ::cudaGraphExecEventWaitNodeSetEvent,
* ::cudaGraphExecExternalSemaphoresSignalNodeSetParams,
* ::cudaGraphExecExternalSemaphoresWaitNodeSetParams,
* ::cudaGraphExecUpdate,
* ::cudaGraphInstantiate
*/
int handle_cudaGraphExecHostNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    struct cudaHostNodeParams pNodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &pNodeParams, sizeof(struct cudaHostNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecHostNodeSetParams(hGraphExec, node, &pNodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecChildGraphNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t node;
    cudaGraph_t childGraph;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &childGraph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecChildGraphNodeSetParams(hGraphExec, node, childGraph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecEventRecordNodeSetEvent(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    cudaEvent_t event;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecEventRecordNodeSetEvent(hGraphExec, hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecEventWaitNodeSetEvent(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    cudaEvent_t event;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &event, sizeof(cudaEvent_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecEventWaitNodeSetEvent(hGraphExec, hNode, event);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecExternalSemaphoresSignalNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreSignalNodeParams nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecExternalSemaphoresSignalNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphExecExternalSemaphoresWaitNodeSetParams(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    struct cudaExternalSemaphoreWaitNodeParams nodeParams;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &nodeParams, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecExternalSemaphoresWaitNodeSetParams(hGraphExec, hNode, &nodeParams);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphNodeSetEnabled(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    unsigned int isEnabled;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeSetEnabled(hGraphExec, hNode, isEnabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGraphNodeGetEnabled(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraphNode_t hNode;
    unsigned int isEnabled;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphNodeGetEnabled(hGraphExec, hNode, &isEnabled);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &isEnabled, sizeof(unsigned int)) < 0)
        return -1;

    return result;
}

/**
* \brief Check whether an executable graph can be updated with a graph and perform the update if possible
*
* Updates the node parameters in the instantiated graph specified by \p hGraphExec with the
* node parameters in a topologically identical graph specified by \p hGraph.
*
* Limitations:
*
* - Kernel nodes:
*   - The owning context of the function cannot change.
*   - A node whose function originally did not use CUDA dynamic parallelism cannot be updated
*     to a function which uses CDP.
*   - A cooperative node cannot be updated to a non-cooperative node, and vice-versa.
*   - If the graph was instantiated with cudaGraphInstantiateFlagUseNodePriority, the
*     priority attribute cannot change. Equality is checked on the originally requested
*     priority values, before they are clamped to the device's supported range.
*   - If \p hGraphExec was not instantiated for device launch, a node whose function originally
*     did not use device-side cudaGraphLaunch() cannot be updated to a function which uses
*     device-side cudaGraphLaunch() unless the node resides on the same device as nodes which
*     contained such calls at instantiate-time. If no such calls were present at instantiation,
*     these updates cannot be performed at all.
* - Memset and memcpy nodes:
*   - The CUDA device(s) to which the operand(s) was allocated/mapped cannot change.
*   - The source/destination memory must be allocated from the same contexts as the original
*     source/destination memory.
*   - Only 1D memsets can be changed.
* - Additional memcpy node restrictions:
*   - Changing either the source or destination memory type(i.e. CU_MEMORYTYPE_DEVICE,
*     CU_MEMORYTYPE_ARRAY, etc.) is not supported.
*
* Note:  The API may add further restrictions in future releases.  The return code should always be checked.
*
* cudaGraphExecUpdate sets the result member of \p resultInfo to cudaGraphExecUpdateErrorTopologyChanged
* under the following conditions:
* - The count of nodes directly in \p hGraphExec and \p hGraph differ, in which case resultInfo->errorNode
*   is set to NULL.
* - \p hGraph has more exit nodes than \p hGraph, in which case resultInfo->errorNode is set to one of
*   the exit nodes in hGraph. 
* - A node in \p hGraph has a different number of dependencies than the node from \p hGraphExec it is paired with,
*   in which case resultInfo->errorNode is set to the node from \p hGraph.
* - A node in \p hGraph has a dependency that does not match with the corresponding dependency of the paired node
*   from \p hGraphExec. resultInfo->errorNode will be set to the node from \p hGraph. resultInfo->errorFromNode
*   will be set to the mismatched dependency. The dependencies are paired based on edge order and a dependency
*   does not match when the nodes are already paired based on other edges examined in the graph.
*
* cudaGraphExecUpdate sets \p the result member of \p resultInfo to:
* - cudaGraphExecUpdateError if passed an invalid value.
* - cudaGraphExecUpdateErrorTopologyChanged if the graph topology changed
* - cudaGraphExecUpdateErrorNodeTypeChanged if the type of a node changed, in which case
*   \p hErrorNode_out is set to the node from \p hGraph.
* - cudaGraphExecUpdateErrorFunctionChanged if the function of a kernel node changed (CUDA driver < 11.2)
* - cudaGraphExecUpdateErrorUnsupportedFunctionChange if the func field of a kernel changed in an
*   unsupported way(see note above), in which case \p hErrorNode_out is set to the node from \p hGraph
* - cudaGraphExecUpdateErrorParametersChanged if any parameters to a node changed in a way 
*   that is not supported, in which case \p hErrorNode_out is set to the node from \p hGraph
* - cudaGraphExecUpdateErrorAttributesChanged if any attributes of a node changed in a way 
*   that is not supported, in which case \p hErrorNode_out is set to the node from \p hGraph
* - cudaGraphExecUpdateErrorNotSupported if something about a node is unsupported, like 
*   the node's type or configuration, in which case \p hErrorNode_out is set to the node from \p hGraph
*
* If the update fails for a reason not listed above, the result member of \p resultInfo will be set
* to cudaGraphExecUpdateError. If the update succeeds, the result member will be set to cudaGraphExecUpdateSuccess.
*
* cudaGraphExecUpdate returns cudaSuccess when the updated was performed successfully.  It returns
* cudaErrorGraphExecUpdateFailure if the graph update was not performed because it included 
* changes which violated constraints specific to instantiated graph update.
*
* \param hGraphExec The instantiated graph to be updated
* \param hGraph The graph containing the updated parameters
   \param resultInfo the error info structure
*
* \return
* ::cudaSuccess,
* ::cudaErrorGraphExecUpdateFailure,
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphInstantiate
*/
int handle_cudaGraphExecUpdate(void *conn) {
    cudaGraphExec_t hGraphExec;
    cudaGraph_t hGraph;
    cudaGraphExecUpdateResultInfo resultInfo;

    if (rpc_read(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &hGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &resultInfo, sizeof(cudaGraphExecUpdateResultInfo)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecUpdate(hGraphExec, hGraph, &resultInfo);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &resultInfo, sizeof(cudaGraphExecUpdateResultInfo)) < 0)
        return -1;

    return result;
}

int handle_cudaGraphUpload(void *conn) {
    cudaGraphExec_t graphExec;
    cudaStream_t stream;

    if (rpc_read(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphUpload(graphExec, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Launches an executable graph in a stream
*
* Executes \p graphExec in \p stream. Only one instance of \p graphExec may be executing
* at a time. Each launch is ordered behind both any previous work in \p stream
* and any previous launches of \p graphExec. To execute a graph concurrently, it must be
* instantiated multiple times into multiple executable graphs.
*
* If any allocations created by \p graphExec remain unfreed (from a previous launch) and
* \p graphExec was not instantiated with ::cudaGraphInstantiateFlagAutoFreeOnLaunch,
* the launch will fail with ::cudaErrorInvalidValue.
*
* \param graphExec - Executable graph to launch
* \param stream    - Stream in which to launch the graph
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
*
* \sa
* ::cudaGraphInstantiate,
* ::cudaGraphUpload,
* ::cudaGraphExecDestroy
*/
int handle_cudaGraphLaunch(void *conn) {
    cudaGraphExec_t graphExec;
    cudaStream_t stream;

    if (rpc_read(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &stream, sizeof(cudaStream_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphLaunch(graphExec, stream);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys an executable graph
*
* Destroys the executable graph specified by \p graphExec.
*
* \param graphExec - Executable graph to destroy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaGraphInstantiate,
* ::cudaGraphUpload,
* ::cudaGraphLaunch
*/
int handle_cudaGraphExecDestroy(void *conn) {
    cudaGraphExec_t graphExec;

    if (rpc_read(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphExecDestroy(graphExec);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Destroys a graph
*
* Destroys the graph specified by \p graph, as well as all of its nodes.
*
* \param graph - Graph to destroy
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
* \note_graph_thread_safety
* \notefnerr
* \note_init_rt
* \note_callback
* \note_destroy_ub
*
* \sa
* ::cudaGraphCreate
*/
int handle_cudaGraphDestroy(void *conn) {
    cudaGraph_t graph;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphDestroy(graph);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Write a DOT file describing graph structure
*
* Using the provided \p graph, write to \p path a DOT formatted description of the graph.
* By default this includes the graph topology, node types, node id, kernel names and memcpy direction.
* \p flags can be specified to write more detailed information about each node type such as
* parameter values, kernel attributes, node and function handles.
*
* \param graph - The graph to create a DOT file from
* \param path  - The path to write the DOT file to
* \param flags - Flags from cudaGraphDebugDotFlags for specifying which additional node information to write
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue,
* ::cudaErrorOperatingSystem
*/
int handle_cudaGraphDebugDotPrint(void *conn) {
    cudaGraph_t graph;
    char path;
    unsigned int flags;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &path, sizeof(char)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphDebugDotPrint(graph, &path, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Create a user object
*
* Create a user object with the specified destructor callback and initial reference count. The
* initial references are owned by the caller.
*
* Destructor callbacks cannot make CUDA API calls and should avoid blocking behavior, as they
* are executed by a shared internal thread. Another thread may be signaled to perform such
* actions, if it does not block forward progress of tasks scheduled through CUDA.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object_out      - Location to return the user object handle
* \param ptr             - The pointer to pass to the destroy function
* \param destroy         - Callback to free the user object when it is no longer in use
* \param initialRefcount - The initial refcount to create the object with, typically 1. The
*                          initial references are owned by the calling thread.
* \param flags           - Currently it is required to pass ::cudaUserObjectNoDestructorSync,
*                          which is the only defined flag. This indicates that the destroy
*                          callback cannot be waited on by any CUDA API. Users requiring
*                          synchronization of the callback should signal its completion
*                          manually.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa
* ::cudaUserObjectRetain,
* ::cudaUserObjectRelease,
* ::cudaGraphRetainUserObject,
* ::cudaGraphReleaseUserObject,
* ::cudaGraphCreate
*/
int handle_cudaUserObjectCreate(void *conn) {
    cudaUserObject_t object_out;
    void* ptr;
    cudaHostFn_t destroy;
    unsigned int initialRefcount;
    unsigned int flags;

    if (rpc_read(conn, &object_out, sizeof(cudaUserObject_t)) < 0 ||
        rpc_read(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &destroy, sizeof(cudaHostFn_t)) < 0 ||
        rpc_read(conn, &initialRefcount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaUserObjectCreate(&object_out, &ptr, destroy, initialRefcount, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &object_out, sizeof(cudaUserObject_t)) < 0 ||
        rpc_write(conn, &ptr, sizeof(void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Retain a reference to a user object
*
* Retains new references to a user object. The new references are owned by the caller.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object - The object to retain
* \param count  - The number of references to retain, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa
* ::cudaUserObjectCreate,
* ::cudaUserObjectRelease,
* ::cudaGraphRetainUserObject,
* ::cudaGraphReleaseUserObject,
* ::cudaGraphCreate
*/
int handle_cudaUserObjectRetain(void *conn) {
    cudaUserObject_t object;
    unsigned int count = 1;

    if (rpc_read(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaUserObjectRetain(object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Release a reference to a user object
*
* Releases user object references owned by the caller. The object's destructor is invoked if
* the reference count reaches zero.
*
* It is undefined behavior to release references not owned by the caller, or to use a user
* object handle after all references are released.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param object - The object to release
* \param count  - The number of references to release, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa
* ::cudaUserObjectCreate,
* ::cudaUserObjectRetain,
* ::cudaGraphRetainUserObject,
* ::cudaGraphReleaseUserObject,
* ::cudaGraphCreate
*/
int handle_cudaUserObjectRelease(void *conn) {
    cudaUserObject_t object;
    unsigned int count = 1;

    if (rpc_read(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaUserObjectRelease(object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Retain a reference to a user object from a graph
*
* Creates or moves user object references that will be owned by a CUDA graph.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param graph  - The graph to associate the reference with
* \param object - The user object to retain a reference for
* \param count  - The number of references to add to the graph, typically 1. Must be
*                 nonzero and not larger than INT_MAX.
* \param flags  - The optional flag ::cudaGraphUserObjectMove transfers references
*                 from the calling thread, rather than create new references. Pass 0
*                 to create new references.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa
* ::cudaUserObjectCreate
* ::cudaUserObjectRetain,
* ::cudaUserObjectRelease,
* ::cudaGraphReleaseUserObject,
* ::cudaGraphCreate
*/
int handle_cudaGraphRetainUserObject(void *conn) {
    cudaGraph_t graph;
    cudaUserObject_t object;
    unsigned int count = 1;
    unsigned int flags = 0;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphRetainUserObject(graph, object, count, flags);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

/**
* \brief Release a user object reference from a graph
*
* Releases user object references owned by a graph.
*
* See CUDA User Objects in the CUDA C++ Programming Guide for more information on user objects.
*
* \param graph  - The graph that will release the reference
* \param object - The user object to release a reference for
* \param count  - The number of references to release, typically 1. Must be nonzero
*                 and not larger than INT_MAX.
*
* \return
* ::cudaSuccess,
* ::cudaErrorInvalidValue
*
* \sa
* ::cudaUserObjectCreate
* ::cudaUserObjectRetain,
* ::cudaUserObjectRelease,
* ::cudaGraphRetainUserObject,
* ::cudaGraphCreate
*/
int handle_cudaGraphReleaseUserObject(void *conn) {
    cudaGraph_t graph;
    cudaUserObject_t object;
    unsigned int count = 1;

    if (rpc_read(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_read(conn, &count, sizeof(unsigned int)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGraphReleaseUserObject(graph, object, count);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    return result;
}

int handle_cudaGetDriverEntryPoint(void *conn) {
    char symbol;
    void* funcPtr;
    unsigned long long flags;
    enum cudaDriverEntryPointQueryResult driverStatus;

    if (rpc_read(conn, &symbol, sizeof(char)) < 0 ||
        rpc_read(conn, &funcPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &driverStatus, sizeof(enum cudaDriverEntryPointQueryResult)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetDriverEntryPoint(&symbol, &funcPtr, flags, &driverStatus);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &funcPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &driverStatus, sizeof(enum cudaDriverEntryPointQueryResult)) < 0)
        return -1;

    return result;
}

/** \cond impl_private */
int handle_cudaGetExportTable(void *conn) {
    const void* ppExportTable;
    cudaUUID_t pExportTableId;

    if (rpc_read(conn, &ppExportTable, sizeof(const void*)) < 0 ||
        rpc_read(conn, &pExportTableId, sizeof(cudaUUID_t)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetExportTable(&ppExportTable, &pExportTableId);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &ppExportTable, sizeof(const void*)) < 0)
        return -1;

    return result;
}

/**
* \brief Get pointer to device entry function that matches entry function \p symbolPtr
*
* Returns in \p functionPtr the device entry function corresponding to the symbol \p symbolPtr.
*
* \param functionPtr     - Returns the device entry function
* \param symbolPtr       - Pointer to device entry function to search for
*
* \return
* ::cudaSuccess
*
*/
int handle_cudaGetFuncBySymbol(void *conn) {
    cudaFunction_t functionPtr;
    void* symbolPtr;

    if (rpc_read(conn, &functionPtr, sizeof(cudaFunction_t)) < 0 ||
        rpc_read(conn, &symbolPtr, sizeof(void*)) < 0)
        return -1;

    int request_id = rpc_end_request(conn);
    if (request_id < 0)
        return -1;

    cudaError_t result = cudaGetFuncBySymbol(&functionPtr, &symbolPtr);

    if (rpc_start_response(conn, request_id) < 0)
        return -1;

    if (rpc_write(conn, &functionPtr, sizeof(cudaFunction_t)) < 0)
        return -1;

    return result;
}

static RequestHandler opHandlers[] = {
    handle_nvmlInit_v2,
    handle_nvmlInitWithFlags,
    handle_nvmlShutdown,
    handle_nvmlSystemGetDriverVersion,
    handle_nvmlSystemGetNVMLVersion,
    handle_nvmlSystemGetCudaDriverVersion,
    handle_nvmlSystemGetCudaDriverVersion_v2,
    handle_nvmlSystemGetProcessName,
    handle_nvmlUnitGetCount,
    handle_nvmlUnitGetHandleByIndex,
    handle_nvmlUnitGetUnitInfo,
    handle_nvmlUnitGetLedState,
    handle_nvmlUnitGetPsuInfo,
    handle_nvmlUnitGetTemperature,
    handle_nvmlUnitGetFanSpeedInfo,
    handle_nvmlUnitGetDevices,
    handle_nvmlSystemGetHicVersion,
    handle_nvmlDeviceGetCount_v2,
    handle_nvmlDeviceGetAttributes_v2,
    handle_nvmlDeviceGetHandleByIndex_v2,
    handle_nvmlDeviceGetHandleBySerial,
    handle_nvmlDeviceGetHandleByUUID,
    handle_nvmlDeviceGetHandleByPciBusId_v2,
    handle_nvmlDeviceGetName,
    handle_nvmlDeviceGetBrand,
    handle_nvmlDeviceGetIndex,
    handle_nvmlDeviceGetSerial,
    handle_nvmlDeviceGetMemoryAffinity,
    handle_nvmlDeviceGetCpuAffinityWithinScope,
    handle_nvmlDeviceGetCpuAffinity,
    handle_nvmlDeviceSetCpuAffinity,
    handle_nvmlDeviceClearCpuAffinity,
    handle_nvmlDeviceGetTopologyCommonAncestor,
    handle_nvmlDeviceGetTopologyNearestGpus,
    handle_nvmlSystemGetTopologyGpuSet,
    handle_nvmlDeviceGetP2PStatus,
    handle_nvmlDeviceGetUUID,
    handle_nvmlVgpuInstanceGetMdevUUID,
    handle_nvmlDeviceGetMinorNumber,
    handle_nvmlDeviceGetBoardPartNumber,
    handle_nvmlDeviceGetInforomVersion,
    handle_nvmlDeviceGetInforomImageVersion,
    handle_nvmlDeviceGetInforomConfigurationChecksum,
    handle_nvmlDeviceValidateInforom,
    handle_nvmlDeviceGetDisplayMode,
    handle_nvmlDeviceGetDisplayActive,
    handle_nvmlDeviceGetPersistenceMode,
    handle_nvmlDeviceGetPciInfo_v3,
    handle_nvmlDeviceGetMaxPcieLinkGeneration,
    handle_nvmlDeviceGetGpuMaxPcieLinkGeneration,
    handle_nvmlDeviceGetMaxPcieLinkWidth,
    handle_nvmlDeviceGetCurrPcieLinkGeneration,
    handle_nvmlDeviceGetCurrPcieLinkWidth,
    handle_nvmlDeviceGetPcieThroughput,
    handle_nvmlDeviceGetPcieReplayCounter,
    handle_nvmlDeviceGetClockInfo,
    handle_nvmlDeviceGetMaxClockInfo,
    handle_nvmlDeviceGetApplicationsClock,
    handle_nvmlDeviceGetDefaultApplicationsClock,
    handle_nvmlDeviceResetApplicationsClocks,
    handle_nvmlDeviceGetClock,
    handle_nvmlDeviceGetMaxCustomerBoostClock,
    handle_nvmlDeviceGetSupportedMemoryClocks,
    handle_nvmlDeviceGetSupportedGraphicsClocks,
    handle_nvmlDeviceGetAutoBoostedClocksEnabled,
    handle_nvmlDeviceSetAutoBoostedClocksEnabled,
    handle_nvmlDeviceSetDefaultAutoBoostedClocksEnabled,
    handle_nvmlDeviceGetFanSpeed,
    handle_nvmlDeviceGetFanSpeed_v2,
    handle_nvmlDeviceGetTargetFanSpeed,
    handle_nvmlDeviceSetDefaultFanSpeed_v2,
    handle_nvmlDeviceGetMinMaxFanSpeed,
    handle_nvmlDeviceGetFanControlPolicy_v2,
    handle_nvmlDeviceSetFanControlPolicy,
    handle_nvmlDeviceGetNumFans,
    handle_nvmlDeviceGetTemperature,
    handle_nvmlDeviceGetTemperatureThreshold,
    handle_nvmlDeviceSetTemperatureThreshold,
    handle_nvmlDeviceGetThermalSettings,
    handle_nvmlDeviceGetPerformanceState,
    handle_nvmlDeviceGetCurrentClocksThrottleReasons,
    handle_nvmlDeviceGetSupportedClocksThrottleReasons,
    handle_nvmlDeviceGetPowerState,
    handle_nvmlDeviceGetPowerManagementMode,
    handle_nvmlDeviceGetPowerManagementLimit,
    handle_nvmlDeviceGetPowerManagementLimitConstraints,
    handle_nvmlDeviceGetPowerManagementDefaultLimit,
    handle_nvmlDeviceGetPowerUsage,
    handle_nvmlDeviceGetTotalEnergyConsumption,
    handle_nvmlDeviceGetEnforcedPowerLimit,
    handle_nvmlDeviceGetGpuOperationMode,
    handle_nvmlDeviceGetMemoryInfo,
    handle_nvmlDeviceGetMemoryInfo_v2,
    handle_nvmlDeviceGetComputeMode,
    handle_nvmlDeviceGetCudaComputeCapability,
    handle_nvmlDeviceGetEccMode,
    handle_nvmlDeviceGetDefaultEccMode,
    handle_nvmlDeviceGetBoardId,
    handle_nvmlDeviceGetMultiGpuBoard,
    handle_nvmlDeviceGetTotalEccErrors,
    handle_nvmlDeviceGetDetailedEccErrors,
    handle_nvmlDeviceGetMemoryErrorCounter,
    handle_nvmlDeviceGetUtilizationRates,
    handle_nvmlDeviceGetEncoderUtilization,
    handle_nvmlDeviceGetEncoderCapacity,
    handle_nvmlDeviceGetEncoderStats,
    handle_nvmlDeviceGetEncoderSessions,
    handle_nvmlDeviceGetDecoderUtilization,
    handle_nvmlDeviceGetFBCStats,
    handle_nvmlDeviceGetFBCSessions,
    handle_nvmlDeviceGetDriverModel,
    handle_nvmlDeviceGetVbiosVersion,
    handle_nvmlDeviceGetBridgeChipInfo,
    handle_nvmlDeviceGetComputeRunningProcesses_v3,
    handle_nvmlDeviceGetGraphicsRunningProcesses_v3,
    handle_nvmlDeviceGetMPSComputeRunningProcesses_v3,
    handle_nvmlDeviceOnSameBoard,
    handle_nvmlDeviceGetAPIRestriction,
    handle_nvmlDeviceGetSamples,
    handle_nvmlDeviceGetBAR1MemoryInfo,
    handle_nvmlDeviceGetViolationStatus,
    handle_nvmlDeviceGetIrqNum,
    handle_nvmlDeviceGetNumGpuCores,
    handle_nvmlDeviceGetPowerSource,
    handle_nvmlDeviceGetMemoryBusWidth,
    handle_nvmlDeviceGetPcieLinkMaxSpeed,
    handle_nvmlDeviceGetPcieSpeed,
    handle_nvmlDeviceGetAdaptiveClockInfoStatus,
    handle_nvmlDeviceGetAccountingMode,
    handle_nvmlDeviceGetAccountingStats,
    handle_nvmlDeviceGetAccountingPids,
    handle_nvmlDeviceGetAccountingBufferSize,
    handle_nvmlDeviceGetRetiredPages,
    handle_nvmlDeviceGetRetiredPages_v2,
    handle_nvmlDeviceGetRetiredPagesPendingStatus,
    handle_nvmlDeviceGetRemappedRows,
    handle_nvmlDeviceGetRowRemapperHistogram,
    handle_nvmlDeviceGetArchitecture,
    handle_nvmlUnitSetLedState,
    handle_nvmlDeviceSetPersistenceMode,
    handle_nvmlDeviceSetComputeMode,
    handle_nvmlDeviceSetEccMode,
    handle_nvmlDeviceClearEccErrorCounts,
    handle_nvmlDeviceSetDriverModel,
    handle_nvmlDeviceSetGpuLockedClocks,
    handle_nvmlDeviceResetGpuLockedClocks,
    handle_nvmlDeviceSetMemoryLockedClocks,
    handle_nvmlDeviceResetMemoryLockedClocks,
    handle_nvmlDeviceSetApplicationsClocks,
    handle_nvmlDeviceGetClkMonStatus,
    handle_nvmlDeviceSetPowerManagementLimit,
    handle_nvmlDeviceSetGpuOperationMode,
    handle_nvmlDeviceSetAPIRestriction,
    handle_nvmlDeviceSetAccountingMode,
    handle_nvmlDeviceClearAccountingPids,
    handle_nvmlDeviceGetNvLinkState,
    handle_nvmlDeviceGetNvLinkVersion,
    handle_nvmlDeviceGetNvLinkCapability,
    handle_nvmlDeviceGetNvLinkRemotePciInfo_v2,
    handle_nvmlDeviceGetNvLinkErrorCounter,
    handle_nvmlDeviceResetNvLinkErrorCounters,
    handle_nvmlDeviceSetNvLinkUtilizationControl,
    handle_nvmlDeviceGetNvLinkUtilizationControl,
    handle_nvmlDeviceGetNvLinkUtilizationCounter,
    handle_nvmlDeviceFreezeNvLinkUtilizationCounter,
    handle_nvmlDeviceResetNvLinkUtilizationCounter,
    handle_nvmlDeviceGetNvLinkRemoteDeviceType,
    handle_nvmlEventSetCreate,
    handle_nvmlDeviceRegisterEvents,
    handle_nvmlDeviceGetSupportedEventTypes,
    handle_nvmlEventSetWait_v2,
    handle_nvmlEventSetFree,
    handle_nvmlDeviceModifyDrainState,
    handle_nvmlDeviceQueryDrainState,
    handle_nvmlDeviceRemoveGpu_v2,
    handle_nvmlDeviceDiscoverGpus,
    handle_nvmlDeviceGetFieldValues,
    handle_nvmlDeviceClearFieldValues,
    handle_nvmlDeviceGetVirtualizationMode,
    handle_nvmlDeviceGetHostVgpuMode,
    handle_nvmlDeviceSetVirtualizationMode,
    handle_nvmlDeviceGetGridLicensableFeatures_v4,
    handle_nvmlDeviceGetProcessUtilization,
    handle_nvmlDeviceGetGspFirmwareVersion,
    handle_nvmlDeviceGetGspFirmwareMode,
    handle_nvmlGetVgpuDriverCapabilities,
    handle_nvmlDeviceGetVgpuCapabilities,
    handle_nvmlDeviceGetSupportedVgpus,
    handle_nvmlDeviceGetCreatableVgpus,
    handle_nvmlVgpuTypeGetClass,
    handle_nvmlVgpuTypeGetName,
    handle_nvmlVgpuTypeGetGpuInstanceProfileId,
    handle_nvmlVgpuTypeGetDeviceID,
    handle_nvmlVgpuTypeGetFramebufferSize,
    handle_nvmlVgpuTypeGetNumDisplayHeads,
    handle_nvmlVgpuTypeGetResolution,
    handle_nvmlVgpuTypeGetLicense,
    handle_nvmlVgpuTypeGetFrameRateLimit,
    handle_nvmlVgpuTypeGetMaxInstances,
    handle_nvmlVgpuTypeGetMaxInstancesPerVm,
    handle_nvmlDeviceGetActiveVgpus,
    handle_nvmlVgpuInstanceGetVmID,
    handle_nvmlVgpuInstanceGetUUID,
    handle_nvmlVgpuInstanceGetVmDriverVersion,
    handle_nvmlVgpuInstanceGetFbUsage,
    handle_nvmlVgpuInstanceGetLicenseStatus,
    handle_nvmlVgpuInstanceGetType,
    handle_nvmlVgpuInstanceGetFrameRateLimit,
    handle_nvmlVgpuInstanceGetEccMode,
    handle_nvmlVgpuInstanceGetEncoderCapacity,
    handle_nvmlVgpuInstanceSetEncoderCapacity,
    handle_nvmlVgpuInstanceGetEncoderStats,
    handle_nvmlVgpuInstanceGetEncoderSessions,
    handle_nvmlVgpuInstanceGetFBCStats,
    handle_nvmlVgpuInstanceGetFBCSessions,
    handle_nvmlVgpuInstanceGetGpuInstanceId,
    handle_nvmlVgpuInstanceGetGpuPciId,
    handle_nvmlVgpuTypeGetCapabilities,
    handle_nvmlVgpuInstanceGetMetadata,
    handle_nvmlDeviceGetVgpuMetadata,
    handle_nvmlGetVgpuCompatibility,
    handle_nvmlDeviceGetPgpuMetadataString,
    handle_nvmlDeviceGetVgpuSchedulerLog,
    handle_nvmlDeviceGetVgpuSchedulerState,
    handle_nvmlDeviceGetVgpuSchedulerCapabilities,
    handle_nvmlGetVgpuVersion,
    handle_nvmlSetVgpuVersion,
    handle_nvmlDeviceGetVgpuUtilization,
    handle_nvmlDeviceGetVgpuProcessUtilization,
    handle_nvmlVgpuInstanceGetAccountingMode,
    handle_nvmlVgpuInstanceGetAccountingPids,
    handle_nvmlVgpuInstanceGetAccountingStats,
    handle_nvmlVgpuInstanceClearAccountingPids,
    handle_nvmlVgpuInstanceGetLicenseInfo_v2,
    handle_nvmlGetExcludedDeviceCount,
    handle_nvmlGetExcludedDeviceInfoByIndex,
    handle_nvmlDeviceSetMigMode,
    handle_nvmlDeviceGetMigMode,
    handle_nvmlDeviceGetGpuInstanceProfileInfo,
    handle_nvmlDeviceGetGpuInstanceProfileInfoV,
    handle_nvmlDeviceGetGpuInstancePossiblePlacements_v2,
    handle_nvmlDeviceGetGpuInstanceRemainingCapacity,
    handle_nvmlDeviceCreateGpuInstance,
    handle_nvmlDeviceCreateGpuInstanceWithPlacement,
    handle_nvmlGpuInstanceDestroy,
    handle_nvmlDeviceGetGpuInstances,
    handle_nvmlDeviceGetGpuInstanceById,
    handle_nvmlGpuInstanceGetInfo,
    handle_nvmlGpuInstanceGetComputeInstanceProfileInfo,
    handle_nvmlGpuInstanceGetComputeInstanceProfileInfoV,
    handle_nvmlGpuInstanceGetComputeInstanceRemainingCapacity,
    handle_nvmlGpuInstanceGetComputeInstancePossiblePlacements,
    handle_nvmlGpuInstanceCreateComputeInstance,
    handle_nvmlGpuInstanceCreateComputeInstanceWithPlacement,
    handle_nvmlComputeInstanceDestroy,
    handle_nvmlGpuInstanceGetComputeInstances,
    handle_nvmlGpuInstanceGetComputeInstanceById,
    handle_nvmlComputeInstanceGetInfo_v2,
    handle_nvmlDeviceIsMigDeviceHandle,
    handle_nvmlDeviceGetGpuInstanceId,
    handle_nvmlDeviceGetComputeInstanceId,
    handle_nvmlDeviceGetMaxMigDeviceCount,
    handle_nvmlDeviceGetMigDeviceHandleByIndex,
    handle_nvmlDeviceGetDeviceHandleFromMigDeviceHandle,
    handle_nvmlDeviceGetBusType,
    handle_nvmlDeviceGetDynamicPstatesInfo,
    handle_nvmlDeviceSetFanSpeed_v2,
    handle_nvmlDeviceGetGpcClkVfOffset,
    handle_nvmlDeviceSetGpcClkVfOffset,
    handle_nvmlDeviceGetMemClkVfOffset,
    handle_nvmlDeviceSetMemClkVfOffset,
    handle_nvmlDeviceGetMinMaxClockOfPState,
    handle_nvmlDeviceGetSupportedPerformanceStates,
    handle_nvmlDeviceGetGpcClkMinMaxVfOffset,
    handle_nvmlDeviceGetMemClkMinMaxVfOffset,
    handle_nvmlDeviceGetGpuFabricInfo,
    handle_nvmlGpmMetricsGet,
    handle_nvmlGpmSampleFree,
    handle_nvmlGpmSampleAlloc,
    handle_nvmlGpmSampleGet,
    handle_nvmlGpmMigSampleGet,
    handle_nvmlGpmQueryDeviceSupport,
    handle_nvmlDeviceSetNvLinkDeviceLowPowerThreshold,
    handle_cuGetErrorString,
    handle_cuGetErrorName,
    handle_cuInit,
    handle_cuDriverGetVersion,
    handle_cuDeviceGet,
    handle_cuDeviceGetCount,
    handle_cuDeviceGetName,
    handle_cuDeviceGetUuid,
    handle_cuDeviceGetUuid_v2,
    handle_cuDeviceGetLuid,
    handle_cuDeviceTotalMem_v2,
    handle_cuDeviceGetTexture1DLinearMaxWidth,
    handle_cuDeviceGetAttribute,
    handle_cuDeviceGetNvSciSyncAttributes,
    handle_cuDeviceSetMemPool,
    handle_cuDeviceGetMemPool,
    handle_cuDeviceGetDefaultMemPool,
    handle_cuDeviceGetExecAffinitySupport,
    handle_cuFlushGPUDirectRDMAWrites,
    handle_cuDeviceGetProperties,
    handle_cuDeviceComputeCapability,
    handle_cuDevicePrimaryCtxRetain,
    handle_cuDevicePrimaryCtxRelease_v2,
    handle_cuDevicePrimaryCtxSetFlags_v2,
    handle_cuDevicePrimaryCtxGetState,
    handle_cuDevicePrimaryCtxReset_v2,
    handle_cuCtxCreate_v2,
    handle_cuCtxCreate_v3,
    handle_cuCtxDestroy_v2,
    handle_cuCtxPushCurrent_v2,
    handle_cuCtxPopCurrent_v2,
    handle_cuCtxSetCurrent,
    handle_cuCtxGetCurrent,
    handle_cuCtxGetDevice,
    handle_cuCtxGetFlags,
    handle_cuCtxGetId,
    handle_cuCtxSynchronize,
    handle_cuCtxSetLimit,
    handle_cuCtxGetLimit,
    handle_cuCtxGetCacheConfig,
    handle_cuCtxSetCacheConfig,
    handle_cuCtxGetSharedMemConfig,
    handle_cuCtxSetSharedMemConfig,
    handle_cuCtxGetApiVersion,
    handle_cuCtxGetStreamPriorityRange,
    handle_cuCtxResetPersistingL2Cache,
    handle_cuCtxGetExecAffinity,
    handle_cuCtxAttach,
    handle_cuCtxDetach,
    handle_cuModuleLoad,
    handle_cuModuleLoadData,
    handle_cuModuleLoadDataEx,
    handle_cuModuleLoadFatBinary,
    handle_cuModuleUnload,
    handle_cuModuleGetLoadingMode,
    handle_cuModuleGetFunction,
    handle_cuModuleGetGlobal_v2,
    handle_cuLinkCreate_v2,
    handle_cuLinkAddData_v2,
    handle_cuLinkAddFile_v2,
    handle_cuLinkComplete,
    handle_cuLinkDestroy,
    handle_cuModuleGetTexRef,
    handle_cuModuleGetSurfRef,
    handle_cuLibraryLoadData,
    handle_cuLibraryLoadFromFile,
    handle_cuLibraryUnload,
    handle_cuLibraryGetKernel,
    handle_cuLibraryGetModule,
    handle_cuKernelGetFunction,
    handle_cuLibraryGetGlobal,
    handle_cuLibraryGetManaged,
    handle_cuLibraryGetUnifiedFunction,
    handle_cuKernelGetAttribute,
    handle_cuKernelSetAttribute,
    handle_cuKernelSetCacheConfig,
    handle_cuMemGetInfo_v2,
    handle_cuMemAlloc_v2,
    handle_cuMemAllocPitch_v2,
    handle_cuMemFree_v2,
    handle_cuMemGetAddressRange_v2,
    handle_cuMemAllocHost_v2,
    handle_cuMemFreeHost,
    handle_cuMemHostAlloc,
    handle_cuMemHostGetDevicePointer_v2,
    handle_cuMemHostGetFlags,
    handle_cuMemAllocManaged,
    handle_cuDeviceGetByPCIBusId,
    handle_cuDeviceGetPCIBusId,
    handle_cuIpcGetEventHandle,
    handle_cuIpcOpenEventHandle,
    handle_cuIpcGetMemHandle,
    handle_cuIpcOpenMemHandle_v2,
    handle_cuIpcCloseMemHandle,
    handle_cuMemHostRegister_v2,
    handle_cuMemHostUnregister,
    handle_cuMemcpy,
    handle_cuMemcpyPeer,
    handle_cuMemcpyHtoD_v2,
    handle_cuMemcpyDtoH_v2,
    handle_cuMemcpyDtoD_v2,
    handle_cuMemcpyDtoA_v2,
    handle_cuMemcpyAtoD_v2,
    handle_cuMemcpyHtoA_v2,
    handle_cuMemcpyAtoH_v2,
    handle_cuMemcpyAtoA_v2,
    handle_cuMemcpy2D_v2,
    handle_cuMemcpy2DUnaligned_v2,
    handle_cuMemcpy3D_v2,
    handle_cuMemcpy3DPeer,
    handle_cuMemcpyAsync,
    handle_cuMemcpyPeerAsync,
    handle_cuMemcpyHtoDAsync_v2,
    handle_cuMemcpyDtoHAsync_v2,
    handle_cuMemcpyDtoDAsync_v2,
    handle_cuMemcpyHtoAAsync_v2,
    handle_cuMemcpyAtoHAsync_v2,
    handle_cuMemcpy2DAsync_v2,
    handle_cuMemcpy3DAsync_v2,
    handle_cuMemcpy3DPeerAsync,
    handle_cuMemsetD8_v2,
    handle_cuMemsetD16_v2,
    handle_cuMemsetD32_v2,
    handle_cuMemsetD2D8_v2,
    handle_cuMemsetD2D16_v2,
    handle_cuMemsetD2D32_v2,
    handle_cuMemsetD8Async,
    handle_cuMemsetD16Async,
    handle_cuMemsetD32Async,
    handle_cuMemsetD2D8Async,
    handle_cuMemsetD2D16Async,
    handle_cuMemsetD2D32Async,
    handle_cuArrayCreate_v2,
    handle_cuArrayGetDescriptor_v2,
    handle_cuArrayGetSparseProperties,
    handle_cuMipmappedArrayGetSparseProperties,
    handle_cuArrayGetMemoryRequirements,
    handle_cuMipmappedArrayGetMemoryRequirements,
    handle_cuArrayGetPlane,
    handle_cuArrayDestroy,
    handle_cuArray3DCreate_v2,
    handle_cuArray3DGetDescriptor_v2,
    handle_cuMipmappedArrayCreate,
    handle_cuMipmappedArrayGetLevel,
    handle_cuMipmappedArrayDestroy,
    handle_cuMemGetHandleForAddressRange,
    handle_cuMemAddressReserve,
    handle_cuMemAddressFree,
    handle_cuMemCreate,
    handle_cuMemRelease,
    handle_cuMemMap,
    handle_cuMemMapArrayAsync,
    handle_cuMemUnmap,
    handle_cuMemSetAccess,
    handle_cuMemGetAccess,
    handle_cuMemExportToShareableHandle,
    handle_cuMemImportFromShareableHandle,
    handle_cuMemGetAllocationGranularity,
    handle_cuMemGetAllocationPropertiesFromHandle,
    handle_cuMemRetainAllocationHandle,
    handle_cuMemFreeAsync,
    handle_cuMemAllocAsync,
    handle_cuMemPoolTrimTo,
    handle_cuMemPoolSetAttribute,
    handle_cuMemPoolGetAttribute,
    handle_cuMemPoolSetAccess,
    handle_cuMemPoolGetAccess,
    handle_cuMemPoolCreate,
    handle_cuMemPoolDestroy,
    handle_cuMemAllocFromPoolAsync,
    handle_cuMemPoolExportToShareableHandle,
    handle_cuMemPoolImportFromShareableHandle,
    handle_cuMemPoolExportPointer,
    handle_cuMemPoolImportPointer,
    handle_cuPointerGetAttribute,
    handle_cuMemPrefetchAsync,
    handle_cuMemAdvise,
    handle_cuMemRangeGetAttribute,
    handle_cuMemRangeGetAttributes,
    handle_cuPointerSetAttribute,
    handle_cuPointerGetAttributes,
    handle_cuStreamCreate,
    handle_cuStreamCreateWithPriority,
    handle_cuStreamGetPriority,
    handle_cuStreamGetFlags,
    handle_cuStreamGetId,
    handle_cuStreamGetCtx,
    handle_cuStreamWaitEvent,
    handle_cuStreamAddCallback,
    handle_cuStreamBeginCapture_v2,
    handle_cuThreadExchangeStreamCaptureMode,
    handle_cuStreamEndCapture,
    handle_cuStreamIsCapturing,
    handle_cuStreamGetCaptureInfo_v2,
    handle_cuStreamUpdateCaptureDependencies,
    handle_cuStreamAttachMemAsync,
    handle_cuStreamQuery,
    handle_cuStreamSynchronize,
    handle_cuStreamDestroy_v2,
    handle_cuStreamCopyAttributes,
    handle_cuStreamGetAttribute,
    handle_cuStreamSetAttribute,
    handle_cuEventCreate,
    handle_cuEventRecord,
    handle_cuEventRecordWithFlags,
    handle_cuEventQuery,
    handle_cuEventSynchronize,
    handle_cuEventDestroy_v2,
    handle_cuEventElapsedTime,
    handle_cuImportExternalMemory,
    handle_cuExternalMemoryGetMappedBuffer,
    handle_cuExternalMemoryGetMappedMipmappedArray,
    handle_cuDestroyExternalMemory,
    handle_cuImportExternalSemaphore,
    handle_cuSignalExternalSemaphoresAsync,
    handle_cuWaitExternalSemaphoresAsync,
    handle_cuDestroyExternalSemaphore,
    handle_cuStreamWaitValue32_v2,
    handle_cuStreamWaitValue64_v2,
    handle_cuStreamWriteValue32_v2,
    handle_cuStreamWriteValue64_v2,
    handle_cuStreamBatchMemOp_v2,
    handle_cuFuncGetAttribute,
    handle_cuFuncSetAttribute,
    handle_cuFuncSetCacheConfig,
    handle_cuFuncSetSharedMemConfig,
    handle_cuFuncGetModule,
    handle_cuLaunchKernel,
    handle_cuLaunchKernelEx,
    handle_cuLaunchCooperativeKernel,
    handle_cuLaunchCooperativeKernelMultiDevice,
    handle_cuLaunchHostFunc,
    handle_cuFuncSetBlockShape,
    handle_cuFuncSetSharedSize,
    handle_cuParamSetSize,
    handle_cuParamSeti,
    handle_cuParamSetf,
    handle_cuParamSetv,
    handle_cuLaunch,
    handle_cuLaunchGrid,
    handle_cuLaunchGridAsync,
    handle_cuParamSetTexRef,
    handle_cuGraphCreate,
    handle_cuGraphAddKernelNode_v2,
    handle_cuGraphKernelNodeGetParams_v2,
    handle_cuGraphKernelNodeSetParams_v2,
    handle_cuGraphAddMemcpyNode,
    handle_cuGraphMemcpyNodeGetParams,
    handle_cuGraphMemcpyNodeSetParams,
    handle_cuGraphAddMemsetNode,
    handle_cuGraphMemsetNodeGetParams,
    handle_cuGraphMemsetNodeSetParams,
    handle_cuGraphAddHostNode,
    handle_cuGraphHostNodeGetParams,
    handle_cuGraphHostNodeSetParams,
    handle_cuGraphAddChildGraphNode,
    handle_cuGraphChildGraphNodeGetGraph,
    handle_cuGraphAddEmptyNode,
    handle_cuGraphAddEventRecordNode,
    handle_cuGraphEventRecordNodeGetEvent,
    handle_cuGraphEventRecordNodeSetEvent,
    handle_cuGraphAddEventWaitNode,
    handle_cuGraphEventWaitNodeGetEvent,
    handle_cuGraphEventWaitNodeSetEvent,
    handle_cuGraphAddExternalSemaphoresSignalNode,
    handle_cuGraphExternalSemaphoresSignalNodeGetParams,
    handle_cuGraphExternalSemaphoresSignalNodeSetParams,
    handle_cuGraphAddExternalSemaphoresWaitNode,
    handle_cuGraphExternalSemaphoresWaitNodeGetParams,
    handle_cuGraphExternalSemaphoresWaitNodeSetParams,
    handle_cuGraphAddBatchMemOpNode,
    handle_cuGraphBatchMemOpNodeGetParams,
    handle_cuGraphBatchMemOpNodeSetParams,
    handle_cuGraphExecBatchMemOpNodeSetParams,
    handle_cuGraphAddMemAllocNode,
    handle_cuGraphMemAllocNodeGetParams,
    handle_cuGraphAddMemFreeNode,
    handle_cuGraphMemFreeNodeGetParams,
    handle_cuDeviceGraphMemTrim,
    handle_cuDeviceGetGraphMemAttribute,
    handle_cuDeviceSetGraphMemAttribute,
    handle_cuGraphClone,
    handle_cuGraphNodeFindInClone,
    handle_cuGraphNodeGetType,
    handle_cuGraphGetNodes,
    handle_cuGraphGetRootNodes,
    handle_cuGraphGetEdges,
    handle_cuGraphNodeGetDependencies,
    handle_cuGraphNodeGetDependentNodes,
    handle_cuGraphAddDependencies,
    handle_cuGraphRemoveDependencies,
    handle_cuGraphDestroyNode,
    handle_cuGraphInstantiateWithFlags,
    handle_cuGraphInstantiateWithParams,
    handle_cuGraphExecGetFlags,
    handle_cuGraphExecKernelNodeSetParams_v2,
    handle_cuGraphExecMemcpyNodeSetParams,
    handle_cuGraphExecMemsetNodeSetParams,
    handle_cuGraphExecHostNodeSetParams,
    handle_cuGraphExecChildGraphNodeSetParams,
    handle_cuGraphExecEventRecordNodeSetEvent,
    handle_cuGraphExecEventWaitNodeSetEvent,
    handle_cuGraphExecExternalSemaphoresSignalNodeSetParams,
    handle_cuGraphExecExternalSemaphoresWaitNodeSetParams,
    handle_cuGraphNodeSetEnabled,
    handle_cuGraphNodeGetEnabled,
    handle_cuGraphUpload,
    handle_cuGraphLaunch,
    handle_cuGraphExecDestroy,
    handle_cuGraphDestroy,
    handle_cuGraphExecUpdate_v2,
    handle_cuGraphKernelNodeCopyAttributes,
    handle_cuGraphKernelNodeGetAttribute,
    handle_cuGraphKernelNodeSetAttribute,
    handle_cuGraphDebugDotPrint,
    handle_cuUserObjectCreate,
    handle_cuUserObjectRetain,
    handle_cuUserObjectRelease,
    handle_cuGraphRetainUserObject,
    handle_cuGraphReleaseUserObject,
    handle_cuOccupancyMaxActiveBlocksPerMultiprocessor,
    handle_cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
    handle_cuOccupancyAvailableDynamicSMemPerBlock,
    handle_cuOccupancyMaxPotentialClusterSize,
    handle_cuOccupancyMaxActiveClusters,
    handle_cuTexRefSetArray,
    handle_cuTexRefSetMipmappedArray,
    handle_cuTexRefSetAddress_v2,
    handle_cuTexRefSetAddress2D_v3,
    handle_cuTexRefSetFormat,
    handle_cuTexRefSetAddressMode,
    handle_cuTexRefSetFilterMode,
    handle_cuTexRefSetMipmapFilterMode,
    handle_cuTexRefSetMipmapLevelBias,
    handle_cuTexRefSetMipmapLevelClamp,
    handle_cuTexRefSetMaxAnisotropy,
    handle_cuTexRefSetBorderColor,
    handle_cuTexRefSetFlags,
    handle_cuTexRefGetAddress_v2,
    handle_cuTexRefGetArray,
    handle_cuTexRefGetMipmappedArray,
    handle_cuTexRefGetAddressMode,
    handle_cuTexRefGetFilterMode,
    handle_cuTexRefGetFormat,
    handle_cuTexRefGetMipmapFilterMode,
    handle_cuTexRefGetMipmapLevelBias,
    handle_cuTexRefGetMipmapLevelClamp,
    handle_cuTexRefGetMaxAnisotropy,
    handle_cuTexRefGetBorderColor,
    handle_cuTexRefGetFlags,
    handle_cuTexRefCreate,
    handle_cuTexRefDestroy,
    handle_cuSurfRefSetArray,
    handle_cuSurfRefGetArray,
    handle_cuTexObjectCreate,
    handle_cuTexObjectDestroy,
    handle_cuTexObjectGetResourceDesc,
    handle_cuTexObjectGetTextureDesc,
    handle_cuTexObjectGetResourceViewDesc,
    handle_cuSurfObjectCreate,
    handle_cuSurfObjectDestroy,
    handle_cuSurfObjectGetResourceDesc,
    handle_cuTensorMapEncodeTiled,
    handle_cuTensorMapEncodeIm2col,
    handle_cuTensorMapReplaceAddress,
    handle_cuDeviceCanAccessPeer,
    handle_cuCtxEnablePeerAccess,
    handle_cuCtxDisablePeerAccess,
    handle_cuDeviceGetP2PAttribute,
    handle_cuGraphicsUnregisterResource,
    handle_cuGraphicsSubResourceGetMappedArray,
    handle_cuGraphicsResourceGetMappedMipmappedArray,
    handle_cuGraphicsResourceGetMappedPointer_v2,
    handle_cuGraphicsResourceSetMapFlags_v2,
    handle_cuGraphicsMapResources,
    handle_cuGraphicsUnmapResources,
    handle_cuGetExportTable,
    handle_cudaDeviceReset,
    handle_cudaDeviceSynchronize,
    handle_cudaDeviceSetLimit,
    handle_cudaDeviceGetLimit,
    handle_cudaDeviceGetTexture1DLinearMaxWidth,
    handle_cudaDeviceGetCacheConfig,
    handle_cudaDeviceGetStreamPriorityRange,
    handle_cudaDeviceSetCacheConfig,
    handle_cudaDeviceGetSharedMemConfig,
    handle_cudaDeviceSetSharedMemConfig,
    handle_cudaDeviceGetByPCIBusId,
    handle_cudaDeviceGetPCIBusId,
    handle_cudaIpcGetEventHandle,
    handle_cudaIpcOpenEventHandle,
    handle_cudaIpcGetMemHandle,
    handle_cudaIpcOpenMemHandle,
    handle_cudaIpcCloseMemHandle,
    handle_cudaDeviceFlushGPUDirectRDMAWrites,
    handle_cudaThreadExit,
    handle_cudaThreadSynchronize,
    handle_cudaThreadSetLimit,
    handle_cudaThreadGetLimit,
    handle_cudaThreadGetCacheConfig,
    handle_cudaThreadSetCacheConfig,
    handle_cudaGetLastError,
    handle_cudaPeekAtLastError,
    handle_cudaGetDeviceCount,
    handle_cudaGetDeviceProperties_v2,
    handle_cudaDeviceGetAttribute,
    handle_cudaDeviceGetDefaultMemPool,
    handle_cudaDeviceSetMemPool,
    handle_cudaDeviceGetMemPool,
    handle_cudaDeviceGetNvSciSyncAttributes,
    handle_cudaDeviceGetP2PAttribute,
    handle_cudaChooseDevice,
    handle_cudaInitDevice,
    handle_cudaSetDevice,
    handle_cudaGetDevice,
    handle_cudaSetValidDevices,
    handle_cudaSetDeviceFlags,
    handle_cudaGetDeviceFlags,
    handle_cudaStreamCreate,
    handle_cudaStreamCreateWithFlags,
    handle_cudaStreamCreateWithPriority,
    handle_cudaStreamGetPriority,
    handle_cudaStreamGetFlags,
    handle_cudaStreamGetId,
    handle_cudaCtxResetPersistingL2Cache,
    handle_cudaStreamCopyAttributes,
    handle_cudaStreamGetAttribute,
    handle_cudaStreamSetAttribute,
    handle_cudaStreamDestroy,
    handle_cudaStreamWaitEvent,
    handle_cudaStreamAddCallback,
    handle_cudaStreamSynchronize,
    handle_cudaStreamQuery,
    handle_cudaStreamAttachMemAsync,
    handle_cudaStreamBeginCapture,
    handle_cudaThreadExchangeStreamCaptureMode,
    handle_cudaStreamEndCapture,
    handle_cudaStreamIsCapturing,
    handle_cudaStreamGetCaptureInfo_v2,
    handle_cudaStreamUpdateCaptureDependencies,
    handle_cudaEventCreate,
    handle_cudaEventCreateWithFlags,
    handle_cudaEventRecord,
    handle_cudaEventRecordWithFlags,
    handle_cudaEventQuery,
    handle_cudaEventSynchronize,
    handle_cudaEventDestroy,
    handle_cudaEventElapsedTime,
    handle_cudaImportExternalMemory,
    handle_cudaExternalMemoryGetMappedBuffer,
    handle_cudaExternalMemoryGetMappedMipmappedArray,
    handle_cudaDestroyExternalMemory,
    handle_cudaImportExternalSemaphore,
    handle_cudaSignalExternalSemaphoresAsync_v2,
    handle_cudaWaitExternalSemaphoresAsync_v2,
    handle_cudaDestroyExternalSemaphore,
    handle_cudaLaunchKernel,
    handle_cudaLaunchKernelExC,
    handle_cudaLaunchCooperativeKernel,
    handle_cudaLaunchCooperativeKernelMultiDevice,
    handle_cudaFuncSetCacheConfig,
    handle_cudaFuncSetSharedMemConfig,
    handle_cudaFuncGetAttributes,
    handle_cudaFuncSetAttribute,
    handle_cudaSetDoubleForDevice,
    handle_cudaSetDoubleForHost,
    handle_cudaLaunchHostFunc,
    handle_cudaOccupancyMaxActiveBlocksPerMultiprocessor,
    handle_cudaOccupancyAvailableDynamicSMemPerBlock,
    handle_cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
    handle_cudaOccupancyMaxPotentialClusterSize,
    handle_cudaOccupancyMaxActiveClusters,
    handle_cudaMallocManaged,
    handle_cudaMalloc,
    handle_cudaMallocHost,
    handle_cudaMallocPitch,
    handle_cudaMallocArray,
    handle_cudaFree,
    handle_cudaFreeHost,
    handle_cudaFreeArray,
    handle_cudaFreeMipmappedArray,
    handle_cudaHostAlloc,
    handle_cudaHostRegister,
    handle_cudaHostUnregister,
    handle_cudaHostGetDevicePointer,
    handle_cudaHostGetFlags,
    handle_cudaMalloc3D,
    handle_cudaMalloc3DArray,
    handle_cudaMallocMipmappedArray,
    handle_cudaGetMipmappedArrayLevel,
    handle_cudaMemcpy3D,
    handle_cudaMemcpy3DPeer,
    handle_cudaMemcpy3DAsync,
    handle_cudaMemcpy3DPeerAsync,
    handle_cudaMemGetInfo,
    handle_cudaArrayGetInfo,
    handle_cudaArrayGetPlane,
    handle_cudaArrayGetMemoryRequirements,
    handle_cudaMipmappedArrayGetMemoryRequirements,
    handle_cudaArrayGetSparseProperties,
    handle_cudaMipmappedArrayGetSparseProperties,
    handle_cudaMemcpy,
    handle_cudaMemcpyPeer,
    handle_cudaMemcpy2D,
    handle_cudaMemcpy2DToArray,
    handle_cudaMemcpy2DFromArray,
    handle_cudaMemcpy2DArrayToArray,
    handle_cudaMemcpyToSymbol,
    handle_cudaMemcpyFromSymbol,
    handle_cudaMemcpyAsync,
    handle_cudaMemcpyPeerAsync,
    handle_cudaMemcpy2DAsync,
    handle_cudaMemcpy2DToArrayAsync,
    handle_cudaMemcpy2DFromArrayAsync,
    handle_cudaMemcpyToSymbolAsync,
    handle_cudaMemcpyFromSymbolAsync,
    handle_cudaMemset,
    handle_cudaMemset2D,
    handle_cudaMemset3D,
    handle_cudaMemsetAsync,
    handle_cudaMemset2DAsync,
    handle_cudaMemset3DAsync,
    handle_cudaGetSymbolAddress,
    handle_cudaGetSymbolSize,
    handle_cudaMemPrefetchAsync,
    handle_cudaMemAdvise,
    handle_cudaMemRangeGetAttribute,
    handle_cudaMemRangeGetAttributes,
    handle_cudaMemcpyToArray,
    handle_cudaMemcpyFromArray,
    handle_cudaMemcpyArrayToArray,
    handle_cudaMemcpyToArrayAsync,
    handle_cudaMemcpyFromArrayAsync,
    handle_cudaMallocAsync,
    handle_cudaFreeAsync,
    handle_cudaMemPoolTrimTo,
    handle_cudaMemPoolSetAttribute,
    handle_cudaMemPoolGetAttribute,
    handle_cudaMemPoolSetAccess,
    handle_cudaMemPoolGetAccess,
    handle_cudaMemPoolCreate,
    handle_cudaMemPoolDestroy,
    handle_cudaMallocFromPoolAsync,
    handle_cudaMemPoolExportToShareableHandle,
    handle_cudaMemPoolImportFromShareableHandle,
    handle_cudaMemPoolExportPointer,
    handle_cudaMemPoolImportPointer,
    handle_cudaPointerGetAttributes,
    handle_cudaDeviceCanAccessPeer,
    handle_cudaDeviceEnablePeerAccess,
    handle_cudaDeviceDisablePeerAccess,
    handle_cudaGraphicsUnregisterResource,
    handle_cudaGraphicsResourceSetMapFlags,
    handle_cudaGraphicsMapResources,
    handle_cudaGraphicsUnmapResources,
    handle_cudaGraphicsResourceGetMappedPointer,
    handle_cudaGraphicsSubResourceGetMappedArray,
    handle_cudaGraphicsResourceGetMappedMipmappedArray,
    handle_cudaGetChannelDesc,
    handle_cudaCreateTextureObject,
    handle_cudaDestroyTextureObject,
    handle_cudaGetTextureObjectResourceDesc,
    handle_cudaGetTextureObjectTextureDesc,
    handle_cudaGetTextureObjectResourceViewDesc,
    handle_cudaCreateSurfaceObject,
    handle_cudaDestroySurfaceObject,
    handle_cudaGetSurfaceObjectResourceDesc,
    handle_cudaDriverGetVersion,
    handle_cudaRuntimeGetVersion,
    handle_cudaGraphCreate,
    handle_cudaGraphAddKernelNode,
    handle_cudaGraphKernelNodeGetParams,
    handle_cudaGraphKernelNodeSetParams,
    handle_cudaGraphKernelNodeCopyAttributes,
    handle_cudaGraphKernelNodeGetAttribute,
    handle_cudaGraphKernelNodeSetAttribute,
    handle_cudaGraphAddMemcpyNode,
    handle_cudaGraphAddMemcpyNodeToSymbol,
    handle_cudaGraphAddMemcpyNodeFromSymbol,
    handle_cudaGraphAddMemcpyNode1D,
    handle_cudaGraphMemcpyNodeGetParams,
    handle_cudaGraphMemcpyNodeSetParams,
    handle_cudaGraphMemcpyNodeSetParamsToSymbol,
    handle_cudaGraphMemcpyNodeSetParamsFromSymbol,
    handle_cudaGraphMemcpyNodeSetParams1D,
    handle_cudaGraphAddMemsetNode,
    handle_cudaGraphMemsetNodeGetParams,
    handle_cudaGraphMemsetNodeSetParams,
    handle_cudaGraphAddHostNode,
    handle_cudaGraphHostNodeGetParams,
    handle_cudaGraphHostNodeSetParams,
    handle_cudaGraphAddChildGraphNode,
    handle_cudaGraphChildGraphNodeGetGraph,
    handle_cudaGraphAddEmptyNode,
    handle_cudaGraphAddEventRecordNode,
    handle_cudaGraphEventRecordNodeGetEvent,
    handle_cudaGraphEventRecordNodeSetEvent,
    handle_cudaGraphAddEventWaitNode,
    handle_cudaGraphEventWaitNodeGetEvent,
    handle_cudaGraphEventWaitNodeSetEvent,
    handle_cudaGraphAddExternalSemaphoresSignalNode,
    handle_cudaGraphExternalSemaphoresSignalNodeGetParams,
    handle_cudaGraphExternalSemaphoresSignalNodeSetParams,
    handle_cudaGraphAddExternalSemaphoresWaitNode,
    handle_cudaGraphExternalSemaphoresWaitNodeGetParams,
    handle_cudaGraphExternalSemaphoresWaitNodeSetParams,
    handle_cudaGraphAddMemAllocNode,
    handle_cudaGraphMemAllocNodeGetParams,
    handle_cudaGraphAddMemFreeNode,
    handle_cudaGraphMemFreeNodeGetParams,
    handle_cudaDeviceGraphMemTrim,
    handle_cudaDeviceGetGraphMemAttribute,
    handle_cudaDeviceSetGraphMemAttribute,
    handle_cudaGraphClone,
    handle_cudaGraphNodeFindInClone,
    handle_cudaGraphNodeGetType,
    handle_cudaGraphGetNodes,
    handle_cudaGraphGetRootNodes,
    handle_cudaGraphGetEdges,
    handle_cudaGraphNodeGetDependencies,
    handle_cudaGraphNodeGetDependentNodes,
    handle_cudaGraphAddDependencies,
    handle_cudaGraphRemoveDependencies,
    handle_cudaGraphDestroyNode,
    handle_cudaGraphInstantiate,
    handle_cudaGraphInstantiateWithFlags,
    handle_cudaGraphInstantiateWithParams,
    handle_cudaGraphExecGetFlags,
    handle_cudaGraphExecKernelNodeSetParams,
    handle_cudaGraphExecMemcpyNodeSetParams,
    handle_cudaGraphExecMemcpyNodeSetParamsToSymbol,
    handle_cudaGraphExecMemcpyNodeSetParamsFromSymbol,
    handle_cudaGraphExecMemcpyNodeSetParams1D,
    handle_cudaGraphExecMemsetNodeSetParams,
    handle_cudaGraphExecHostNodeSetParams,
    handle_cudaGraphExecChildGraphNodeSetParams,
    handle_cudaGraphExecEventRecordNodeSetEvent,
    handle_cudaGraphExecEventWaitNodeSetEvent,
    handle_cudaGraphExecExternalSemaphoresSignalNodeSetParams,
    handle_cudaGraphExecExternalSemaphoresWaitNodeSetParams,
    handle_cudaGraphNodeSetEnabled,
    handle_cudaGraphNodeGetEnabled,
    handle_cudaGraphExecUpdate,
    handle_cudaGraphUpload,
    handle_cudaGraphLaunch,
    handle_cudaGraphExecDestroy,
    handle_cudaGraphDestroy,
    handle_cudaGraphDebugDotPrint,
    handle_cudaUserObjectCreate,
    handle_cudaUserObjectRetain,
    handle_cudaUserObjectRelease,
    handle_cudaGraphRetainUserObject,
    handle_cudaGraphReleaseUserObject,
    handle_cudaGetDriverEntryPoint,
    handle_cudaGetExportTable,
    handle_cudaGetFuncBySymbol,
};

RequestHandler get_handler(const int op)
{
    return opHandlers[op];
}
